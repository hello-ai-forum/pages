<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8" />
    <title>ZENKEI AI FORUM (2023/02/22)</title>
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+1p:100,400,700&display=swap&subset=japanese" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../bright-M_PLUS_1p.css" />

    <link rel="stylesheet"
	  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="text/javascript" id="MathJax-script" async
	    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

<body style="font-size: 20px;">

<header>
<center><h1>ZENKEI AI FORUM 2023/02/22</h1></center>
</header>

<article>

<section id="main">

  <center>
    <a href="ZENKEI_AI_FORUM_zoom_20230222-2488x1400.jpg"><!-- 2341x1400 -->
      <img src="ZENKEI_AI_FORUM_zoom_20230222-2488x1400_thumb.jpg" width="800" height="478" style="border: 2px #ccc solid;" /></a>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      ZAF 2023 年 2 月 22 日
    </div>
    <div style="font-size: 40px;">＜本日のテーマ＞</div>
    <div style="font-size: 70px;">
      ChatGPT 話題ですね
    </div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <h2 id="toc">目次</h2>
  <ul>
    <li>[6:30 - 7:00]
      <b>前座</b>
      <a href="#part0">ポッドキャストについて、あれこれ</a>
      <ul>
	<li><a href="#part0-1">ZENKEI AI ポッドキャストをアップグレード！</a></li>
	<li><a href="#part0-music-and-math">音楽と数理 🎼 ♾ ポッドキャスト</a></li>
      </ul>
    </li>

    <li>[7:00 - 8:00]
      <b>パート１</b>
      <a href="#part1">Transformer を完璧に理解する！</a></li>
    </li>

    <li>[8:00 - 9:00]
      <b>パート２</b>
      <a href="#part2">音声合成</a>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>

    <li><a href="#detailed-toc">総合目次</a></li>
  </ul>

  <hr />

  <center>
    <br />
    <div style="font-size: 30px;">
      YouTube のアーカイブ・ビデオはこちら
    </div>
    (<a href="https://youtube.com/live/dBtz5ATfvJ0">
      https://youtube.com/live/dBtz5ATfvJ0</a>)
    <br /><br />
    <a href="https://youtube.com/live/dBtz5ATfvJ0">
      <img src="Screen Shot 2023-03-31 at 10.37.21_thumb.jpg" width="500" height="282" style="border: 2px #ccc solid;" /></a>
    <br /><br />
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0">
    <div style="font-size: 50px; font-weight: bolder;">
      （前座）
      <br />
      ポッドキャストについて、あれこれ
    </div>
    <br /><br />
  </center>

  <ul>
    <li>ポッドキャスト、日々、続けています</li>
    <li>その活動の、この１ヶ月のなかで起こったはなし、など</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-1">
    <div style="font-size: 50px; font-weight: bolder;">
      ZENKEI AI ポッドキャストをアップグレード！
    </div>
    <br /><br />
  </center>

  <ul>
    <li>ZAP こと <a href="https://zenkei.seesaa.net/">ZENKEI AI ポッドキャスト</a>の公開スケジュールを変更します
      <br />
      (<a href="https://twitter.com/zenkeiaif/status/1625697590347182083">https://twitter.com/zenkeiaif/status/1625697590347182083</a>)
      <center>
	<a href="Screen Shot 2023-02-18 at 14.01.42.png"><!-- 1185x1174 -->
	  <img src="Screen Shot 2023-02-18 at 14.01.42_thumb.jpg" width="400" height="396" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>つまり
      <center>
	<div style="font-size: 40px;">
	  <br />
	  毎週１エピソード公開から
	</div>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  毎週２エピソード公開に
	</div>
	<div style="font-size: 50px; font-weight: bolder;">
	  アップグレード！
	  <br /><br />
	</div>
      </center>
      <ul>
	<li>毎週、水曜日と日曜日の正午にリリース</li>
      </ul>
    </li>

    <li>その心は？
      <center>
	<div style="font-size: 50px; font-weight: bolder;">
	  <br />
	  ZAF から遅れすぎなので、少し追いつくぞ！
	  <br /><br />
	</div>
      </center>
      <ul>
	<li>イメージとしては、半年くらいの時間差で進行しようと思ってましたが、</li>
	<li>現在、 ZAP はシーズン２９として ZAF-2205 をリリースしようとしているところ</li>
	<li>つまり９ヶ月遅れ</li>
	<li>６ヶ月程度になるまで、週２本ペースで進める予定です
	  <ul>
	    <li>２月末 (ZAF-2303)〜 : ZAP S29 (ZAF-2205), ZAP S30 (ZAF-2206)</li>
	    <li>３月末 (ZAF-2303)〜 : ZAP S31 (ZAF-2207), ZAP S32 (ZAF-2208)</li>
	    <li>４月末 (ZAF-2304)〜 : ZAP S33 (ZAF-2209), ZAP S34 (ZAF-2210)</li>
	  </ul>
	</li>大雑把には、５月末には、週１ペースに戻る予定ですね</li>
      </ul>
    </li>      

    <li>現在の仕込み状況：
      <center>
	<a href="Screen Shot 2023-02-18 at 14.17.38.png"><!-- 2745x1078 -->
	  <img src="Screen Shot 2023-02-18 at 14.17.38_thumb.jpg" width="800" height="314" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-music-and-math">
    <div style="font-size: 50px; font-weight: bolder;">
      もう１つのポッドキャスト<br />
      音楽と数理 🎼 ♾ ポッドキャスト
    </div>
  </center>

  <ul>
    <li>ぼくの趣味１００％のポッドキャストである「<a href="https://music0math.wordpress.com/">音楽と数理 🎼 ♾ ポッドキャスト</a>」
      <center>
	<a href="Screen Shot 2023-02-22 at 14.34.14.png"><!-- 2761x1251 -->
	  <img src="Screen Shot 2023-02-22 at 14.34.14_thumb.jpg" width="800" height="362" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>こちらは、今年 2023 年から、金曜日の夜１０時に
      定期的にリリースすることにしました</li>

    <li>というのは、既に告知してますが、<br />
      今日のアナウンスは、
      <center>
	<br />
	<div style="font-size: 50px; font-weight: bolder;">
	  これまでリリースされた全３９エピソード<br />
	  書き起こしがつきました！
	  <br /><br />
	</div>
	<a href="Screen Shot 2023-02-22 at 17.49.27.png"><!-- 2750x1253 -->
	  <img src="Screen Shot 2023-02-22 at 17.49.27_thumb.jpg" width="400" height="182" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 17.49.36.png"><!-- 2744x1096 -->
	  <img src="Screen Shot 2023-02-22 at 17.49.36_thumb.jpg" width="400" height="160" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 17.49.42.png"><!-- 2732x1103 -->
	  <img src="Screen Shot 2023-02-22 at 17.49.42_thumb.jpg" width="400" height="161" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 17.49.51.png"><!-- 2741x1031 -->
	  <img src="Screen Shot 2023-02-22 at 17.49.51_thumb.jpg" width="400" height="150" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 17.49.57.png"><!-- 2738x877 -->
	  <img src="Screen Shot 2023-02-22 at 17.49.57_thumb.jpg" width="400" height="128" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 17.50.05.png"><!-- 2738x883 -->
	  <img src="Screen Shot 2023-02-22 at 17.50.05_thumb.jpg" width="400" height="129" style="border: 2px #ccc solid;" /></a>
	<br /><br /><br />
	例えば、こんな感じ（<a href="https://music0math.wordpress.com/2022/04/08/%e9%9f%b3%e6%a5%bd%e3%81%a8%e6%95%b0%e7%90%86%e3%81%a8%e3%81%af%ef%bc%88all-of-you%ef%bc%89/">S01E01</a>）
	<br /><br />
	<a href="Screen Shot 2023-02-22 at 18.01.03.png"><!-- 2748x1211 -->
	  <img src="Screen Shot 2023-02-22 at 18.01.03_thumb.jpg" width="800" height="353" style="border: 2px #ccc solid;" /></a>	
	<a href="Screen Shot 2023-02-22 at 18.00.50.png"><!-- 2743x1246 -->
	  <img src="Screen Shot 2023-02-22 at 18.00.50_thumb.jpg" width="800" height="363" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>とはいえ、
	  まだほとんどの回は Whisper (large-v2) の出力そのままで、<br />
	  校正はできてません（こちらは、マイペースでやっていく予定）</li>
      </ul>
    </li>

    <li>この「音楽と数理 🎼 ♾ ポッドキャスト」の書き下ろしは、どこに向かっているのか？
      <ul>
	<li>それは、もちろん
	  <center>
	    <br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      技術書典１４
	      <br />
	    </div>
	    <a href="Screen Shot 2023-02-22 at 18.04.42.png"><!-- 2780x1079 -->
	      <img src="Screen Shot 2023-02-22 at 18.04.42_thumb.jpg" width="800" height="311" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>つまり
	  <center>
	    <br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      文庫本を出す！<br />
	    </div>
	    <div style="font-size: 60px; font-weight: bolder;">
	      エッセイ　音楽と数理<br />
	      ポッドキャストは自由にする（仮）
	      <br /><br />
	    </div>
	    <a href="Screen Shot 2023-02-22 at 18.04.56.png"><!-- 2803x953 -->
	      <img src="Screen Shot 2023-02-22 at 18.04.56_thumb.jpg" width="800" height="272" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>（サブ）タイトルは仮で、もう１つの案
	  <center>
	    <div style="font-size: 60px; font-weight: bolder;">
	      エッセイ　音楽と数理<br />
	      Conversations with Myself（仮）
	      <br /><br />
	    </div>
	  </center>
	  と、悩み中
	</li>
      </ul>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      パート１
      <br />
      Transformer を完璧に理解する！
    </div>
  </center>

  <ul>
    <li>2023 年の今（通りいっぺんの情報通を超えたレベルで）
      押さえておきたい AI 技術は
      <center>
	<br />
	<div style="font-size: 50px; font-weight: bolder;">
	  Transformer
	  <br />
	  Diffusion models
	  <br /><br />
	</div>
      </center>
      に、間違いない（むしろ、今更という感じ）
      <ul>
	<li>５年前とかは、これが
	  <center>
	    <br />
	    <div style="font-size: 35px;">
	      Convolutional Neural Networks (CNN)
	      <br />
	      Recurrent Neural Network (RNN)
	      <br /><br />
	    </div>
	  </center>
	  だったわけ
	</li>
	<li>未だに、そのレベルにとどまっていないか？というはなし</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-transformer-before">
    <div style="font-size: 40px; font-weight: bolder;">
      <br />
      これまでの試み、その１
    </div>
    <div style="font-size: 50px; font-weight: bolder;">
      Transformer
    </div>
  </center>

  <ul>
    <li><a href="https://hello-ai-forum.github.io/pages/ZAF202108/ichiki/#whats-new">ZAF-2108</a> HuggingFace Transformers 眺める
      <center>
	<a href="Screen Shot 2023-02-18 at 14.34.07.png"><!-- 2808x1156 -->
	  <img src="Screen Shot 2023-02-18 at 14.34.07_thumb.jpg" width="800" height="329" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-18 at 14.34.26.png"><!-- 2809x1259 -->
	  <img src="Screen Shot 2023-02-18 at 14.34.26_thumb.jpg" width="800" height="359" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li><a href="https://hello-ai-forum.github.io/pages/ZAF202109/ichiki/#reading">ZAF-2109</a> Transformer 論文読む
      <center>
	<a href="Screen Shot 2023-02-15 at 18.17.58.png"><!-- 2784x1365 -->
	  <img src="Screen Shot 2023-02-15 at 18.17.58_thumb.jpg" width="800" height="392" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-15 at 18.18.32.png"><!-- 2782x827 -->
	  <img src="Screen Shot 2023-02-15 at 18.18.32_thumb.jpg" width="800" height="238" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-15 at 18.18.36.png"><!-- 2776x1224 -->
	  <img src="Screen Shot 2023-02-15 at 18.18.36_thumb.jpg" width="800" height="353" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-15 at 18.18.54.png"><!-- 2785x1156 -->
	  <img src="Screen Shot 2023-02-15 at 18.18.54_thumb.jpg" width="800" height="332" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>（付記）この「構想」が良くなかったか……
      <ul>
	<li>Transformer 論文を読んだけど、ピンとこなかったので、</li>
	<li>LSTM with Attention に降りてみたのだが、</li>
	<li>その結果、戻れなくなってた（良くあるパターン）</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-diffusion-before">
    <div style="font-size: 40px; font-weight: bolder;">
      <br />
      これまでの試み、その２
    </div>
    <div style="font-size: 50px; font-weight: bolder;">
      Diffusion models
    </div>
  </center>

  <ul>
    <li><a href="https://hello-ai-forum.github.io/pages/ZAF202209/ichiki/#part1">ZAF-2209</a> Diffusion Model レビューペーパー出てる
      <center>
	<a href="Screen Shot 2023-02-18 at 14.40.40.png"><!-- 2800x811 -->
	  <img src="Screen Shot 2023-02-18 at 14.40.40_thumb.jpg" width="800" height="232" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-18 at 14.40.48.png"><!-- 2801x427 -->
	  <img src="Screen Shot 2023-02-18 at 14.40.48_thumb.jpg" width="800" height="122" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-18 at 14.41.19.png"><!-- 2791x1129 -->
	  <img src="Screen Shot 2023-02-18 at 14.41.19_thumb.jpg" width="800" height="324" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li><a href="https://hello-ai-forum.github.io/pages/ZAF202211/ichiki/#part2">ZAF-2211</a> Diffusion Model ちょっと分かります
      <center>
	<a href="Screen Shot 2023-02-18 at 14.29.08.png"><!-- 2807x1208 -->
	  <img src="Screen Shot 2023-02-18 at 14.29.08_thumb.jpg" width="800" height="344" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>このあと、また止まっている
      <ul>
	<li>岡野原さんの本がもうすぐ届くので、
	  <center>
	    <a href="https://www.amazon.co.jp/gp/product/400006343X">
	      <img src="Screen Shot 2023-02-23 at 12.38.45_thumb.jpg" width="600" height="391" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>その前に、自分で目処をつけておきたいとは思っているが……</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-transformer-from-scratch">
    <br />
    <div style="font-size: 40px;">
      改めて、素朴に
    </div>
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      プログラマ的に
      <br />
      Transformer 入門
    </div>
    <br /><br />
  </center>

  <ul>
    <li>前回 (<a href="https://hello-ai-forum.github.io/pages/ZAF202301/ichiki/#part1">ZAF-2301</a>) も、
      世間の喧騒に嫌気がさして、純粋にコードと対話したい気分になった時
      <center>
	<a href="Screen Shot 2023-02-18 at 14.27.26.png"><!-- 2813x1226 -->
	  <img src="Screen Shot 2023-02-18 at 14.27.26_thumb.jpg" width="800" height="349" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>ということで、重い腰を上げて、<br />
      先日やっと Andrej Karpathy の <a href="https://youtu.be/kCc8FmEb1nY">YouTube ビデオ</a> を見ました！
      <center>
	<a href="https://youtu.be/kCc8FmEb1nY">
	  <img src="Screen Shot 2023-02-14 at 21.33.24_thumb.jpg" width="800" height="409" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-andrej-karpathy">
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      Andrej Karpathy とは
    </div>
    <br /><br />
  </center>

  <ul>
    <li>Andrej Karpathy (<a href="https://karpathy.ai/">karpathy.ai</a>)
      といえば
      <center>
	<a href="Screen Shot 2023-02-22 at 9.06.30.png"><!-- 2815x552 -->
	  <img src="Screen Shot 2023-02-22 at 9.06.30_thumb.jpg" width="800" height="157" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 9.08.31.png"><!-- 2809x852 -->
	  <img src="Screen Shot 2023-02-22 at 9.08.31_thumb.jpg" width="800" height="243" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>それは 2015 年のことだったみたい（？）
	  <ul>
	    <li>（あるいは、 2017 年とかの FastAI で jeremy が紹介してたかな）</li>
	  </ul>
	</li>

	<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (May 21, 2015)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 9.08.48.png"><!-- 2805x895 -->
	      <img src="Screen Shot 2023-02-22 at 9.08.48_thumb.jpg" width="800" height="255" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>


	<li>そこではスクラッチから RNN を実装していて、<br />
	  character ベースの Language Model を作ってた
	  <center>
	    <a href="Screen Shot 2023-02-22 at 9.16.36.png"><!-- 2798x1240 -->
	      <img src="Screen Shot 2023-02-22 at 9.16.36_thumb.jpg" width="800" height="355" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>その「応用例」として、<br />
	  定番の、誰かのブログを学習させて、
	  その人が書いた文章みたいなものを生成させてみたり
	  <center>
	    <a href="Screen Shot 2023-02-22 at 9.16.44.png"><!-- 2804x604 -->
	      <img src="Screen Shot 2023-02-22 at 9.16.44_thumb.jpg" width="800" height="172" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>もっと面白かったのは（この辺が記憶に残ってる）<br />
	  LaTeX のソースファイル（テキストファイル）を学習させると、
	  LaTeX で書かれた文書（論文）を生成した、とか
	  <center>
	    <a href="Screen Shot 2023-02-22 at 9.17.06.png"><!-- 2798x1349 -->
	      <img src="Screen Shot 2023-02-22 at 9.17.06_thumb.jpg" width="800" height="386" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>今回、見直してて「おぉ 2015 年にこんなことやってたんだ」と思った<br />
	  GitHub の Linux のレポジトリ (<a href="https://github.com/torvalds/linux">https://github.com/torvalds/linux</a>) のソースコードを学習させると、
	  プログラムを生成した、とか
	  <center>
	    <a href="Screen Shot 2023-02-22 at 9.18.31.png"><!-- 2785x1184 -->
	      <img src="Screen Shot 2023-02-22 at 9.18.31_thumb.jpg" width="800" height="340" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li>

    <li>昨年の春ごろだったかな、
      <ul>
	<li>Tesla を辞めて<br />
	  （サバティカル的に）<br />
	  「先のことは決まってないけど数ヶ月休む」<br />
	  みたいなツイートしてました<br />
	  (<a href="https://twitter.com/karpathy/status/1547332300186066944">https://twitter.com/karpathy/status/1547332300186066944</a>)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 10.00.34.png"><!-- 1245x854 -->
	      <img src="Screen Shot 2023-02-22 at 10.00.34_thumb.jpg" width="400" height="274" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>それが、先頃（今年に入って）<br />
	  「OpenAI に行くことに決めた」<br />
	  とツイートしてた<br />
	  (<a href="https://twitter.com/karpathy/status/1623476659369443328">https://twitter.com/karpathy/status/1623476659369443328</a>)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 10.11.49.png"><!-- 1183x596 -->
	      <img src="Screen Shot 2023-02-22 at 10.11.49_thumb.jpg" width="400" height="202" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>そういうことなので、<br />
      今 Transformer を（GPT を）スクラッチから書いて解説するのに<br />
      多分、世界で一番最適な人だと思います
      <ul>
	<li>「GPT をスクラッチから書く」こと自体は、<br />
	  １年くらい前だったかな、 minGPT というコードを github に
	  既に公開してた（のは、知ってた）<br />
	  (<a href="https://twitter.com/karpathy/status/1295410274095095810">https://twitter.com/karpathy/status/1295410274095095810</a>)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 10.20.00.png"><!-- 1184x1238 -->
	      <img src="Screen Shot 2023-02-22 at 10.20.00_thumb.jpg" width="383" height="400" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>github: <a href="https://github.com/karpathy/minGPT">karpathy/minGPT</a>
	      <center>
		<a href="Screen Shot 2023-02-22 at 10.21.13.png"><!-- 2801x1268 -->
		  <img src="Screen Shot 2023-02-22 at 10.21.13_thumb.jpg" width="800" height="362" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	  </ul>
	</li>
	<li>……けど、「勉強しよう、勉強しよう」と言ってて、<br />
	  結局、今まで見れてなかった</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      いざ、スクラッチから GPT を実装！
    </div>
  </center>

  <ul>
    <li>ぼくは「<b>スパルタン</b>」なので（ご承知の通り、だよね）
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  みなさんに
	  <br />
	  ノートブックとかソースコードとかは
	  <br />
	  共有しません！
	  <br /><br />
	</div>
	その代わり
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  スクリーンショットを示します
	  <br /><br />
	</div>
	つまり（分かりますね？）
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  画面を見て、自分でタイプしましょう！
	  <br /><br />
	</div>
      </center>
      <ul>
	<li>あ、ちなみに、ぼく自身、今回の Andrej Karpathy のビデオを<br />
	  一時停止しながら、 Jupyter Notebook に写経しました</li>
	<li>その結果、１つ、間違い（多分）を見つけました
	  <ul>
	    <li>（些細なことですが）</li>
	  </ul>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch-01">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      Character Level の GPT
    </div>
  </center>

  <ul>
    <li>まず、ぼくが Andrej Karpathy のビデオをみながら板書を取った
      コードの最終形をみておく
      <ul>
	<li>パラメータなど。<br />
	  データセットは shapespeare
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.17.png"><!-- 2260x414 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.17_thumb.jpg" width="800" height="147" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.40.23.png"><!-- 2250x1180 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.23_thumb.jpg" width="800" height="420" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>tokenizer はお手製の char-level のもの<br />
	  dataloader もお手製のもの
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.26.png"><!-- 2255x989 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.26_thumb.jpg" width="800" height="351" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.40.31.png"><!-- 2253x819 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.31_thumb.jpg" width="800" height="291" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>ここからがモデルの定義</li>
	<li>最初に一番のコアである <tt>Head</tt>
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.46.png"><!-- 2255x904 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.46_thumb.jpg" width="800" height="321" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>ここ、多分 Andrej のコード、間違ってると思う</li>
	    <li><tt>wei</tt> の normalization が、<br />
	      <tt>C</tt> という <tt>x</tt> のチャンネル数が使われていたが、<br />
	      （彼自身、解説部分で説明していた通り）<br />
	      <tt>k, q, v</tt> とかのテンソルのチャンネル数とすべき
	    </li>
	    <li>（とはいえ、多分、影響するのは学習の初期の収束性くらいかな？）</li>
	  </ul>
	</li>
	<li>以下、 <tt>MultiHeadAttention, FeedForward, Block</tt> と続く
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.49.png"><!-- 2249x1026 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.49_thumb.jpg" width="800" height="365" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.40.35.png"><!-- 2253x519 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.35_thumb.jpg" width="800" height="184" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>下らない修正（タイポ）</li>
	    <li>Andrej のコードは <tt>FeedFoward</tt> ってなってた</li>
	  </ul>
	</li>
	<li>以上をまとめて、モデル全体
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.40.png"><!-- 2251x1069 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.40_thumb.jpg" width="800" height="380" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.40.43.png"><!-- 2245x412 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.43_thumb.jpg" width="800" height="147" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>お手製の学習ループで学習させた結果
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.40.57.png"><!-- 2258x1180 -->
	      <img src="Screen Shot 2023-02-22 at 15.40.57_thumb.jpg" width="800" height="418" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.41.01.png"><!-- 2258x554 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.01_thumb.jpg" width="800" height="196" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>ほぼ、 Andrej の結果と同じ</li>
	    <li>（seed も同じなので、本当なら完璧に一致すべきところだが、<br />
	      normalization の bug fix の影響かな？）</li>
	  </ul>
	</li>
      </ul>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch-02">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      tiktokes
    </div>
  </center>

  <ul>
    <li>OpenAI が提供している Tokenizer の tiktoken を使ってみる
      <ul>
	<li>GPT で、今や世界を征服したと言っても過言ではない OpenAI さんの<br />
	  謹製 tokenizer なので、これ使っとけ場いいだろう的な<br />
	  （日本語がぁ、とかも、当然、気にしなくていいだろうし）
	</li>
	<li>github: <a href="https://github.com/openai/tiktoken">openai/tiktoken</a>
	  <center>
	    <a href="Screen Shot 2023-02-22 at 16.52.44.png"><!-- 2799x1057 -->
	      <img src="Screen Shot 2023-02-22 at 16.52.44_thumb.jpg" width="800" height="302" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>Character level のモデルとどれくらい性能が違うのか、<br />
      あと、当然のこととして日本語を使いたいので、試してみる
      <ul>
	<li>パラメータを調整<br />
	  （メモリーの問題で <tt>batch_size</tt> と <tt>block_size</tt> を減らした）
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.41.07.png"><!-- 2243x1091 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.07_thumb.jpg" width="800" height="389" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><tt>encoder</tt>, <tt>decode</tt> は
	  これまでのコードがそのまま使えるように<br />
	  （データセットは shakespeare のまま）
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.41.13.png"><!-- 2250x896 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.13_thumb.jpg" width="800" height="319" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>いい機会だったので、モデルの名前を
	  <tt>tinyGPT</tt> にしておく
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.41.34.png"><!-- 2243x1067 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.34_thumb.jpg" width="800" height="381" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.41.49.png"><!-- 2250x404 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.49_thumb.jpg" width="800" height="144" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.41.53.png"><!-- 2251x1122 -->
	      <img src="Screen Shot 2023-02-22 at 15.41.53_thumb.jpg" width="800" height="399" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>結果
	  <center>
	    <a href="Screen Shot 2023-02-22 at 15.42.02.png"><!-- 2249x1116 -->
	      <img src="Screen Shot 2023-02-22 at 15.42.02_thumb.jpg" width="800" height="397" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 15.42.14.png"><!-- 2246x790 -->
	      <img src="Screen Shot 2023-02-22 at 15.42.14_thumb.jpg" width="800" height="281" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch-03">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      D-Adaptation
    </div>
  </center>

  <ul>
    <li>Facebook AI Research が、パラメーターフリーな（自動でパラメータ調整してくれる）
      optimizer を発表したらしい
      <br />
      <a href="https://twitter.com/_akhaliq/status/1616266295330185218">https://twitter.com/_akhaliq/status/1616266295330185218</a>
      <center>
	<a href="Screen Shot 2023-02-22 at 16.44.44.png"><!-- 1187x1201 -->
	  <img src="Screen Shot 2023-02-22 at 16.44.44_thumb.jpg" width="395" height="400" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/2301.07733">2301.07733</a>
	  (<a href="arxiv-2301.07733-dadaptation.pdf">local copy</a>)<br />
	  Learning-Rate-Free Learning by D-Adaptation
	  (Aaron Defazio, Konstantin Mishchenko)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 16.47.38.png"><!-- 2186x803 -->
	      <img src="Screen Shot 2023-02-22 at 16.47.38_thumb.jpg" width="800" height="294" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>github: <a href="https://github.com/facebookresearch/dadaptation">facebookresearch/dadaptation</a>
	  <center>
	    <a href="Screen Shot 2023-02-22 at 16.43.56.png"><!-- 2806x1178 -->
	      <img src="Screen Shot 2023-02-22 at 16.43.56_thumb.jpg" width="800" height="336" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>
    <li>ちょうどいい機会なので、試してみた
      <center>
	<a href="Screen Shot 2023-02-22 at 15.42.20.png"><!-- 2260x662 -->
	  <img src="Screen Shot 2023-02-22 at 15.42.20_thumb.jpg" width="800" height="234" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 15.42.25.png"><!-- 2258x1061 -->
	  <img src="Screen Shot 2023-02-22 at 15.42.25_thumb.jpg" width="800" height="376" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 15.42.30.png"><!-- 2253x801 -->
	  <img src="Screen Shot 2023-02-22 at 15.42.30_thumb.jpg" width="800" height="284" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch-04">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      青空文庫
    </div>
  </center>

  <ul>
    <li>せっかく tiktoken を使ったので、<br />
      日本語に挑戦<br />
      日本語のデータセットといえば「青空文庫」ですね
      <ul>
	<li>ZENKEI AI FORUM でも NLP の話で、昔、データセット作りました<br />
	  cf. Qiita:
	  <a href="https://qiita.com/kichiki/items/bb65f7b57e09789a05ce">青空文庫の外字をPythonでUnicodeに置換</a> (@kichiki posted at 2018-10-23 updated at 2018-10-24)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 17.18.32.png"><!-- 1963x711 -->
	      <img src="Screen Shot 2023-02-22 at 17.18.32_thumb.jpg" width="800" height="290" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>
    <li>データセットの準備
      <center>
	<a href="Screen Shot 2023-02-22 at 15.42.52.png"><!-- 2261x1103 -->
	  <img src="Screen Shot 2023-02-22 at 15.42.52_thumb.jpg" width="800" height="390" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 15.43.05.png"><!-- 2252x766 -->
	  <img src="Screen Shot 2023-02-22 at 15.43.05_thumb.jpg" width="800" height="272" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>Language Model 用のデータセットなので、<br />
	  （作家別にまとめてあったけれど）<br />
	  今回は手元にあった１４作家を全員まとめて１つのデータとした
	</li>
      </ul>
    </li>
    <li>パラメータなどは、先の <tt>tiktoken</tt> の時と同じ
      <center>
	<a href="Screen Shot 2023-02-22 at 15.43.11.png"><!-- 2259x1051 -->
	  <img src="Screen Shot 2023-02-22 at 15.43.11_thumb.jpg" width="800" height="372" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>データのサイズはこれくらい
      <center>
	<a href="Screen Shot 2023-02-22 at 15.43.22.png"><!-- 2262x904 -->
	  <img src="Screen Shot 2023-02-22 at 15.43.22_thumb.jpg" width="800" height="320" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>学習の結果
      <center>
	<a href="Screen Shot 2023-02-22 at 15.44.23.png"><!-- 2262x1171 -->
	  <img src="Screen Shot 2023-02-22 at 15.44.23_thumb.jpg" width="800" height="414" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 15.44.27.png"><!-- 2257x458 -->
	  <img src="Screen Shot 2023-02-22 at 15.44.27_thumb.jpg" width="800" height="162" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>もっと学習させた結果
      <center>
	<a href="Screen Shot 2023-02-22 at 15.46.41.png"><!-- 2258x983 -->
	  <img src="Screen Shot 2023-02-22 at 15.46.41_thumb.jpg" width="800" height="348" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 15.45.06.png"><!-- 2258x1171 -->
	  <img src="Screen Shot 2023-02-22 at 15.45.06_thumb.jpg" width="800" height="415" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-gpt-from-scratch-summary">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      GPT をスクラッチから実装<br />
      まとめ
    </div>
  </center>

  <ul>
    <li>GPT (Transformer の decoder 部分）のコードは全部書いて、動作確認できた</li>
    <li>subword タイプの tokenizer である <tt>tiktoken</tt> を使ってみた</li>
    <li>Language Model を、いくつかのセータセットに対して（簡単に）学習させてみた
      <ul>
	<li>RNN (LSTM) の時に受けた印象を、ことさら上回るのもは（残念ながら）
	  感じなかった</li>
	<li>しかし architecture を考えると、<br />
	  RNN 部分に本質はなく、<br />
	  attention の方できちんと学習すれば、いいのだ、ということは分かった</li>
      </ul>
    </li>
    <li>この architecture の単純さは Transformer の「スケーリング」に
      つながっているのだろう
      <ul>
	<li>つまり、今の小さいデータセットで、小さいモデルサイズでみると<br />
	  RNN 世代のものと質的に違いを感じられないが、</li>
	<li>大きなデータセット、
	  大きなモデルサイズにスケールアップすれば、
	  そのまま性能もスケールアップしていく</li>
	<li>そのことは、実は、結局、リソースを持っているものが圧倒的に有利なゲーム、
	  ということなんだなぁ</li>
      </ul>
    </li>
  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-misc">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      その他の情報
    </div>
  </center>

  <ul>
    <li>GPT-2
      <ul>
	<li>OpenAI: <a href="https://openai.com/blog/better-language-models/">Better Language Models and Their Implications (February 14, 2019)</a>
	</li>
	<li>paper: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">PDF</a>
	  (<a href="GPT2-language_models_are_unsupervised_multitask_learners.pdf">local copy</a>)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.30.17.png"><!-- 2215x810 -->
	      <img src="Screen Shot 2023-02-20 at 19.30.17_thumb.jpg" width="800" height="293" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>GPT-3
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/2005.14165">2005.14165</a>
	  (<a href="arxiv-2005.14165-gpt3.pdf">local copy</a>)
	  <br />
	  Language Models are Few-Shot Learners
	  (Tom B. Brown, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.30.26.png"><!-- 2205x846 -->
	      <img src="Screen Shot 2023-02-20 at 19.30.26_thumb.jpg" width="800" height="307" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- GPT-3 -->


    <li>Transformer カタログ<br />
      <a href="https://twitter.com/xamat/status/1626081981246341120">https://twitter.com/xamat/status/1626081981246341120</a>
      <pre>
My Transformers Catalog has become one of my most popular posts ever.
Some of you told me that you turned into a pdf for easier reading.
I thought I should make it into an arXiv preprint. Here you go:
60 Transformers in 36 pages 🤖 🎉 

<a href="https://arxiv.org/abs/2302.07730">https://arxiv.org/abs/2302.07730</a>
      </pre>
      <center>
	<a href="Screen Shot 2023-02-17 at 11.11.07.png"><!-- 1182x951 -->
	  <img src="Screen Shot 2023-02-17 at 11.11.07_thumb.jpg" width="400" height="322" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/2302.07730">2302.07730</a>
	  (<a href="arxiv-2302.07730-catalog.pdf">local copy</a>)
	  <br />
	  Transformer models: an introduction and catalog
	  (Xavier Amatriain)
	  <center>
	    <a href="Screen Shot 2023-02-17 at 11.05.13.png"><!-- 2124x930 -->
	      <img src="Screen Shot 2023-02-17 at 11.05.13_thumb.jpg" width="800" height="350" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-17 at 11.05.39.png"><!-- 2117x948 -->
	      <img src="Screen Shot 2023-02-17 at 11.05.39_thumb.jpg" width="800" height="358" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-17 at 11.06.14.png"><!-- 2117x974 -->
	      <img src="Screen Shot 2023-02-17 at 11.06.14_thumb.jpg" width="800" height="368" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>


    <li><a href="https://twitter.com/Yamkaz/status/1625639281430786049">https://twitter.com/Yamkaz/status/1625639281430786049</a>
      <center>
	<a href="Screen Shot 2023-02-20 at 18.57.14.png"><!-- 1192x845 -->
	  <img src="Screen Shot 2023-02-20 at 18.57.14_thumb.jpg" width="400" height="284" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
30分で完全理解するTransformerの世界
      </pre>
      <ul>
	<li><a href="https://zenn.dev/zenkigen/articles/2023-01-shimizu">30分で完全理解するTransformerの世界</a> (株式会社ZENKIGEN)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 18.57.24.png"><!-- 2747x1064 -->
	      <img src="Screen Shot 2023-02-20 at 18.57.24_thumb.jpg" width="800" height="310" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>


    <li>スケーリング
      <ul>
	<li><a href="https://twitter.com/ImAI_Eruel/status/1627191853983612928">https://twitter.com/ImAI_Eruel/status/1627191853983612928</a>
	  <center>
	    <a href="Screen Shot 2023-02-20 at 18.14.24.png"><!-- 1182x567 -->
	      <img src="Screen Shot 2023-02-20 at 18.14.24_thumb.jpg" width="400" height="192" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
今思うと、この時のツイートに
ChatGPT登場以降のゲームのルールが凝縮されていた感じがあります
	  </pre>
	  <ul>
	    <li><a href="https://twitter.com/ImAI_Eruel/status/1346273648651321344">https://twitter.com/ImAI_Eruel/status/1346273648651321344</a>
	      <center>
		<a href="Screen Shot 2023-02-20 at 18.14.31.png"><!-- 1183x1306 -->
		  <img src="Screen Shot 2023-02-20 at 18.14.31_thumb.jpg" width="362" height="400" style="border: 2px #ccc solid;" /></a>
	      </center>
	      <pre>
議論の余地はあるとはいえ,かなり凄いことが書かれてます
・Transformerの性能はたった3つの変数のべき乗則に支配される
・理論上,3つの変数を上げ続ければTransformerの性能は無限に上昇
・Transformerを利用する多くの分野が「お金をどれだけ払えるか？」の問題になる可能性
	      </pre>
	    </li>

	    <li>ディープラーニングブログ (Ryobot)<br />
	      <a href="https://deeplearning.hatenablog.com/entry/scaling_law">OpenAIが発見したScaling Lawの秘密 (2021-01-05)</a>
	      <center>
		<a href="Screen Shot 2023-02-20 at 18.16.43.png"><!-- 953x1243 -->
		  <img src="Screen Shot 2023-02-20 at 18.16.43_thumb.jpg" width="307" height="400" style="border: 2px #ccc solid;" /></a>
		<br />
		<a href="Screen Shot 2023-02-20 at 18.17.29.png"><!-- 1721x594 -->
		  <img src="Screen Shot 2023-02-20 at 18.17.29_thumb.jpg" width="800" height="276" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	  </ul>
	</li>

	<li>先の「<a href="https://zenn.dev/zenkigen/articles/2023-01-shimizu">30分で完全理解するTransformerの世界</a>」にも言及がありましたね
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.02.04.png"><!-- 2747x882 -->
	      <img src="Screen Shot 2023-02-20 at 19.02.04_thumb.jpg" width="800" height="257" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>arxiv: <a href="https://arxiv.org/abs/2001.08361">2001.08361</a>
	  (<a href="arxiv-2001.08361-scaling-laws-for-LM.pdf">local copy</a>)
	  <br />
	  Scaling Laws for Neural Language Models<br />
	  (Jared Kaplan, et al. OpenAI)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 18.43.09.png"><!-- 2209x693 -->
	      <img src="Screen Shot 2023-02-20 at 18.43.09_thumb.jpg" width="800" height="251" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-20 at 18.43.48.png"><!-- 2203x524 -->
	      <img src="Screen Shot 2023-02-20 at 18.43.48_thumb.jpg" width="800" height="190" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/2010.14701">2010.14701</a>
	  (<a href="arxiv-2010.14701-scaling-laws-for-AR.pdf">local copy</a>)
	  <br />
	  Scaling Laws for Autoregressive Generative Modeling<br />
	  (Tom Henighan, et al. OpenAI)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 18.43.56.png"><!-- 2204x662 -->
	      <img src="Screen Shot 2023-02-20 at 18.43.56_thumb.jpg" width="800" height="240" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-20 at 18.44.41.png"><!-- 2203x633 -->
	      <img src="Screen Shot 2023-02-20 at 18.44.41_thumb.jpg" width="800" height="230" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li><!-- スケーリング -->


    <li><a href="https://twitter.com/rasbt/status/1626588781594955776">https://twitter.com/rasbt/status/1626588781594955776</a>
      <center>
	<a href="Screen Shot 2023-02-20 at 18.53.44.png"><!-- 1187x638 -->
	  <img src="Screen Shot 2023-02-20 at 18.53.44_thumb.jpg" width="400" height="215" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
GPT in 60 Lines of NumPy
-- a simple yet complete technical introduction to the GPT
as an educational tool.

<a href="https://jaykmody.com/blog/gpt-from-scratch/">https://jaykmody.com/blog/gpt-from-scratch/</a>

I love this!
It reminds me why coding in Python can be so much fun!
      </pre>
      <ul>
	<li>Jay Mody: <a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of NumPy (January 30, 2023)</a>
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.15.37.png"><!-- 2783x579 -->
	      <img src="Screen Shot 2023-02-20 at 19.15.37_thumb.jpg" width="800" height="166" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>「Numpy で GPT を 60 行で」というのは、 inference 部分</li>
	<li>学習するには Numpy を JAX にして（するだけで、いいのか！）
	  できる（らしい）</li>
      </ul>
    </li>


    <li>
      <pre>
MetaFormer
-----------

https://arxiv.org/abs/2111.11418
MetaFormer Is Actually What You Need for Vision
Weihao Yu et al.



Toolformer
-----------

https://arxiv.org/abs/2302.04761
Toolformer: Language Models Can Teach Themselves to Use Tools
Timo Schick et al.

      </pre>
      <ul>
	<li><a href="https://twitter.com/Yamkaz/status/1627106567350022147">https://twitter.com/Yamkaz/status/1627106567350022147</a>
	  <center>
	    <a href="Screen Shot 2023-02-20 at 18.49.49.png"><!-- 1185x969 -->
	      <img src="Screen Shot 2023-02-20 at 18.49.49_thumb.jpg" width="400" height="327" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
すごい！！
自律的にAPIを呼び出し情報を得るToolformerを、
微調整なし、Zero/Few-shotプロンプトで再現した
「Toolformer zero」が公開
<a href="http://toolformerzero.com">http://toolformerzero.com</a>
<a href="https://github.com/minosvasilias/toolformer-zero">https://github.com/minosvasilias/toolformer-zero</a>
	  </pre>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-chatgpt">
    <div style="font-size: 50px; font-weight: bolder;">
      ChatGPT とは？
    </div>
    <br /><br />
  </center>

  <ul>
    <li>ChatGPT どれほどのものよ？（技術的な話）</li>

    <li>Instruct GPT
      <center>
	<a href="Screen Shot 2023-02-16 at 19.06.36.png"><!-- 2723x1216 -->
	  <img src="Screen Shot 2023-02-16 at 19.06.36_thumb.jpg" width="800" height="357" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-16 at 19.06.42.png"><!-- 2744x1301 -->
	  <img src="Screen Shot 2023-02-16 at 19.06.42_thumb.jpg" width="800" height="379" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>OpenAI: <a href="https://openai.com/blog/instruction-following/">Aligning Language Models to Follow Instructions (January 27, 2022)</a>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/2203.02155">2203.02155</a>
	  (<a href="arxiv-2203.02155-instruct-gpt.pdf">local copy</a>)
	  <br />
	  Training language models to follow instructions with human feedback<br />
	  (Long Ouyang, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.21.56.png"><!-- 2207x699 -->
	      <img src="Screen Shot 2023-02-20 at 19.21.56_thumb.jpg" width="800" height="253" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- Instruct GPT -->

    <li>ChatGPT
      <center>
	<a href="Screen Shot 2023-02-16 at 19.09.54.png"><!-- 2739x1055 -->
	  <img src="Screen Shot 2023-02-16 at 19.09.54_thumb.jpg" width="800" height="308" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-16 at 19.10.01.png"><!-- 2744x1123 -->
	  <img src="Screen Shot 2023-02-16 at 19.10.01_thumb.jpg" width="800" height="327" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-16 at 19.10.22.png"><!-- 2725x858 -->
	  <img src="Screen Shot 2023-02-16 at 19.10.22_thumb.jpg" width="800" height="252" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-16 at 19.10.30.png"><!-- 2756x1421 -->
	  <img src="Screen Shot 2023-02-16 at 19.10.30_thumb.jpg" width="800" height="412" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>OpenAI: <a href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue</a>
	</li>
      </ul>
    </li><!-- ChatGPT -->


    <li>PPO
      <center>
	<a href="Screen Shot 2023-02-16 at 19.10.43.png"><!-- 2758x1419 -->
	  <img src="Screen Shot 2023-02-16 at 19.10.43_thumb.jpg" width="800" height="412" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>OpenAI: <a href="https://openai.com/blog/openai-baselines-ppo/">Proximal Policy Optimization</a>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1707.06347">1707.06347</a>
	  (<a href="arxiv-1707.06347-ppo.pdf">local copy</a>)
	  <br />
	  Proximal Policy Optimization Algorithms<br />
	  (John Schulman, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.23.16.png"><!-- 2199x674 -->
	      <img src="Screen Shot 2023-02-20 at 19.23.16_thumb.jpg" width="800" height="245" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- PPO -->

  </ul>
  
  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-rlhf">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      HuggingFace の RLHF
    </div>
  </center>

  <ul>
    <li>HuggingFace: <a href="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a>
      <center>
        <a href="Screen Shot 2023-02-20 at 20.38.35.png"><!-- 2814x840 -->
         <img src="Screen Shot 2023-02-20 at 20.38.35_thumb.jpg" width="800" height="239" style="border: 2px #ccc solid;" /></a>
        <a href="Screen Shot 2023-02-20 at 20.38.54.png"><!-- 2799x764 -->
         <img src="Screen Shot 2023-02-20 at 20.38.54_thumb.jpg" width="800" height="218" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>(1) Pretraining language models
      <center>
      </center>
    </li>

    <li>(2) Reward model (RM) training
      <center>
      </center>
    </li>

    <li>(3) Fine-tuning with RL
      <center>
      </center>
    </li>

    <li>Open-source tools for RLHF
      <ul>
	<li>PyTorch でのもの
	  <ul>
      	    <li>TRL - Transformer Reinforcement Learning
      	      (<a href="https://github.com/lvwerra/trl">https://github.com/lvwerra/trl</a>)
	      <ul>
		<li>HuggingFace ecosystem 内の LMs に対して
		  PPO で fine-tuning できる</li>
	      </ul>
      	    </li>
      	    <li>TRLX (<a href="https://github.com/CarperAI/trlx">https://github.com/CarperAI/trlx</a>)
	      <ul>
		<li>TRL の拡張で、より大きな LMs に対して
		  Online/Offline 学習できる</li>
		<li>PPO に加え、 Implicit Language Q-Learning
		  (ILQL - <a href="https://sea-snell.github.io/ILQL_site/">https://sea-snell.github.io/ILQL_site/</a>)
		  も使える</li>
	      </ul>
      	    </li>
	    <li>RL4LMs (<a href="https://github.com/allenai/RL4LMs">https://github.com/allenai/RL4LMs</a>)
	      <ul>
		<li>よりたくさんの RL algorithms (PPO の他、 NLOP, A2C, TRPO)、
		  reward functions や metrics が使える</li>
	      </ul>
	    </li>
	  </ul>
	</li>

      </ul>
    </li>

  </ul>
  
  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-ngyuen">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      HuggingFace の RLHF に対するコメント
      <br />
      by Khanh X. Nguyen
    </div>
  </center>

  <ul>
    <li><a href="https://twitter.com/khanhxuannguyen/status/1627446913275404289">https://twitter.com/khanhxuannguyen/status/1627446913275404289</a>
      <center>
	<a href="Screen Shot 2023-02-20 at 12.54.40.png"><!-- 1181x1091 -->
	  <img src="Screen Shot 2023-02-20 at 12.54.40_thumb.jpg" width="400" height="370" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
The RLHF page of HuggingFace (<a href="http://huggingface.co/blog/rlhf">http://huggingface.co/blog/rlhf</a>)
misses many important citations.
Here are some classical RLHF papers that you should cite and why.

(0/7) To some people, RLHF means
"learn a reward model from human rankings and RL on it".
But the term literally conveys a much broader meaning:
any RL method that can learn from any type of human scalar feedback.

(1/7) In terms of RL for text gen,
cite Ranzato+15 (<a href="http://arxiv.org/abs/1511.06732">http://arxiv.org/abs/1511.06732</a>)
and Shen+ (<a href="http://arxiv.org/abs/1512.02433">http://arxiv.org/abs/1512.02433</a>)
who pioneer training text generators to optimize rewards,
and Bahdanau+17 (<a href="https://arxiv.org/abs/1607.07086">https://arxiv.org/abs/1607.07086</a>)
who attempt the first actor-critic solution.

(2/7) In those works,
rewards given to the model were dense and computed automatically (BLEU).
Sokolov+15,16,17 (<a href="http://arxiv.org/abs/1601.04468">http://arxiv.org/abs/1601.04468</a>,
<a href="http://arxiv.org/abs/1606.00739">http://arxiv.org/abs/1606.00739</a>,
<a href="https://aclanthology.org/P16-1152/">aclanthology.org/P16-1152</a>)
is one of the first to really think about learning from human ratings,
modeling the problem as bandit learning.

(3/7) "Bandit" is important because
naturally you could only ask a human to give one rating for a whole text.
Sokolov formulation characterizes how difficult the problem is
compared to video-game dense-reward RL problems.

(4/7) Our 2017 paper (<a href="http://arxiv.org/abs/1707.07402">http://arxiv.org/abs/1707.07402</a>)
is first to present and simulate the risk of using
user ratings for training text generators.
People have different opinions; one's opinion varies over time.
We show RL is robust to granularity, skew in rewards but not variance.

(5/7) Julia Kreutzer is a veteran on this topic.
She authors so many papers that analyze the feasbility of
learning translation systems from human feedback
(those with Sokolov,
and <a href="http://arxiv.org/abs/1804.05958">http://arxiv.org/abs/1804.05958</a>,
<a href="http://arxiv.org/abs/1805.01553">http://arxiv.org/abs/1805.01553</a>,
<a href="https://arxiv.org/abs/2011.02511">arxiv.org/abs/2011.02511</a>).

(6/7) All of these works happened before or around the time of
Christiano+17 (<a href="http://arxiv.org/abs/1706.03741">http://arxiv.org/abs/1706.03741</a>)
who introduce the now well-known method for learning from rankings,
and Stiennon+20 (<a href="https://arxiv.org/abs/2009.01325">https://arxiv.org/abs/2009.01325</a>)
who apply the method with real humans on text summarization.

(7/7) I hope this tweet conveys a better snapshot of the history of RLHF.
Thanks for reading :)

I should say that the scope of this tweet is text gen.
The history of RL from humans of course dates way further back than this
(e.g. TAMER by Knox and Stone, Littman et al., etc.)
      </pre>
    </li>

    <li>papers:
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/1511.06732">1511.06732</a>
	  (<a href="arxiv-1511.06732-RLHF.pdf">local copy</a>)<br />
	  Sequence Level Training with Recurrent Neural Networks
	  (Marc'Aurelio Ranzato, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.47.53.png"><!-- 2204x412 -->
	      <img src="Screen Shot 2023-02-20 at 19.47.53_thumb.jpg" width="800" height="150" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1512.02433">1512.02433</a>
	  (<a href="arxiv-1512.02433-RLHF.pdf">local copy</a>)<br />
	  Minimum Risk Training for Neural Machine Translation
	  (Shiqi Shen, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.49.01.png"><!-- 2196x377 -->
	      <img src="Screen Shot 2023-02-20 at 19.49.01_thumb.jpg" width="800" height="137" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1607.07086">1607.07086</a>
	  (<a href="arxiv-1607.07086-RLHF.pdf">local copy</a>)<br />
	  An Actor-Critic Algorithm for Sequence Prediction
	  (Dzmitry Bahdanau, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.50.11.png"><!-- 2209x520 -->
	      <img src="Screen Shot 2023-02-20 at 19.50.11_thumb.jpg" width="800" height="188" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1601.04468">1601.04468</a>
	  (<a href="arxiv-1601.04468-sokolov.pdf">local copy</a>)<br />
	  Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation
	  (Artem Sokolov, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.51.39.png"><!-- 2206x453 -->
	      <img src="Screen Shot 2023-02-20 at 19.51.39_thumb.jpg" width="800" height="164" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1606.00739">1606.00739</a>
	  (<a href="arxiv-1606.00739-sokolov.pdf">local copy</a>)<br />
	  Stochastic Structured Prediction under Bandit Feedback
	  (Artem Sokolov, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.52.35.png"><!-- 2207x473 -->
	      <img src="Screen Shot 2023-02-20 at 19.52.35_thumb.jpg" width="800" height="171" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://aclanthology.org/P16-1152/">aclanthology.org/P16-1152</a>
	  (<a href="aclanthology-P16-1152-sokolov.pdf">local copy</a>)<br />
	  Learning Structured Predictors from Bandit Feedback for Interactive NLP
	  (Artem Sokolov, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.42.35.png"><!-- 2203x385 -->
	      <img src="Screen Shot 2023-02-20 at 19.42.35_thumb.jpg" width="800" height="140" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1707.07402">1707.07402</a>
	  (<a href="arxiv-1707.07402-RLHF.pdf">local copy</a>)<br />
	  Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback
	  (Khanh Nguyen, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.54.29.png"><!-- 2207x336 -->
	      <img src="Screen Shot 2023-02-20 at 19.54.29_thumb.jpg" width="800" height="122" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1804.05958">1804.05958</a>
	  (<a href="arxiv-1804.05958-RLHF.pdf">local copy</a>)<br />
	  Can Neural Machine Translation be Improved with User Feedback?
	  (Julia Kreutzer, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.55.45.png"><!-- 2200x303 -->
	      <img src="Screen Shot 2023-02-20 at 19.55.45_thumb.jpg" width="800" height="110" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1805.01553">1805.01553</a>
	  (<a href="arxiv-1805.01553-RLHF.pdf">local copy</a>)<br />
	  A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation
	  (Tsz Kin Lam, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.56.52.png"><!-- 2196x316 -->
	      <img src="Screen Shot 2023-02-20 at 19.56.52_thumb.jpg" width="800" height="115" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/2011.02511">2011.02511</a>
	  (<a href="arxiv-2011.02511-RLHF.pdf"></a>)<br />
	  Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks
	  (Julia Kreutzer, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.58.08.png"><!-- 2206x399 -->
	      <img src="Screen Shot 2023-02-20 at 19.58.08_thumb.jpg" width="800" height="145" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/1706.03741">1706.03741</a>
	  (<a href="arxiv-1706.03741-RLHF.pdf"></a>)<br />
	  Deep reinforcement learning from human preferences
	  (Paul Christiano, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 19.59.21.png"><!-- 2203x514 -->
	      <img src="Screen Shot 2023-02-20 at 19.59.21_thumb.jpg" width="800" height="187" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>arxiv: <a href="https://arxiv.org/abs/2009.01325">2009.01325</a>
	  (<a href="arxiv-2009.01325-RLHF.pdf">local copy</a>)<br />
	  Learning to summarize from human feedback
	  (Nisan Stiennon, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-20 at 20.00.26.png"><!-- 2205x446 -->
	      <img src="Screen Shot 2023-02-20 at 20.00.26_thumb.jpg" width="800" height="162" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li>

  </ul>
  
  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>


    <li>Mathematica で有名な Stephen Wolfram のポストに対する意見
      <ul>
	<li><a href="https://twitter.com/jeremyphoward/status/1627481905125003264">https://twitter.com/jeremyphoward/status/1627481905125003264</a>
	  <center>
	    <a href="Screen Shot 2023-02-20 at 10.59.23.png"><!-- 1190x896 -->
	      <img src="Screen Shot 2023-02-20 at 10.59.23_thumb.jpg" width="400" height="301" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
It's wrong though.
By failing to mention instruction tuning / RLHF
it gives completely the wrong understanding of what's actually going on.

ChatGPT isn't an LLM.
Yes, it's based on one,
but instructions tuning turns it into something very different.
	  </pre>
	</li>
      </ul>
    </li>


  </ul>
  
  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2">
    <div style="font-size: 50px; font-weight: bolder;">
      パート２
      <br />
      音声合成
    </div>
    <br /><br />
  </center>

  <ul>
    <li>前回 (<a href="https://hello-ai-forum.github.io/pages/ZAF202301/ichiki/#part1-espnet">ZAF-2301</a>)
      <center>
	<a href="Screen Shot 2023-02-21 at 22.07.24.png"><!-- 2802x1214 -->
	  <img src="Screen Shot 2023-02-21 at 22.07.24_thumb.jpg" width="800" height="347" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 22.07.32.png"><!-- 2808x1271 -->
	  <img src="Screen Shot 2023-02-21 at 22.07.32_thumb.jpg" width="800" height="362" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 22.07.41.png"><!-- 2790x370 -->
	  <img src="Screen Shot 2023-02-21 at 22.07.41_thumb.jpg" width="800" height="106" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>要するに
	  <center>
	    <br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      ３秒の音声を準備すれば、
	      <br />
	      自分の声で喋ってくれるという VALL-E
	      <br /><br />
	    </div>
	  </center>
	  しかし、まだ実際に使うことができなかった……<br />
	  とガッカリしたけど、それでも、なんとかならないの？と調べた結果、
	  <center>
	    <br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      ESPnet2
	      <br /><br />
	    </div>
	  </center>
	  というものを使えば、できそうだ、と言うところまで分かった。
	</li>

	<li>今回は、そのつづき。</li>
      </ul>
    </li>

    <li>【情報１】<br />
      もみあげコレクション
      「<a href="https://momicolle.hatenablog.com/entry/2022/12/15/182519">ESPnetで高森藍子の声を錬成した (2022-12-15)</a>」
      <center>
	<a href="Screen Shot 2023-02-21 at 22.07.55.png"><!-- 2789x891 -->
	  <img src="Screen Shot 2023-02-21 at 22.07.55_thumb.jpg" width="800" height="256" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 22.08.04.png"><!-- 1850x693 -->
	  <img src="Screen Shot 2023-02-21 at 22.08.04_thumb.jpg" width="800" height="300" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>ここに、 ESPnet2 を使って、データセットを使って、
	  音声合成モデルを作った話が紹介してある</li>
	<li>読んでみると
	  <center>
	    <a href="Screen Shot 2023-02-21 at 22.09.46.png"><!-- 2180x647 -->
	      <img src="Screen Shot 2023-02-21 at 22.09.46_thumb.jpg" width="800" height="237" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>この辺りでいろいろ情報を書いてくれている npaka さんが、
	  ESPnet2 について紹介しているらしい。</li>
	<li>

      </ul>
    </li>

    <li>【情報２】<br />
      <a href="https://note.com/npaka/n/nc51f2c5c4eb5">ESPnet 入門 - 音声合成 (npaka 2020年11月14日 17:17)</a>
      <center>
	<a href="Screen Shot 2023-02-21 at 22.12.15.png"><!-- 1850x824 -->
	  <img src="Screen Shot 2023-02-21 at 22.12.15_thumb.jpg" width="800" height="356" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>ESPnet は ASR （自動音声認識）と TTS （テキスト音声合成）の両方をカバー</li>
	<li>TTS の方法は、音響モデルとボコーダーに分かれるらしい
	  <center>
	    <a href="Screen Shot 2023-02-21 at 22.13.47.png"><!-- 1919x908 -->
	      <img src="Screen Shot 2023-02-21 at 22.13.47_thumb.jpg" width="800" height="379" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 22.13.52.png"><!-- 1862x1035 -->
	      <img src="Screen Shot 2023-02-21 at 22.13.52_thumb.jpg" width="800" height="445" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 22.14.03.png"><!-- 1824x892 -->
	      <img src="Screen Shot 2023-02-21 at 22.14.03_thumb.jpg" width="800" height="391" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>【情報１】に戻る
      <ul>
	<li>そこでは、最初 Tacotron と ParallelWaveGAN を使ったが、<br />
	  その後、新しいモデル VITS を使った、とある
	  <center>
	    <a href="Screen Shot 2023-02-21 at 23.18.42.png"><!-- 2222x948 -->
	      <img src="Screen Shot 2023-02-21 at 23.18.42_thumb.jpg" width="800" height="341" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>【情報３】
      github: <a href="https://github.com/jaywalnut310/vits">jaywalnut310/vits</a><br />
      VITS: Conditional Variational Autoencoder with Adversarial Learning
      for End-to-End Text-to-Speech<br />
      (Jaehyeon Kim, Jungil Kong, and Juhee Son)
      <center>
	<a href="Screen Shot 2023-02-22 at 0.39.50.png"><!-- 2801x575 -->
	  <img src="Screen Shot 2023-02-22 at 0.39.50_thumb.jpg" width="800" height="164" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/2106.06103">2106.06103</a>
	  (<a href="arxiv-2106.06103-VITS.pdf">local copy</a>)<br />
	  Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech
	  (Jaehyeon Kim, et al.)
	  <center>
	    <a href="Screen Shot 2023-02-21 at 23.20.14.png"><!-- 2203x525 -->
	      <img src="Screen Shot 2023-02-21 at 23.20.14_thumb.jpg" width="800" height="191" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 23.20.22.png"><!-- 2212x868 -->
	      <img src="Screen Shot 2023-02-21 at 23.20.22_thumb.jpg" width="800" height="314" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li>

    <li>【情報１】に戻る
      <ul>
	<li>そこでは、最初 VITS をスクラッチから学習して、うまくいかなかったようだが、
	  <br />
	  その後、 ESPnet2 を使うと（簡単に） fine-tuning できることが分かったらしい。
	  <center>
	    <a href="Screen Shot 2023-02-21 at 23.21.54.png"><!-- 2201x753 -->
	      <img src="Screen Shot 2023-02-21 at 23.21.54_thumb.jpg" width="800" height="274" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>ということで、 ESPnet2 をセットアップして、実際に試してみることにする</li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2">
    <div style="font-size: 50px; font-weight: bolder;">
      ESPnet2
    </div>
    <br /><br />
  </center>

  <ul>
    <li>本家の情報
      <ul>
	<li>github: <a href="https://github.com/espnet/espnet">espnet/espnet</a>
	  <center>
	    <a href="Screen Shot 2023-02-22 at 0.47.04.png"><!-- 2800x1295 -->
	      <img src="Screen Shot 2023-02-22 at 0.47.04_thumb.jpg" width="800" height="370" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>Qiita: <a href="https://qiita.com/kan-bayashi/items/0371e06202641dbfa0ad">ESPnet2で始めるEnd-to-Endテキスト音声合成</a><br />
	  (@kan-bayashi posted at 2020-09-19 updated at 2020-09-19)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 0.48.23.png"><!-- 1995x708 -->
	      <img src="Screen Shot 2023-02-22 at 0.48.23_thumb.jpg" width="800" height="284" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
ESPnet 開発チームの一員の林知樹さん自身による Qiita への解説記事
音声合成のモデルに VITS が振れられてない（導入される前の記事かな？）
	  </pre>
	</li>
	<li><a href="https://kan-bayashi.github.io/asj-espnet2-tutorial/">ESPnet2で始めるEnd-to-End音声処理</a><br />
	  (林 知樹 Sep 15, 2020)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 1.33.48.png"><!-- 2796x762 -->
	      <img src="Screen Shot 2023-02-22 at 1.33.48_thumb.jpg" width="800" height="218" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 1.33.54.png"><!-- 2788x1023 -->
	      <img src="Screen Shot 2023-02-22 at 1.33.54_thumb.jpg" width="800" height="294" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 1.34.25.png"><!-- 2793x1287 -->
	      <img src="Screen Shot 2023-02-22 at 1.34.25_thumb.jpg" width="800" height="369" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>ESPnet2 で、自分でデータセット作って学習させる手順
      <ul>
	<li><a href="#part2-espnet2-install">（１）インストール</a></li>
	<li><a href="#part2-espnet2-dataset">（２） データセット準備</a></li>
	<li><a href="#part2-espnet2-training">（３） 学習</a></li>
	<li><a href="#part2-espnet2-inference">（４） 推論（音声合成）</a></li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2-install">
    <div style="font-size: 50px; font-weight: bolder;">
      （１） ESPnet2 のインストール
    </div>
    <br /><br />
  </center>

  <ul>
    <li><a href="https://ssohsn.fanbox.cc/posts/3377355">【全体公開】 誰でも簡単に高品質な音声合成を作れる方法について考える【機械学習】</a><br />
      (February 6th, 2022 11:03)
      <center>
	<a href="Screen Shot 2023-02-21 at 23.35.03.png"><!-- 1821x1144 -->
	  <img src="Screen Shot 2023-02-21 at 23.35.03_thumb.jpg" width="800" height="503" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>少し古いかな？
	  <pre>
現在では誰でも自分の声を音声合成にできる
【MYCOE(https://coeiroink.com/mycoeiroink)】が公開されており、
サポートもしっかりされているので
自分の声を音声合成にしたい場合は、こちらをおすすめしています。
	  </pre>
	</li>
	<li>この記事の
	  <center>
	    <a href="Screen Shot 2023-02-21 at 23.41.14.png"><!-- 2130x536 -->
	      <img src="Screen Shot 2023-02-21 at 23.41.14_thumb.jpg" width="800" height="201" style="border: 2px #ccc solid;" /></a>
	  </center>
	  で紹介されている colab ノートブックを参考にする。
	</li>

      </ul>
    </li>

    <li>google colab: <a href="https://colab.research.google.com/drive/12buuborkXNFoef1UptjPr0jyw9qLc12u#scrollTo=qVpwwHigwGui">ESPnetTest6.ipynb</a><br />
      「ESPnetをターミナルを使わずに回してみる」
      <center>
	<a href="Screen Shot 2023-02-22 at 0.57.07.png"><!-- 2816x1353 -->
	  <img src="Screen Shot 2023-02-22 at 0.57.07_thumb.jpg" width="800" height="384" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>


    <li>Jupyter Notebook のターミナルから、インストール
      <center>
	<a href="Screen Shot 2023-02-21 at 17.54.20.png"><!-- 2811x1309 -->
	  <img src="Screen Shot 2023-02-21 at 17.54.20_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 17.58.32.png"><!-- 2811x1304 -->
	  <img src="Screen Shot 2023-02-21 at 17.58.32_thumb.jpg" width="800" height="371" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 18.04.42.png"><!-- 2809x1311 -->
	  <img src="Screen Shot 2023-02-21 at 18.04.42_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 18.04.55.png"><!-- 2810x1303 -->
	  <img src="Screen Shot 2023-02-21 at 18.04.55_thumb.jpg" width="800" height="371" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 18.05.11.png"><!-- 2810x1304 -->
	  <img src="Screen Shot 2023-02-21 at 18.05.11_thumb.jpg" width="800" height="371" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-21 at 18.05.13.png"><!-- 2812x1301 -->
	  <img src="Screen Shot 2023-02-21 at 18.05.13_thumb.jpg" width="800" height="370" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2-dataset">
    <div style="font-size: 50px; font-weight: bolder;">
      （２） データセットの準備
    </div>
    <br /><br />
  </center>

  <ul>
    <li>Qiita: <a href="https://qiita.com/3253/items/58260fa3388cd0ebad67">[Whisper+ESPnet2(VITS)]音声合成システムを作った備忘録</a><br />
      (@3253 posted at 2022-10-28 updated at 2022-11-02)
      <center>
	<a href="Screen Shot 2023-02-21 at 23.32.41.png"><!-- 1991x671 -->
	  <img src="Screen Shot 2023-02-21 at 23.32.41_thumb.jpg" width="800" height="270" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>ここでは、 OpenAI の書き起こしモデル Whisper を使って、<br />
	  （ラベルなしの）音声ファイルだけから、
	  EPSnet の学習に必要なデータセットを作る手順が紹介されている</li>
      </ul>
    </li>
    <li>この記事を参考にして、以下のように、 ZAF-2301 の音声情報からデータセットを作った
      <center>
	<a href="Screen Shot 2023-02-22 at 1.06.45.png"><!-- 2808x511 -->
	  <img src="Screen Shot 2023-02-22 at 1.06.45_thumb.jpg" width="800" height="146" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 1.07.03.png"><!-- 2802x1001 -->
	  <img src="Screen Shot 2023-02-22 at 1.07.03_thumb.jpg" width="800" height="286" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 1.07.17.png"><!-- 2807x615 -->
	  <img src="Screen Shot 2023-02-22 at 1.07.17_thumb.jpg" width="800" height="175" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 1.07.26.png"><!-- 2808x801 -->
	  <img src="Screen Shot 2023-02-22 at 1.07.26_thumb.jpg" width="800" height="228" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 1.07.31.png"><!-- 2800x763 -->
	  <img src="Screen Shot 2023-02-22 at 1.07.31_thumb.jpg" width="800" height="218" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 1.07.35.png"><!-- 2800x456 -->
	  <img src="Screen Shot 2023-02-22 at 1.07.35_thumb.jpg" width="800" height="130" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2-training">
    <div style="font-size: 50px; font-weight: bolder;">
      （３） 学習
    </div>
    <br /><br />
  </center>

  <ul>
    <li>モデルの学習は、<br />
      先に紹介した google colab
      <center>
	<a href="Screen Shot 2023-02-22 at 0.57.07.png"><!-- 2816x1353 -->
	  <img src="Screen Shot 2023-02-22 at 0.57.07_thumb.jpg" width="800" height="384" style="border: 2px #ccc solid;" /></a>
      </center>
      及び、以下の記事<br />
      Qiita: <a href="https://qiita.com/RRR_troisR/items/6288b9bdc6e725aa8440">ESPNetで作るキャラクター音声合成</a><br />
      (@RRR_troisR posted at 2021-12-31 updated at 2022-07-09)
      <center>
	<a href="Screen Shot 2023-02-21 at 23.25.09.png"><!-- 1984x594 -->
	  <img src="Screen Shot 2023-02-21 at 23.25.09_thumb.jpg" width="800" height="240" style="border: 2px #ccc solid;" /></a>
      </center>
      を参考に、進めた。</li>


    <li>この記事の最初の部分（セットアップ）は、
      既に<a href="#part2-espnet2-install">（１）インストール</a>のところで実行済
      <center>
	<a href="Screen Shot 2023-02-22 at 1.23.04.png"><!-- 1989x1200 -->
	  <img src="Screen Shot 2023-02-22 at 1.23.04_thumb.jpg" width="800" height="483" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>具体的には以下
	  <center>
	    <a href="Screen Shot 2023-02-21 at 18.08.12.png"><!-- 2809x1308 -->
	      <img src="Screen Shot 2023-02-21 at 18.08.12_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 18.12.09.png"><!-- 2812x1311 -->
	      <img src="Screen Shot 2023-02-21 at 18.12.09_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 18.15.36.png"><!-- 2811x1312 -->
	      <img src="Screen Shot 2023-02-21 at 18.15.36_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>このあと、TTS レシピの step 1 から step 5 に相当する、
      学習のための準備を行う
      <center>
	<a href="Screen Shot 2023-02-22 at 1.34.25.1.png"><!-- 2793x1287 -->
	  <img src="Screen Shot 2023-02-22 at 1.34.25.1_thumb.jpg" width="800" height="369" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>この記事では、以下の部分
	  <center>
	    <a href="Screen Shot 2023-02-22 at 1.23.41.png"><!-- 1977x1027 -->
	      <img src="Screen Shot 2023-02-22 at 1.23.41_thumb.jpg" width="800" height="416" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>ここでの本題である（３）学習は、 TTS レシピの step 6 に相当する。
      <center>
	<a href="Screen Shot 2023-02-22 at 1.34.25.2.png"><!-- 2793x1287 -->
	  <img src="Screen Shot 2023-02-22 at 1.34.25.2_thumb.jpg" width="800" height="369" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>この記事では、以下の部分
	  <center>
	    <a href="Screen Shot 2023-02-22 at 1.23.49.png"><!-- 1982x999 -->
	      <img src="Screen Shot 2023-02-22 at 1.23.49_thumb.jpg" width="800" height="403" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>ここで、使ってるマシンの GPU (RTX-2080 Ti) のメモリの関係で、
	  <center>
	    <a href="Screen Shot 2023-02-21 at 18.18.08.png"><!-- 2813x1310 -->
	      <img src="Screen Shot 2023-02-21 at 18.18.08_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 18.28.45.png"><!-- 2813x1415 -->
	      <img src="Screen Shot 2023-02-21 at 18.28.45_thumb.jpg" width="800" height="402" style="border: 2px #ccc solid;" /></a>
	  </center>
	  記事の通り batch_bins を調整する必要があった
	  <center>
	    <a href="Screen Shot 2023-02-22 at 1.23.54.png"><!-- 1966x961 -->
	      <img src="Screen Shot 2023-02-22 at 1.23.54_thumb.jpg" width="800" height="391" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>学習には１５時間ほどかかる（予定）
	  <center>
	    <a href="Screen Shot 2023-02-21 at 19.31.39.png"><!-- 2813x1423 -->
	      <img src="Screen Shot 2023-02-21 at 19.31.39_thumb.jpg" width="800" height="405" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 20.14.36.png"><!-- 2810x1416 -->
	      <img src="Screen Shot 2023-02-21 at 20.14.36_thumb.jpg" width="800" height="403" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-21 at 22.06.52.png"><!-- 2813x1421 -->
	      <img src="Screen Shot 2023-02-21 at 22.06.52_thumb.jpg" width="800" height="404" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 0.50.20.png"><!-- 2814x1421 -->
	      <img src="Screen Shot 2023-02-22 at 0.50.20_thumb.jpg" width="800" height="404" style="border: 2px #ccc solid;" /></a>
	  </center>
	  ...
	  <center>
	    <a href="Screen Shot 2023-02-22 at 2.12.10.png"><!-- 2805x1415 -->
	      <img src="Screen Shot 2023-02-22 at 2.12.10_thumb.jpg" width="800" height="404" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 8.41.06.png"><!-- 2806x1419 -->
	      <img src="Screen Shot 2023-02-22 at 8.41.06_thumb.jpg" width="800" height="405" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 10.24.11.png"><!-- 2815x1420 -->
	      <img src="Screen Shot 2023-02-22 at 10.24.11_thumb.jpg" width="800" height="404" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 11.36.30.png"><!-- 2815x1305 -->
	      <img src="Screen Shot 2023-02-22 at 11.36.30_thumb.jpg" width="800" height="371" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 11.37.12.png"><!-- 2817x1315 -->
	      <img src="Screen Shot 2023-02-22 at 11.37.12_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 11.53.35.png"><!-- 2814x1311 -->
	      <img src="Screen Shot 2023-02-22 at 11.53.35_thumb.jpg" width="800" height="373" style="border: 2px #ccc solid;" /></a>

	  </center>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2-inference">
    <div style="font-size: 50px; font-weight: bolder;">
      （４） 推論
    </div>
    <br /><br />
  </center>

  <ul>
    <li>次に、 TTS のレシピの step 7
      <center>
	<a href="Screen Shot 2023-02-22 at 1.34.25.3.png"><!-- 2793x1287 -->
	  <img src="Screen Shot 2023-02-22 at 1.34.25.3_thumb.jpg" width="800" height="369" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>この記事では、以下の部分
	  <center>
	    <a href="Screen Shot 2023-02-22 at 1.24.05.png"><!-- 1974x1034 -->
	      <img src="Screen Shot 2023-02-22 at 1.24.05_thumb.jpg" width="800" height="419" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>ここで紹介されている記事<br />
	  Qiita: <a href="https://qiita.com/seichi25/items/bb79f66d3eaa6a605c74">ESPnetの日本語TTSモデルで簡易的にアクセント指定できるスクリプトを作ってみた。</a><br />
	  (@seichi25 posted at 2021-12-05 updated at 2022-09-04)
	  <center>
	    <a href="Screen Shot 2023-02-22 at 2.02.50.png"><!-- 1990x668 -->
	      <img src="Screen Shot 2023-02-22 at 2.02.50_thumb.jpg" width="800" height="269" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>さて、実験台には（学習には使ってない）「<a href="https://music0math.wordpress.com/">音楽と数理 🎼 ♾ ポッドキャスト</a>」
      を使うことにします
      <center>
	<a href="Screen Shot 2023-02-22 at 14.34.14.png"><!-- 2761x1251 -->
	  <img src="Screen Shot 2023-02-22 at 14.34.14_thumb.jpg" width="800" height="362" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>最新回
	  <a href="https://music0math.wordpress.com/2023/02/17/%e3%80%88%e3%83%88%e3%83%bc%e3%82%af%e3%80%89%e6%9f%84%e3%81%ab%e3%82%82%e3%81%aa%e3%81%8f%e6%99%82%e4%ba%8b%e8%a9%95%e8%ab%96%e3%80%81%e3%81%aa%e3%81%a9-s02e07/">S02E07</a>
	  「〈トーク〉柄にもなく時事評論、など」
	  <center>
	    <a href="Screen Shot 2023-02-22 at 14.38.09.png"><!-- 2751x1138 -->
	      <img src="Screen Shot 2023-02-22 at 14.38.09_thumb.jpg" width="800" height="331" style="border: 2px #ccc solid;" /></a>
	  </center>
	  の終わりの方のセクションを Whisper で書き起こした文章
	  <center>
	    <a href="Screen Shot 2023-02-22 at 14.38.21.png"><!-- 2756x1202 -->
	      <img src="Screen Shot 2023-02-22 at 14.38.21_thumb.jpg" width="800" height="349" style="border: 2px #ccc solid;" /></a>
	  </center>
	  を <b>kengo</b> モデルに喋ってもらいましょう！
	</li>

	<li>ちなみに、これが正解（というか、ぼくが実際に喋ってるもの）
	  <center>
	    <audio preload="metadata" controls>
	      <source src="20230211-22.wav" type="audio/wav" />
	    </audio>
	  </center>
	</li>

	<li>Jupyter Notebook で
	  <center>
	    <a href="Screen Shot 2023-02-22 at 14.45.45.png"><!-- 2263x309 -->
	      <img src="Screen Shot 2023-02-22 at 14.45.45_thumb.jpg" width="800" height="109" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 14.45.48.png"><!-- 2266x941 -->
	      <img src="Screen Shot 2023-02-22 at 14.45.48_thumb.jpg" width="800" height="332" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-02-22 at 14.45.51.png"><!-- 2259x638 -->
	      <img src="Screen Shot 2023-02-22 at 14.45.51_thumb.jpg" width="800" height="226" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>で、お待ちかねの
	  <center>
	    <br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      テキスト音声合成の
	      <br />
	      結果です！
	    </div>
	    <br /><br />

	    <audio preload="metadata" controls>
	      <source src="out-test-musmath-s02e07-22.wav" type="audio/wav" />
	    </audio>
	    <br /><br />
	    <div style="font-size: 50px; font-weight: bolder;">
	      ドヤ！
	    </div>
	    （ぼくが自慢することでもないですが）
	    <br /><br />
	  </center>
	</li>

      </ul>
    </li>

    <li>ということで、この <b>kengo</b> モデルを使えば、
      <center>
	<br />
	<div style="font-size: 50px; font-weight: bolder;">
	  テキストを与えるだけで
	  <br />
	  すきなだけ kengo さんに<br />喋ってもらうことができます！
	  <br /><br />
	  やったー、これでラクできるー！
	</div>
	<br /><br />
	（今の AI の話題は、ほぼ全て、こういう「ラクできる」ってはなしですよね）
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-espnet2-one-shot-talking-face">
    <div style="font-size: 50px; font-weight: bolder;">
      おまけ<br />
      VTuber けんごさん
    </div>
    <br /><br />
  </center>

  <ul>
    <li>声が合成できたのなら、<br />
      顔も合成してしまおう、<br />
      つまり（本当の）<b>バーチャル YouTuber</b> ができるかどうか、
      実験してみよう！
    </li>
      
    <li>ちなみに、使うものは
      <a href="https://hello-ai-forum.github.io/pages/ZAF202301/ichiki/#part1-one-shot-talking-face">ZAF-2301</a>
      で紹介した <b>One-Shot Talking Face</b>
      (<a href="https://colab.research.google.com/github/camenduru/one-shot-talking-face/blob/main/one_shot_talking_face.ipynb">colab</a>)
      <br />
      音声ファイルと、画像ファイル１枚から、動画を作ってくれる AI モデルです
      <center>
	<a href="Screen Shot 2023-02-22 at 12.53.43.png"><!-- 2807x1376 -->
	  <img src="Screen Shot 2023-02-22 at 12.53.43_thumb.jpg" width="800" height="392" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>そのときの結果はこちら
      <center>
	<video controls
	       width="256" height="256" style="border: 2px #ccc solid;">
	  <source src="one_shot_talking_face.mp4">
	</video>
      </center>
    </li>
    <li>毎度同じ顔でビデオ作るのも面白くないので、今回は１０年以上前の
      「けんごさん」に登場してもらうことにしました！
      <center>
	<img src="portrait0711-300x400.jpeg" width="300" height="400" />
      </center>
    </li>

    <li>この One-Shot Talking Face のコラボノートブックの使い方のポイント
      <ul>
	<li>画像ファイルは 256x256 で PNG にしましょう</li>
	<li>音声ファイルは３０秒程度にしましょう</li>
      </ul>
      <center>
	<a href="Screen Shot 2023-02-22 at 12.53.49.png"><!-- 2788x1365 -->
	  <img src="Screen Shot 2023-02-22 at 12.53.49_thumb.jpg" width="800" height="392" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 14.08.08.png"><!-- 2132x726 -->
	  <img src="Screen Shot 2023-02-22 at 14.08.08_thumb.jpg" width="800" height="272" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 14.08.44.png"><!-- 2136x229 -->
	  <img src="Screen Shot 2023-02-22 at 14.08.44_thumb.jpg" width="800" height="86" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 14.09.01.png"><!-- 2110x592 -->
	  <img src="Screen Shot 2023-02-22 at 14.09.01_thumb.jpg" width="800" height="224" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-02-22 at 14.09.11.png"><!-- 2125x771 -->
	  <img src="Screen Shot 2023-02-22 at 14.09.11_thumb.jpg" width="800" height="290" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>そのノートブックは、セルが２つだけで
	  <ul>
	    <li>（１）環境設定、ツールのインストールなど</li>
	    <li>（２）データの解析、合成</li>
	  </ul>
	  と、全部込み込みな形になってますが、
	</li>
	<li>複数の音声ファイルを取っ替え引っ替えしたいときとかの注意点
	  <ul>
	    <li>音声ファイルは <tt>pocketsphinx</tt> と言うツールで
	      発音の解析を行い、結果を JSON ファイルに出力している</li>
	    <li>動画の合成時には、画像ファイル、音声ファイルに合わせて
	      JSON ファイルも指定するので、<br />
	      音声ファイルと JSON ファイルの整合性に注意すること</li>
	  </ul>
	</li>
      </ul>
    </li>

    <li>ということで、
      <center>
	<br />
	<div style="font-size: 40px; font-weight: bolder;">
	  テキストと画像１枚だけから作られた
	</div>
	<br />
	<div style="font-size: 50px; font-weight: bolder;">
	  VTuber けんごさん
	</div>
	<br />
	<div style="font-size: 40px; font-weight: bolder;">
	  です！
	</div>
	<br /><br />
      </center>
      <ul>
	<li>素材
	  <center>
	    <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;">
	      <tr>
		<th width="500px">テキスト</th>
		<th>画像</th>
	      </tr>
	      <tr>
		<td>
		  <div style="font-size: 20px;">
		    <pre>
ああ、あのあれだ。そう、あのね、僕Amazonプライムに入ってるんで、
Amazonプライムにある映画とかドラマとか見れるんですが、
ちょっとネットで話題になってたやつ、ボザロね。
なんだろうと思ったら、Botch the Rockっていう漫画がっていう話があって、
連続アニメで。もう終わっててAmazonプライムにはもう全話入ってたんで、
見ましたけども、これ結構良かったね。
良かったし、俺はYouTuberとして負けてるなと思いましたね。ひとりちゃんに。
ギター買えるほど、っていうか収益化できてないもんね。
っていうか、そもそもサブスク数が足してないから話にならないんだけども。
っていう意味で負け負けですけどもね。いや、面白かったね、あれね。
あれ見たら、
		    </pre>
		  </div>
		</td>
		<td>
		  <img src="portrait0711-256x256.png"
		       width="256" height="256" />
		</td>
	      </tr>
	    </table>
	  </center>
	</li>
	<li>結果
	  <center>
	    <video controls
		   width="256" height="256" style="border: 2px #ccc solid;">
	      <source src="image_out-test-musmath-s02e07-22_trim.mp4">
	    </video>
	  </center>
	</li>
	<li>比較として、正しい音声を使った <b>One-Shot Talking Face</b> の結果
	  <center>
	    <video controls
		   width="256" height="256" style="border: 2px #ccc solid;">
	      <source src="image_20230211-22_trim2.mp4">
	    </video>
	  </center>
	</li>
      </ul>
    </li>

    <li>付録：時事ネタとして
      <ul>
	<li>数日前のはなし（昨日だったっけ？）
	  暖炉の前でイケメンが喋ってるビデオが、
	  AI が生成したものだ、という話題があった
	</li>
	<li>それはそれとして「フゥン」と思ってたけど、
	  どうも怪しいという話が<br />
	  <a href="https://twitter.com/nomtats/status/1628233243916136450">https://twitter.com/nomtats/status/1628233243916136450</a>
	  <center>
	    <a href="Screen Shot 2023-02-22 at 16.16.36.png"><!-- 1183x1323 -->
	      <img src="Screen Shot 2023-02-22 at 16.16.36_thumb.jpg" width="537" height="600" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>現状、 DIY 的と言うか、 Poor man’s なアプローチというか、<br />
	  上で示したようなことは、もう既にできますね、ということで</li>
	<li>ってか、 Microsoft の VALL-E って、
	  いつ使えるようになるんだろう？</li>
      </ul>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="epilogue">
    <div style="font-size: 50px; font-weight: bolder;">
      今日のおわりに
    </div>
  </center>

  <p>……</p>

  <h3>今後の予定</h3>
  <ul>
    <li>次回 ZAF は 2023 年 3 月 29 日開催の予定です。</li>
    <li>ZAF 講演者、 ZAM 執筆者、絶賛、大募集中です！<br />
      お気軽にお問い合わせください！</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />
  <hr />

  <h2 id="detailed-toc">総合目次</h2>
  <ul>
    <li><b>前座</b>
      <a href="#part0">ポッドキャストについて、あれこれ</a>
      <ul>
	<li><a href="#part0-1">ZENKEI AI ポッドキャストをアップグレード！</a></li>
	<li><a href="#part0-music-and-math">音楽と数理 🎼 ♾ ポッドキャスト</a></li>
      </ul>
    </li>

    <li><b>第１部</b>
      <a href="#part1">Transformer を完璧に理解する！</a>
      <ul>
	<li>これまでの試み
	  <ul>
	    <li><a href="#part1-transformer-before">その１：
		Transformer</a></li>
	    <li><a href="#part1-diffusion-before">その２：
		Diffusion models</a></li>
	  </ul>
	</li>
	<li><a href="#part1-transformer-from-scratch">プログラマ的に Transformer 入門</a></li>
	<li><a href="#part1-andrej-karpathy">Andrej Karpathy とは</a></li>
	<li><a href="#part1-gpt-from-scratch">スクラッチから GPT を実装！</a>
	  <ul>
	    <li><a href="#part1-gpt-from-scratch-01">Character Level の GPT</a></li>
	    <li><a href="#part1-gpt-from-scratch-02">tiktokes</a></li>
	    <li><a href="#part1-gpt-from-scratch-03">D-Adaptation</a></li>
	    <li><a href="#part1-gpt-from-scratch-04">青空文庫</a></li>
	    <li><a href="#part1-gpt-from-scratch-summary">まとめ</a></li>
	  </ul>
	</li>

	<li><a href="#part1-misc">その他の情報</a>
	  <ul>
	    <li>GPT-2,
	      GPT-3,
	      Transformer カタログ,
	      「30分で完全理解するTransformerの世界」,
	      スケーリング,
	      「GPT in 60 Lines of NumPy」,
	      MetaFormer,
	      Toolformer
	    </li>
	  </ul>
	</li>
	<li><a href="#part1-chatgpt">ChatGPT とは？</a>
	  <ul>
	    <li><a href="#part1-rlhf">HuggingFace の RLHF</a></li>
	    <li><a href="#part1-ngyuen">RLHF に関する重要文献 (by Khanh X. Nguyen)</a></li>
	  </ul>
	</li>

      </ul>
    </li>

    <li><b>第２部</b>
      <a href="#part2">音声合成</a>
      <ul>
	<li><a href="#part2-espnet2">ESPnet2</a>
	  <ul>
	    <li><a href="#part2-espnet2-install">（１）インストール</a></li>
	    <li><a href="#part2-espnet2-dataset">（２） データセット準備</a></li>
	    <li><a href="#part2-espnet2-training">（３） 学習</a></li>
	    <li><a href="#part2-espnet2-inference">（４） 推論（音声合成）</a></li>
	  </ul>
	</li>
	<li><a href="#part2-espnet2-one-shot-talking-face">おまけ：
	    VTuber けんごさん</a>
	</li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

</section>

</article>

</body>           
</html>
