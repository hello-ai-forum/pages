<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8" />
    <title>ZENKEI AI FORUM (2023/01/25)</title>
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+1p:100,400,700&display=swap&subset=japanese" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../bright-M_PLUS_1p.css" />

    <link rel="stylesheet"
	  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="text/javascript" id="MathJax-script" async
	    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

<body style="font-size: 20px;">

<header>
<center><h1>ZENKEI AI FORUM 2023/01/25</h1></center>
</header>

<article>

<section id="main">

  <center>
    <a href="ZENKEI_AI_FORUM_zoom_20230125-2488x1400.jpg"><!-- 2341x1400 -->
      <img src="ZENKEI_AI_FORUM_zoom_20230125-2488x1400_thumb.jpg" width="800" height="478" style="border: 2px #ccc solid;" /></a>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      ZAF 2023 年 1 月 25 日
    </div>
    <div style="font-size: 40px;">＜本日のテーマ＞</div>
    <div style="font-size: 70px;">
      2023年のスタート
    </div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <h2 id="toc">目次</h2>
  <ul>
    <li>[6:30 - 7:00]
      <b>前座</b>
      <a href="#part0">１年の計は１月の ZAF にあり</a>
    </li>

    <li>[7:00 - 8:00]
      <b>パート１</b>
      <a href="#part1">最近の話題から</a></li>
    </li>

    <li>[8:00 - 9:00]
      <b>パート２</b>
      <a href="#part2">技術書典１４</a>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>

    <li><a href="#detailed-toc">総合目次</a></li>
  </ul>

  <hr />

  <center>
    <br />
    <div style="font-size: 30px;">
      YouTube のアーカイブ・ビデオはこちら
    </div>
    (<a href="https://youtube.com/live/XO4mGczavuo">
      https://youtube.com/live/XO4mGczavuo</a>)
    <br /><br />
    <a href="https://youtube.com/live/XO4mGczavuo">
      <img src="Screen Shot 2023-03-31 at 10.37.20_thumb.jpg" width="500" height="292" style="border: 2px #ccc solid;" /></a>
    <br /><br />
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0">
    <div style="font-size: 50px; font-weight: bolder;">
      （前座）
      <br />
      １年の計は
      <br />
      １月の ZAF にあり
    </div>
    <br /><br />
  </center>

  <h3 id="part0-musmath">音楽と数理ポッドキャスト</h3>

  <center>
    <a href="https://music0math.wordpress.com/">
      <img src="Screen Shot 2023-01-25 at 15.47.50_thumb.jpg" width="800" height="318" style="border: 2px #ccc solid;" /></a>
    <a href="https://music0math.wordpress.com/">
      <img src="Screen Shot 2023-01-25 at 15.48.06_thumb.jpg" width="800" height="296" style="border: 2px #ccc solid;" /></a>
  </center>

  <ul>
    <li>趣味の方のポッドキャスト、今年から運営方針を変更
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  毎週金曜日 午後１０時に
	  <br />
	  エピソードをリリース
	  <br /><br />
	</div>
      </center>
    </li>

    <li>今年に入って既に３本のエピソードをリリース
      <ul>
	<li><b>S02E01</b>
	  〈ピアノ〉2022年の振り返りと、新レパートリー？
	  <center>
	    <table border="0">
	      <tr>
		<td valign="top">
		  <a href="Screen Shot 2023-01-25 at 16.07.00.png"><!-- 1567x1227 -->
		    <img src="Screen Shot 2023-01-25 at 16.07.00_thumb.jpg" width="400" height="313" style="border: 2px #ccc solid;" /></a>
		</td>
		<td valign="top">
		  <a href="Screen Shot 2023-01-25 at 16.07.10.png"><!-- 1591x454 -->
		    <img src="Screen Shot 2023-01-25 at 16.07.10_thumb.jpg" width="400" height="114" style="border: 2px #ccc solid;" /></a>
		</td>
	      </tr>
	    </table>
	  </center>
	</li>
	<li><b>S02E02</b>
	  〈トーク〉2022年エピソード・ランキング年間ベストテン
	  <center>
	    <table border="0">
	      <tr>
		<td valign="top">
		  <a href="Screen Shot 2023-01-25 at 15.56.38.png"><!-- 1530x1268 -->
		    <img src="Screen Shot 2023-01-25 at 15.56.38_thumb.jpg" width="400" height="332" style="border: 2px #ccc solid;" /></a>
		</td>
		<td valign="top">
		  <a href="Screen Shot 2023-01-25 at 15.57.04.png"><!-- 1443x1290 -->
		    <img src="Screen Shot 2023-01-25 at 15.57.04_thumb.jpg" width="400" height="358" style="border: 2px #ccc solid;" /></a>
		</td>
	      </tr>
	    </table>
	  </center>
	</li>
	<li><b>S02E03</b>
	  (English) My 2023 New Year’s Resolution and Future with AI
	  <ul>
	    <li><b>セクション１：</b>個人的な継続課題
	      <ul>
		<li>ピアノ</li>
		<li>瞑想 (cf. <a href="https://zenkei-ai-forum.github.io/pages/ZAF202212/ichiki/#part0-1a">ZAF-2212</a>)</li>
		<li>ポッドキャスト</li>
	      </ul>
	    </li>
	    <li><b>セクション２：</b>個人的な新規課題
	      <ul>
		<li>（トイ）レコードを作る</li>
		<li>英語の（紙の）本をアマゾンで世界に向けて売る</li>
	      </ul>
	    </li>
	    <li><b>セクション３：</b>非個人的な今年の希望
	      <ul>
		<li>Peace ☮ and Love ❤</li>
	      </ul>
	    </li>
	  </ul>
	</li>
      </ul>
    </li><!-- 今年に入って既に３本のエピソードをリリース -->

    <li>ってことで、これは<b>「いちきけんご」の今年 2023 年の目標</b>でした</li>

    <li>さて、「<b>ZENKEI AI FORUM</b>」の 2023 年の目標は？
      <ul>
	<li>（これが、結構、難しいよね……昨今の技術の進歩とか考えると）</li>
      </ul>
    </li>

    <br /><br />

    <li>もう１つ、アナウンス（決定事項）
      <ul>
	<li>「音楽と数理ポッドキャスト」は、
	  いわゆる「<b>重いコンダラ</b>」システムと呼ばれる
	  エピソードのローテーションを行ってきましたが、
	  <center>
	    <div style="font-size: 40px; font-weight: bolder;">
	      <br /><br />
	      今週金曜日予定の S02E04 で<br />
	      〈Music+Talk〉回（通称「音楽回」）は<br />
	      最終回とします
	      <br /><br />
	    </div>
	  </center>
	</li>
	<li>ポッドキャストとしては<b>〈裏〉回</b>のみとします</li>
	<li><b>YouTube プレイリスト版</b>は、継続します</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="part0-zap">ZENKEI AI ポッドキャスト</h3>

  <ul>
    <li>毎週水曜日、１話ずつ、進んでます
      <center>
	<a href="Screen Shot 2023-01-25 at 12.18.16.png"><!-- 2418x1306 -->
	  <img src="Screen Shot 2023-01-25 at 12.18.16_thumb.jpg" width="800" height="432" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>今日は、シーズン２８エピソード２
      <ul>
	<li>去年 2022 年４月の ZENKEI AI FORUM (<a href="https://zenkei-ai-forum.github.io/pages/ZAF202204/ichiki/">ZAF-2204</a>) からの内容です
	  <center>
	    <a href="https://zenkei-ai-forum.github.io/pages/ZAF202204/ichiki/">
	       <img src="Screen Shot 2023-01-25 at 15.21.44_thumb.jpg" width="800" height="182" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li>シーズン２８は全５話、既に仕込みは完了してます！
      <center>
	<a href="Screen Shot 2023-01-24 at 22.42.42.png"><!-- 2001x1415 -->
	  <img src="Screen Shot 2023-01-24 at 22.42.42_thumb.jpg" width="800" height="566" style="border: 2px #ccc solid;" /></a>

      </center>
    </li>

    <li>さて、本題（？）の
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br /><br />
	  ZENKEI AI FORUM<br />
	  今年 2023 年の目標は？
	  <br /><br />
	</div>
      </center>
    </li>

    <li><b>（１）継続</b>
      <ul>
	<li>世の中には、
	  <center>
	    <div style="font-size: 30px; font-weight: bolder;">
	      <br />
	      スキルを磨くには<br />
	      質よりも、まずは量をこなせ
	      <br /><br />
	    </div>
	  </center>
	  という格言（？）がありますね</li>
	<li><b>ZENKEI AI FORUM</b> も
	  なんだかんだ言って 2018 年の ZENKEI AI SEMINAR から
	  <b>今年で５年が経ちます</b>。</li>
	<li>これからも、弛まず、やっていこうと思ってます！</li>

	<li>また、これからの世の中を進んでいくとき
	  <center>
	    <div style="font-size: 30px; font-weight: bolder;">
	      <br />
	      頼りになる仲間の集団（コミュニティ）
	      <br /><br />
	    </div>
	  </center>
	  となるべく、精進していこうと思ってます
	  <ul>
	    <li><b>瀧本哲史</b> (cf. <a href="https://zenkei-ai-forum.github.io/pages/ZAF202206/ichiki/#part2-mission-statement">ZAF-2206</a>) とか、
	      <b>宮台真司</b> (cf. <a href="https://zenkei-ai-forum.github.io/pages/ZAF202212/ichiki/#part0-1b">ZAF-2212</a>) とかが言ってたことを
	      念頭におきながら</li>
	  </ul>
	</li>
      </ul>
    </li>
    <li><b>（２） ZENKEI AI MAGAZINE 刊行</b>
      <ul>
	<li><a href="#part2">パート２：技術書典１４</a>参照</li>
      </ul>
    </li>
    <li><b>（３）なにかありますか？</b></li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">目次に戻る</a>）
  </div>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      パート１
      <br />
      最近の話題から
    </div>
  </center>

  <center>
    <div style="font-size: 40px;">
      <br />
      困ったときの
      <br />
      「最近の話題から」
      <br />
      ですね
      <br /><br />
    </div>
  </center>

  <h3>これまでの「最近の話題から」</h3>

  <ul>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202210/ichiki/">ZAF-2210</a> - 漠然と、 Whisper と AlphaTensor と</li>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202206/ichiki/">ZAF-2206</a> - 「最近の AI は本当すごいね」変顔したり</li>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202203/ichiki/">ZAF-2203</a> - 「最近の話題から」2022年春</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />
  
  <h2 id="part1-OpenAI">OpenAI 関連</h2>

  <ul>
    <li><a href="https://twitter.com/karpathy/status/1617979122625712128">https://twitter.com/karpathy/status/1617979122625712128</a>
      <center>
	<a href="Screen Shot 2023-01-25 at 13.04.25.png"><!-- 1185x182 -->
	  <img src="Screen Shot 2023-01-25 at 13.04.25_thumb.jpg" width="400" height="61" style="border: 2px #ccc solid;" /></a>
	<pre>
The hottest new programming language is English
	</pre>
      </center>
    </li>


    <li>ChatGPT のことですね
      <ul>
	<li><a href="https://twitter.com/goto_yuta_/status/1615154914275254272">https://twitter.com/goto_yuta_/status/1615154914275254272</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.22.23.png"><!-- 1183x1065 -->
	      <img src="Screen Shot 2023-01-19 at 17.22.23_thumb.jpg" width="400" height="360" style="border: 2px #ccc solid;" /></a>
	    <pre>
ChatGPTに共通テストの英語解かせたら77 % の高得点を記録しただけでなく、
途中から"問題の文章を与えた時点で、設問を勝手に生成しはじめた"り、
"自分で問題を作って答え"たり、別次元の凄さ見せつけてくるらしい。
	    </pre>
	  </center>
	</li>

      </ul>
    </li><!-- ChatGPT のことですね -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="part1-openai-context">ここ最近の流れ</h3>

  <ul>
    <li>ChatGPT が公開されました
      <ul>
	<li>（というのは、既に述べました）</li>
	<li>こいつの本体は、近く公開されると噂の GPT-4 ではなく、<br />
	  既に公開済みの GPT-3 の改良版的な GPT-3.5 らしい</li>
      </ul>
    </li>


    <li>Microsoft が追加の巨大投資を行うと発表
      <ul>
	<li><a href="https://twitter.com/TrungTPhan/status/1612707453787074562">https://twitter.com/TrungTPhan/status/1612707453787074562</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.30.14.png"><!-- 1185x1039 -->
	      <img src="Screen Shot 2023-01-19 at 17.30.14_thumb.jpg" width="400" height="351" style="border: 2px #ccc solid;" /></a>
	    <pre>
Microsoft looking at a $10B investment into OpenAI:

• At a $29B valuation 
• MSFT gets 75% of OpenAI profits until it recoups $10B
• After recouping $10B, the ownership structure becomes MSFT (49%),
other investors (49%) and OpenAI non-profit parent (2%)
	    </pre>
	  </center>
	</li>

      </ul>
    </li><!-- Microsoft が追加の巨大投資を行うと発表 -->

    <li>これを見て、みんな、あれこれ憶測をする
      <ul>
	<li><a href="https://twitter.com/bioshok3/status/1612696245621100544">https://twitter.com/bioshok3/status/1612696245621100544</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.31.59.png"><!-- 1173x669 -->
	      <img src="Screen Shot 2023-01-19 at 17.31.59_thumb.jpg" width="400" height="228" style="border: 2px #ccc solid;" /></a>
	    <pre>
いやなんというかさ、流石に100億ドルの投資って
これ相当なOpenAIの技術に対する確信的な世界を変えるという感覚がないとできないでしょこれ。
つまり、もう現時点でOpenAIとマイクロソフト幹部の一部は
もうこれ産業革命レベルだなって気付いている可能性が激高なんだが？
	    </pre>
	  </center>
	</li>

	<li><a href="https://twitter.com/fladdict/status/1612720829091155968">https://twitter.com/fladdict/status/1612720829091155968</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.32.21.png"><!-- 1185x580 -->
	      <img src="Screen Shot 2023-01-19 at 17.32.21_thumb.jpg" width="400" height="196" style="border: 2px #ccc solid;" /></a>
	    <pre>
素直に考えると、
MSの幹部あつめてGPT4かGPT5の体験デモをした結果、
なにかすごい判断がおきたと解釈した。
	    </pre>
	  </center>
	</li>

      </ul>
    </li><!-- これを見て、みんな、あれこれ憶測をする -->

    <li>そうこうしてたら、これまで公開を渋ってた Google が API 提供などアナウンス
      <ul>
	<li><a href="https://twitter.com/sundarpichai/status/1615820298305118221">https://twitter.com/sundarpichai/status/1615820298305118221</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.12.43.png"><!-- 2880x1800 -->
	      <img src="Screen Shot 2023-01-19 at 17.12.43_thumb.jpg" width="400" height="250" style="border: 2px #ccc solid;" /></a>
	    <pre>
Summary of great research progress on #GoogleAI,
including language models, computer vision, multimodal models, generative ML.
We're building it all into current and upcoming products + APIs,
look forward to sharing more with everyone soon. Stay tuned!

<a href="https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html">https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html</a>
	    </pre>
	    <a href="Screen Shot 2023-01-19 at 17.13.03.png"><!-- 2363x1037 -->
	      <img src="Screen Shot 2023-01-19 at 17.13.03_thumb.jpg" width="400" height="176" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li><a href="https://twitter.com/EMostaque/status/1615863021615697921">https://twitter.com/EMostaque/status/1615863021615697921</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.14.39.png"><!-- 1188x1200 -->
	      <img src="Screen Shot 2023-01-19 at 17.14.39_thumb.jpg" width="396" height="400" style="border: 2px #ccc solid;" /></a>
	    <pre>
Google generative AI APIs incoming, hold onto your hats…
	    </pre>
	  </center>
	</li>

	<li><a href="https://twitter.com/umiyuki_ai/status/1615867321775710209">https://twitter.com/umiyuki_ai/status/1615867321775710209</a>
	  <center>
	    <a href="Screen Shot 2023-01-23 at 17.44.58.png"><!-- 1186x259 -->
	      <img src="Screen Shot 2023-01-23 at 17.44.58_thumb.jpg" width="400" height="87" style="border: 2px #ccc solid;" /></a>
	    <pre>
え…GoogleのLaMDAとかImagenがAPI提供されるわけ？
Googleのヤツ、AzureにOpenAIが来たからって慌てすぎでは。
迂闊に開放したら世界がヤバいAIじゃなかったんか　→RT
	    </pre>
	  </center>
	</li>

      </ul>
    </li><!-- そうこうしてたら、これまで公開を渋ってた Google が... -->


    <li>そんな中、 OpenAI のトップ Sam Altman の
      公開の場でのインタビューのビデオが流れてきた
      <ul>
	<li><a href="https://twitter.com/bioshok3/status/1615894949475799040">https://twitter.com/bioshok3/status/1615894949475799040</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.11.38.png"><!-- 1186x1254 -->
	      <img src="Screen Shot 2023-01-19 at 17.11.38_thumb.jpg" width="378" height="400" style="border: 2px #ccc solid;" /></a>
	    <pre>
<a href="https://youtube.com/watch?v=ebjkD1Om4uw">https://youtube.com/watch?v=ebjkD1Om4uw</a>
以下の記事でGPT4に関しての誇大広告にOpenAI CEOが釘を刺しているが
そのエビデンスの動画が上記。（全体として聞く方がニュアンスがわかると思う）

そして以前紹介したvideo modelについて何らかの発言もしている。これは見なくてはいけない。
	    </pre>
	  </center>
	</li>

	<li>みてみました
	  (<a href="https://youtu.be/ebjkD1Om4uw">https://youtu.be/ebjkD1Om4uw</a>)
	</li>

	<li>OpenAI の書き起こしモデル Whisper で書き起こし
	  <center>
	    <a href="sam-altman-interview.html">
	      <img src="Screen Shot 2023-01-24 at 20.36.25_thumb.jpg" width="800" height="335" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>世間が加熱している真っ只中に出てきてインタビューを受ける姿勢に感動
	  <ul>
	    <li>受け答えも、きちんと考えてきた人間の言葉だったと思った</li>
	  </ul>
	</li>

      </ul>
    </li><!-- そんな中、 OpenAI のトップ Sam Altman の -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="part1-future">未来予想</h3>

  <ul>
    <li>（すると、いつも暗い気分になるので、やりたくないのだが……）</li>

    <li><a href="https://twitter.com/fladdict/status/1618112155165036544">https://twitter.com/fladdict/status/1618112155165036544</a>
      <center>
	<a href="Screen Shot 2023-01-25 at 16.22.22.png"><!-- 1188x751 -->
	  <img src="Screen Shot 2023-01-25 at 16.22.22_thumb.jpg" width="400" height="253" style="border: 2px #ccc solid;" /></a>
	<pre>
もう数年もすれば、AIアシスタント代が払えるかどうかで、
「24時間マサチューセッツの教授チームがメンターしてくれる」
vs
「図書館で独学する人」みたいなエグい差がつく時代になるということ。

月5000円とか3万円払えるかどうかで。
	</pre>
      </center>
    </li>

    <li>具体的な形はどうであれ
      <center>
	<div style="font-size: 30px; font-weight: bolder;">
	  <br />
	  新しい（価値のある）技術は
	  <br />
	  金持ちから使い始める
	  <br /><br />
	</div>
      </center>
      <ul>
	<li>その技術が十分に行き渡るまでの過渡期に、必然的に
	  格差が生まれるだろう</li>
	<li>問うべき問いは、
	  <center>
	    <div style="font-size: 30px; font-weight: bolder;">
	      <br />
	      その格差は、（技術が十分に行き渡る過程で）<br />
	      無くなっていくのかどうか？
	      <br /><br />
	    </div>
	  </center>
	</li>
	<li>あるいは、
	  <center>
	    <div style="font-size: 30px; font-weight: bolder;">
	      果たしてその技術がみんなに行き渡るのか？
	      <br /><br />
	    </div>
	  </center>
	</li>
	<li>（悲観的すぎる？）</li>

      </ul>
    </li><!-- 具体的な形はどうであれ -->

    <li>そう考えると、昨今の OpenAI の
      （DeepMind や Google 他も入れていい？）努力（？）は、<br />
      あれでも頑張っているのかもしれないな、とか思ったり
    </li>

  </ul>

  <h3 id="part1-Society">AI と社会</h3>

  <ul>
    <li>社会との関わりなど考えたとき、
      今回の件で話題になった以下の話とかも、
      重要なポイントだろう
      <ul>
	<li><a href="https://twitter.com/mehori/status/1615894250465681480">https://twitter.com/mehori/status/1615894250465681408</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.11.29.png"><!-- 1182x1151 -->
	      <img src="Screen Shot 2023-01-19 at 17.11.29_thumb.jpg" width="400" height="390" style="border: 2px #ccc solid;" /></a>
	    <pre>
OpenAIはChatGPTの開発にあたってデータから残酷な虐待や殺人などの描写を取り除くために
安価なケニアの労働者にデータのモデレーションをさせていたという報告

我々がAIの生み出すきれいなデータを利用する裏側で、
こうした労働が一部の人間に押し付けられる現実があるのか
	    </pre>
	  </center>
	</li>
      </ul>
    </li><!-- 社会との関わりなど考えたとき... -->


    <li>別の文脈から、 AI と社会との関わりについて
      <ul>

	<li><a href="https://twitter.com/AkioHoshi/status/1615600818849472520">https://twitter.com/AkioHoshi/status/1615600818849472520</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.17.10.png"><!-- 1185x1152 -->
	      <img src="Screen Shot 2023-01-19 at 17.17.10_thumb.jpg" width="400" height="389" style="border: 2px #ccc solid;" /></a>
	    <pre>
「AI技術が民主主義を混乱させ破壊する」という趣旨の警告が相次いで出てきた。
以下、紹介する。
	    </pre>
	  </center>
	</li>

      </ul>
    </li>


    <li>こう考えると、<br />
      ぼくの敬愛する Jeremy Howard (FastAI) は、
      早い時点から、<br />
      「AI と社会」について
      （倫理観や、暗黙のバイアスについて）<br />
      意識的に考えていたのだな、と思ったりする
    </li>

    <br /><br />

    <li>……こうやって考えてくると、疲れてきた……</li>

    <br /><br />

    <li>すると……（いつもの癖で）<br />
      牧歌的な技術の世界（人と人の関わりのない世界）に逃げ込みたくなる……
      <ul>
	<li><a href="https://twitter.com/karpathy/status/1615398117683388417">https://twitter.com/karpathy/status/1615398117683388417</a>
	  <center>
	    <a href="Screen Shot 2023-01-19 at 17.17.34.png"><!-- 1183x974 -->
	      <img src="Screen Shot 2023-01-19 at 17.17.34_thumb.jpg" width="400" height="329" style="border: 2px #ccc solid;" /></a>
	    <pre>
🔥 New (1h56m) video lecture:
"Let's build GPT: from scratch, in code, spelled out."
<a href="https://youtube.com/watch?v=kCc8FmEb1nY">https://youtube.com/watch?v=kCc8FmEb1nY</a>
We build and train a Transformer
following the "Attention Is All You Need" paper
in the language modeling setting
and end up with the core of nanoGPT.
	    </pre>
	  </center>
	</li>

      </ul>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">目次に戻る</a>）
  </div>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h2 id="part1-others">その他の話題</h2>

  <p>OpenAI 以外の、
    <center>
      <div style="font-size: 40px; font-weight: bolder;">
	「最近の話題から」
      </div>
    </center>
  </p>

  <h3 id="part1-Speech">音声合成関連</h3>

  <ul>
    <li>２つのモデル（フレームワーク）を取り上げる
      <ul>
	<li><a href="#part1-valle">Microsoft の発表した VALL-E</a></li>
	<li><a href="#part1-espnet">音声関係のフレームワーク ESPnet</a></li>
      </ul>
    </li>

  </ul>

  <h3 id="part1-valle">VALL-E</h3>

  <ul>
    <li>VALL-E
      <center>
	<a href="Screen Shot 2023-01-19 at 17.39.43.png"><!-- 2793x957 -->
	  <img src="Screen Shot 2023-01-19 at 17.39.43_thumb.jpg" width="400" height="137" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>arxiv: <a href="https://arxiv.org/abs/2301.02111">2301.02111</a>
	  <center>
Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers
	  </center>
	</li>
      </ul>
    </li>


    <li><a href="https://twitter.com/goto_yuta_/status/1612721224647573507">https://twitter.com/goto_yuta_/status/1612721224647573507</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.31.15.png"><!-- 1188x293 -->
	  <img src="Screen Shot 2023-01-19 at 17.31.15_thumb.jpg" width="400" height="99" style="border: 2px #ccc solid;" /></a>
	<pre>
VALL-Eやばすぎる。
自分の声を3秒読み込ませるだけで、自分の声風にテキストを音声に変換できるようになるらしい。
<a href="https://valle-demo.github.io">https://valle-demo.github.io</a>
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/Yamkaz/status/1611715628360171528">https://twitter.com/Yamkaz/status/1611715628360171528</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.34.51.png"><!-- 1184x1203 -->
	  <img src="Screen Shot 2023-01-19 at 17.34.51_thumb.jpg" width="394" height="400" style="border: 2px #ccc solid;" /></a>
	<pre>
Microsoftより音声版DALL-E
「VALL-E」が発表！
<a href="https://valle-demo.github.io">https://valle-demo.github.io</a>

3秒間の人の音声のサンプルを使用し、同じ音声で高品質なテキスト通りの音声を生成できる。
サンプルデータの感情や音響環境まで再現することができる
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/Yamkaz/status/1615552120618643457">https://twitter.com/Yamkaz/status/1615552120618643457</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.18.17.png"><!-- 1183x1170 -->
	  <img src="Screen Shot 2023-01-19 at 17.18.17_thumb.jpg" width="400" height="396" style="border: 2px #ccc solid;" /></a>
	<pre>
VALL-Eの非公式PyTorch実装が公開
<a href="https://github.com/enhuiz/vall-e">https://github.com/enhuiz/vall-e</a>
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/DrJimFan/status/1612496633056620545">https://twitter.com/DrJimFan/status/1612496633056620545</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.33.21.png"><!-- 1183x1045 -->
	  <img src="Screen Shot 2023-01-19 at 17.33.21_thumb.jpg" width="400" height="353" style="border: 2px #ccc solid;" /></a>
	<pre>
Here’s the recipe to make Siri/Alexa 10x better:

1. Whisper to convert speech to text. Best open-source speech model out there.
2. ChatGPT to generate smart home API calls and/or text response.
3. VALL-E to synthesize speech. It can mimic anyone’s voice sample!

Quick figure 1/3
	</pre>
      </center>
    </li>

  </ul>

  <h3 id="part1-espnet">ESPnet</h3>

  <ul>
    <li><a href="https://twitter.com/izutorishima/status/1614874074827853824">https://twitter.com/izutorishima/status/1614874074827853824</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.12.19.png"><!-- 1182x1322 -->
	  <img src="Screen Shot 2023-01-19 at 17.12.19_thumb.jpg" width="358" height="400" style="border: 2px #ccc solid;" /></a>
	<pre>
悪魔の技術だ、すごい

ESPnetで高森藍子の声を錬成した - もみあげコレクション
<a href="https://momicolle.hatenablog.com/entry/2022/12/15/182519">https://momicolle.hatenablog.com/entry/2022/12/15/182519</a>
	</pre>
      </center>
    </li>


    <li>ググってみた
      <ul>
	<li>Qiita: <a href="https://qiita.com/kan-bayashi/items/0371e06202641dbfa0ad">ESPnet2で始めるEnd-to-Endテキスト音声合成 (@kan-bayashi posted at 2020-09-19 updated at 2020-09-19)</a>
	</li>
	<li><a href="https://kan-bayashi.github.io/asj-espnet2-tutorial/">ESPnet2で始めるEnd-to-End音声処理 (Author: 林 知樹(Tomoki Hayashi), Sep 15, 2020)</a>
	</li>
	<li>HuggingFace Spaces: <a href="https://huggingface.co/spaces/akhaliq/ESPnet2-TTS">ESPnet2-TTS</a>
	</li>

	<li><a href="https://qiita.com/RRR_troisR/items/6288b9bdc6e725aa8440">ESPNetで作るキャラクター音声合成 (@RRR_troisR posted at 2021-12-31 updated at 2022-07-09)</a>
	</li>

      </ul>
    </li><!-- ググってみた -->

  </ul>

  <h3 id="part1-WiFi">WiFiのシグナルは重要な情報</h3>

  <ul>
    <li><a href="https://twitter.com/hillbig/status/1615814599705788416">https://twitter.com/hillbig/status/1615814599705788416</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.15.39.png"><!-- 1186x578 -->
	  <img src="Screen Shot 2023-01-19 at 17.15.39_thumb.jpg" width="400" height="195" style="border: 2px #ccc solid;" /></a>
	<pre>
画像でなくWiFiを入力とし空間中の複数人の密な姿勢推定を行う。
照明、遮蔽の影響がなくプライバシー問題が少ない。
3アンテナルーター二台使い、振幅と位相から画像認識器の特徴マップを転移学習で推定し、
RCNNで姿勢推定。画像ベースと比べ精度は劣るが将来性がある

<a href="https://arxiv.org/abs/2301.00250">https://arxiv.org/abs/2301.00250</a>
DensePose From WiFi
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/CeoImed/status/1613480279439556608">https://twitter.com/CeoImed/status/1613480279439556608</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.27.27.png"><!-- 1181x1114 -->
	  <img src="Screen Shot 2023-01-19 at 17.27.27_thumb.jpg" width="400" height="377" style="border: 2px #ccc solid;" /></a>
	<pre>
面白い！！！

｢WiFi信号の深層学習で呼吸不全検知の可能性｣

･ Wi-Fi チャネル状態情報(CSI)と呼ばれる信号に着目
･ 胸部の動きで変化するCSI信号を解析→ 呼吸状態を検出できるアルゴリズムを開発
･ 医療用マネキンを用いて呼吸パターン･呼吸速度を約99 % の精度で分類

<a href="https://medit.tech/monitoring-respiratory-motion-with-wi-fi-csi/">https://medit.tech/monitoring-respiratory-motion-with-wi-fi-csi/</a>
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/AiBreakfast/status/1613550599144091650">https://twitter.com/AiBreakfast/status/1613550599144091650</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.28.26.png"><!-- 1191x1202 -->
	  <img src="Screen Shot 2023-01-19 at 17.28.26_thumb.jpg" width="396" height="400" style="border: 2px #ccc solid;" /></a>
	<pre>
🤯 Full body tracking now possible using only WiFi signals

A deep neural network maps the phase and amplitude of WiFi signals
to UV coordinates within 24 human regions

The model can estimate the dense pose of multiple subjects
by utilizing WiFi signals as the only input

🧵
	</pre>
      </center>
    </li>


  </ul>


  <h3 id="part1-reviews">まとめ系の情報</h3>

  <ul>
    <li><a href="https://twitter.com/ogawa_yutaro_22/status/1615474162671443969">https://twitter.com/ogawa_yutaro_22/status/1615474162671443969</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.19.34.png"><!-- 1182x1215 -->
	  <img src="Screen Shot 2023-01-19 at 17.19.34_thumb.jpg" width="389" height="400" style="border: 2px #ccc solid;" /></a>
	<pre>
各種「Generative AI」について丁寧にまとめた論文？記事？でした

Text-to-image
Text-to-3D
Image-to-Text
Text-to-Video
Text-to-Audio
Text-to-Text
Text-to-Code
Others

<a href="https://arxiv.org/abs/2301.04655">https://arxiv.org/abs/2301.04655</a>

ChatGPT is not all you need.
A State of the Art Review of large Generative AI models
	</pre>
	<a href="Screen Shot 2023-01-19 at 17.19.54.png"><!-- 2338x1011 -->
	  <img src="Screen Shot 2023-01-19 at 17.19.54_thumb.jpg" width="400" height="173" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>


    <li><a href="https://twitter.com/Yamkaz/status/1615536984373751808">https://twitter.com/Yamkaz/status/1615536984373751808</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.21.04.png"><!-- 1185x1136 -->
	  <img src="Screen Shot 2023-01-19 at 17.21.04_thumb.jpg" width="400" height="383" style="border: 2px #ccc solid;" /></a>
	<pre>
Transformerモデルのカタログ、年表
<a href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#Timeline">https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#Timeline</a>
	</pre>
      </center>
      <ul>
	<li><a href="https://twitter.com/xamat/status/1615109028912467970">https://twitter.com/xamat/status/1615109028912467970</a>
	  <center>
	    <pre>
Pretty big update to my Transformer Catalog.
I added ChatGPT, Sparrow, and Stable Diffusion among others.
I also included a section about RLHF and Diffusion models
and a new timeline view. Enjoy!
<a href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/">https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/</a>
	    </pre>
	  </center>
	</li>
      </ul>
    </li>


  </ul>


  <h3 id="part1-DeepMind">DeepMind</h3>

  <ul>
    <li><a href="https://twitter.com/bioshok3/status/1613942246268862467">https://twitter.com/bioshok3/status/1613942246268862467</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.25.23.png"><!-- 1189x1185 -->
	  <img src="Screen Shot 2023-01-19 at 17.25.23_thumb.jpg" width="400" height="399" style="border: 2px #ccc solid;" /></a>
	<pre>
す、すごい・・・。DeepMindがとても印象的な論文を出している！
簡単にいうと人間が解釈可能なプログラム命令をTransformerモデルの重みに変換する
「Tracr」と呼ばれる「コンパイラ」を提案！

例えば以下図で3542をソートするプログラムを書くと
それがコンパイラで「モデルパラメータに変換」される！
	</pre>
      </center>
      <ul>
	<li><a href="https://twitter.com/_akhaliq/status/1613716537944195073">https://twitter.com/_akhaliq/status/1613716537944195073</a>
	  <center>
Tracr: Compiled Transformers as a Laboratory for Interpretability
abs: <a href="https://arxiv.org/abs/2301.05062">https://arxiv.org/abs/2301.05062</a>
	  </center>
	</li>
      </ul>
    </li>


    <li><a href="https://twitter.com/DrJimFan/status/1613243066026168321">https://twitter.com/DrJimFan/status/1613243066026168321</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.28.01.png"><!-- 1184x1081 -->
	  <img src="Screen Shot 2023-01-19 at 17.28.01_thumb.jpg" width="400" height="365" style="border: 2px #ccc solid;" /></a>
	<pre>
Many people don’t understand how challenging Minecraft is for AI agents. 

Let me put it this way. AlphaGo solves a board game with only 1 task,
countably many states, and full observability.

Minecraft has infinite tasks, infinite gameplay,
and tons of hidden world knowledge. 🧵
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/Yamkaz/status/1613356501984514048">https://twitter.com/Yamkaz/status/1613356501984514048</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.29.32.png"><!-- 1182x1181 -->
	  <img src="Screen Shot 2023-01-19 at 17.29.32_thumb.jpg" width="400" height="400" style="border: 2px #ccc solid;" /></a>
	<pre>
DeepMind社より、Minecraftでゼロからダイヤモンドを収集する
初の汎用アルゴリズム「DreamerV3」が発表！
<a href="https://dpmd.ai/dreamerv3">https://dpmd.ai/dreamerv3</a>

チューニングなしで多くの領域を習得することができ、強化学習の適用範囲が広がるとのこと
	</pre>
      </center>
    </li>


  </ul>


  <h3 id="part1-misc">そのほか</h3>

  <ul>
    <li><a href="https://twitter.com/mathemagic1an/status/1615378778863157248">https://twitter.com/mathemagic1an/status/1615378778863157248</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.19.14.png"><!-- 1186x1107 -->
	  <img src="Screen Shot 2023-01-19 at 17.19.14_thumb.jpg" width="400" height="373" style="border: 2px #ccc solid;" /></a>
	<pre>
Clever (and easy!) trick for better LLM context retrieval
for those who haven't seen it:

HyDE: Hypothetical Document Embeddings

<a href="https://arxiv.org/pdf/2212.10496.pdf">https://arxiv.org/pdf/2212.10496.pdf</a>

Take your query
=> create *hypothetical* answer
=> embed hypothetical answer
=> use this to search through doc embeddings

1/
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/kajikent/status/1614244994340237318">https://twitter.com/kajikent/status/1614244994340237318</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.24.18.png"><!-- 1184x930 -->
	  <img src="Screen Shot 2023-01-19 at 17.24.18_thumb.jpg" width="400" height="314" style="border: 2px #ccc solid;" /></a>
	<pre>
ついに「Text to World」とも言うべきテキストを書いただけで3Dゲームを作れるAIも出てきた。

冗談抜きで近い将来、ブログ感覚で人々がセカイをつくる時代が来る。

<a href="https://twitter.com/theCommonToken/status/1612372797355089921">https://twitter.com/theCommonToken/status/1612372797355089921</a>
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/forasteran/status/1613357908598849538">https://twitter.com/forasteran/status/1613357908598849538</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.27.04.png"><!-- 1188x1122 -->
	  <img src="Screen Shot 2023-01-19 at 17.27.04_thumb.jpg" width="400" height="378" style="border: 2px #ccc solid;" /></a>
	<pre>
特定の層のブロック研究してるひともおるんね！
5chの中の人に変態がいるｗ

影響の強いブロックを見て、
特定ブロックに異なる呪文を入れたり、
層別マージに活かしたりｗｗｗ

Dump U-Net （U-Net の特徴量を可視化するための stable-diffusion-webui の拡張）
🔨<a href="http://gitlab.com/hnmr293/stable-diffusion-webui-dumpunet">http://gitlab.com/hnmr293/stable-diffusion-webui-dumpunet</a>
	</pre>
      </center>
    </li>


    <li><a href="https://twitter.com/hillbig/status/1610761874127323137">https://twitter.com/hillbig/status/1610761874127323137</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.35.57.png"><!-- 1181x569 -->
	  <img src="Screen Shot 2023-01-19 at 17.35.57_thumb.jpg" width="400" height="193" style="border: 2px #ccc solid;" /></a>
	<pre>
非平衡熱力学でフォッカープランク方程式から導出されるエントロピー生成率σが
前向き過程の逆向き過程への射影で得られることで
情報幾何学とつながり（変分下限とも関係）、
またσは隣接時刻分布間の最小輸送距離率で得られ最適輸送理論とつながる。
分野が急速につながっている

<a href="https://arxiv.org/abs/2209.00527">https://arxiv.org/abs/2209.00527</a>
Geometric thermodynamics for the Fokker-Planck equation:
Stochastic thermodynamic links between information geometry and optimal transport
	</pre>
      </center>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">目次に戻る</a>）
  </div>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h2 id="part1-demos">デモ</h2>

  <center>
    <div style="font-size: 40px; font-weight: bolder;">
      <br /><br />
      話は十分だ
      <br />
      手を動かそう！
      <br /><br />
    </div>
  </center>

  <h3 id="part1-NeRF">NVIDIA Instant-NGP デモ</h3>

  <ul>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202203/ichiki/#part1-nerf">ZAF-2203</a> でも触れた NeRF
      <center>
	<a href="https://zenkei-ai-forum.github.io/pages/ZAF202203/ichiki/#part1-nerf">
	  <img src="Screen Shot 2023-01-25 at 15.03.05_thumb.jpg" width="800" height="191" style="border: 2px #ccc solid;" /></a>
	<a href="https://zenkei-ai-forum.github.io/pages/ZAF202203/ichiki/#part1-nerf">
	  <img src="Screen Shot 2023-01-25 at 15.03.16_thumb.jpg" width="800" height="262" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li><a href="https://twitter.com/umiyuki_ai/status/1615870454203576321">https://twitter.com/umiyuki_ai/status/1615870454203576321</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.13.25.png"><!-- 1184x740 -->
	  <img src="Screen Shot 2023-01-19 at 17.13.25_thumb.jpg" width="400" height="250" style="border: 2px #ccc solid;" /></a>
	<pre>
Instant NGPが、Windowsバイナリを提供してくれるようになったらしい。
つまり、今までみたいに自分でビルドしなくても良くなった。
グラボさえあればすぐ試せる。じゃあ試してみよっかな　→RT


<a href="https://twitter.com/Peter_shirley/status/1615862200564084737">https://twitter.com/Peter_shirley/status/1615862200564084737</a>
I found instant-NGP fun to use.  Now without a build :)
<a href="https://youtu.be/TA14yYBIRP8">https://youtu.be/TA14yYBIRP8</a>
NVIDIA Introduces No Code Instant NeRF!
	</pre>
      </center>
    </li>


    <li>やってみた
      <ul>
	<li>github: <a href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a> に行って、バイナリーをダウンロード
	  <center>
	    <a href="Screenshot (37).png"><!-- 1996x1329 -->
	      <img src="Screenshot (37)_thumb.jpg" width="400" height="266" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>展開して、そこにある <tt>instant-ngp</tt> を起動（ダブルクリック）
	  <center>
	    <a href="Screenshot (38).png"><!-- 2313x1319 -->
	      <img src="Screenshot (38)_thumb.jpg" width="400" height="228" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>Windows 11 で警告出るが、実行
	  <center>
	    <a href="Screenshot (39).png"><!-- 2310x1329 -->
	      <img src="Screenshot (39)_thumb.jpg" width="400" height="230" style="border: 2px #ccc solid;" /></a>
	    <a href="Screenshot (40).png"><!-- 2308x1331 -->
	      <img src="Screenshot (40)_thumb.jpg" width="400" height="231" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>すると、コンソール・ウィンドウと GUI が立ち上がる
	  <center>
	    <a href="Screenshot (41).png"><!-- 2638x1589 -->
	      <img src="Screenshot (41)_thumb.jpg" width="400" height="241" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>展開したフォルダの中にある NeRF のサンプルデータ <tt>fox</tt> を
	  GUI にドラッグアンドドロップすると、
	  fox の学習とレンダリングが実行される
	  <center>
	    <a href="Screenshot (42).png"><!-- 2066x1155 -->
	      <img src="Screenshot (42)_thumb.jpg" width="400" height="224" style="border: 2px #ccc solid;" /></a>
	    <a href="Screenshot (43).png"><!-- 3109x1992 -->
	      <img src="Screenshot (43)_thumb.jpg" width="400" height="256" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>ということで、インストール完了！</li>

	<li>次は、自分で撮影した映像で NeRF してみよう！</li>
      </ul>
    </li>

    <li>素材 - コロナ前！（2019 年 6 月）に行ったフランスでの早朝散歩のシーンから
      (<a href="https://youtu.be/HjJ97QohfpE?t=1310">https://youtu.be/HjJ97QohfpE?t=1310</a>)
      <center>
	<a href="https://youtu.be/HjJ97QohfpE?t=1310">
	  <img src="Screen Shot 2023-01-25 at 14.06.35_thumb.jpg" width="800" height="475" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>データセット作り
      <ul>
	<li>ビルド済みの GUI アプリ <tt>instant-ngp</tt> は、
	  NeRF の学習処理と、成果物（動画ファイルなど）の生成を行うプログラム</li>
	<li>自分で撮影したデータを NeRF したい場合は、
	  NeRF が使える「データセット」を構成する必要がある
	  (cf. <a href="https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md#preparing-new-nerf-datasets">Preparing new NeRF dataset</a>)
	  <center>
	    <a href="Screen Shot 2023-01-26 at 10.37.01.png"><!-- 2417x396 -->
	      <img src="Screen Shot 2023-01-26 at 10.37.01_thumb.jpg" width="800" height="131" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>ここでは <a href="https://colmap.github.io/">COLMAP</a> を別途インストールして、そこでデータセットを作ってから
	  <tt>instant-ngp</tt> に突っ込んでみた
	  <center>
	    <a href="Screen Shot 2023-01-26 at 10.39.59.png"><!-- 2192x1295 -->
	      <img src="Screen Shot 2023-01-26 at 10.39.59_thumb.jpg" width="600" height="354" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>（詳細は、ここでは省略）</li>

	<li>付記：あと、成果物を動画にするのに、
	  <tt>instant-ngp</tt> をインストールした Windows マシンに
	  <a href="https://ffmpeg.org/">ffmpeg</a> をインストールした（パス通すのとか、手動でちょっと面倒）
	  <center>
	    <a href="Screenshot (53).png"><!-- 3240x2160 -->
	      <img src="Screenshot (53)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>（詳しい手順は &quot;<a href="https://www.techruzz.com/how-to/how-to-download-and-install-ffmpeg-on-windows-11">https://www.techruzz.com/how-to/how-to-download-and-install-ffmpeg-on-windows-11</a>&quot; に書いてあって、ぼくもこれをみながらやった）
	      <center>
		<a href="Screen Shot 2023-01-26 at 10.45.26.png"><!-- 1306x819 -->
		  <img src="Screen Shot 2023-01-26 at 10.45.26_thumb.jpg" width="600" height="376" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	  </ul>
	</li>
      </ul>
    </li><!-- データセット作り -->

    <li>できあがったデータセットを <tt>instant-ngp</tt> にドラッグアンドドロップして、
      あれこれ作業して、
      ...
      <center>
	<a href="Screenshot (44).png"><!-- 2353x1348 -->
	  <img src="Screenshot (44)_thumb.jpg" width="400" height="229" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (45).png"><!-- 3240x2160 -->
	  <img src="Screenshot (45)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (46).png"><!-- 3240x2160 -->
	  <img src="Screenshot (46)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (47).png"><!-- 3240x2160 -->
	  <img src="Screenshot (47)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (48).png"><!-- 3240x2160 -->
	  <img src="Screenshot (48)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (49).png"><!-- 2345x1344 -->
	  <img src="Screenshot (49)_thumb.jpg" width="400" height="229" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (50).png"><!-- 3240x2160 -->
	  <img src="Screenshot (50)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (51).png"><!-- 3240x2160 -->
	  <img src="Screenshot (51)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (52).png"><!-- 3240x2160 -->
	  <img src="Screenshot (52)_thumb.jpg" width="400" height="267" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (55).png"><!-- 2562x1821 -->
	  <img src="Screenshot (55)_thumb.jpg" width="400" height="284" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (56).png"><!-- 3125x1822 -->
	  <img src="Screenshot (56)_thumb.jpg" width="400" height="233" style="border: 2px #ccc solid;" /></a>
	<a href="Screenshot (58).png"><!-- 2367x1305 -->
	  <img src="Screenshot (58)_thumb.jpg" width="400" height="221" style="border: 2px #ccc solid;" /></a>

      </center>
    </li>

    <li>結果
      <center>
	<video controls loop
	       width="960" height="540" style="border: 2px #ccc solid;">
	  <source src="base_video-10fps-10sec.mp4">
	</video>
      </center>
    </li>


  </ul>


  <h3 id="part1-one-shot-talking-face">One-Shot Talking Face デモ</h3>

  <ul>
    <li><a href="https://twitter.com/Yamkaz/status/1614958193951772676">https://twitter.com/Yamkaz/status/1614958193951772676</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.22.44.png"><!-- 1186x888 -->
	  <img src="Screen Shot 2023-01-19 at 17.22.44_thumb.jpg" width="400" height="299" style="border: 2px #ccc solid;" /></a>
	<pre>
音声と画像を入れたら話す顔の動画を生成できる「one-shot-talking-face」のデモが公開
<a href="https://huggingface.co/spaces/camenduru/one-shot-talking-face">https://huggingface.co/spaces/camenduru/one-shot-talking-face</a>
	</pre>
      </center>
    </li>

    <li>そういえば、前に似た様なやつをやってましたね <a href="https://zenkei-ai-forum.github.io/pages/ZAF202206/ichiki/#part1-portrait">ZAF-2206</a>
      <center>
	<a href="Screen Shot 2023-01-25 at 15.14.09.png"><!-- 2821x627 -->
	  <img src="Screen Shot 2023-01-25 at 15.14.09_thumb.jpg" width="800" height="178" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-01-25 at 15.13.16.png"><!-- 2825x549 -->
	  <img src="Screen Shot 2023-01-25 at 15.13.16_thumb.jpg" width="800" height="155" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-01-25 at 15.13.29.png"><!-- 2828x593 -->
	  <img src="Screen Shot 2023-01-25 at 15.13.29_thumb.jpg" width="800" height="168" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>こちらは、ターゲットとなる動画と、置き換えたい顔写真を入力とするもの</li>
	<li>one-shot talking face は、顔写真と音声ファイルを入力とする</li>
      </ul>
    </li>

    <li>やってみた
      <ul>
	<li>素材
	  <ul>
	    <li>画像
	      <center>
		<a href="KI20220401-orig.jpg"><!-- 956x957 -->
		  <img src="KI20220401-orig_thumb.jpg" width="256" height="256" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	    <li>音声
	      <center>
		<audio preload="metadata" controls>
		  <source src="20220427-00-mono.wav" type="audio/wav" />
		</audio>
	      </center>
	    </li>
	  </ul>
	</li>


	<li>huggingface spaces
	  (<a href="https://huggingface.co/spaces/camenduru/one-shot-talking-face">https://huggingface.co/spaces/camenduru/one-shot-talking-face</a>)
	  は、うまくいかなかった……
	  <center>
	    <a href="Screen Shot 2023-01-25 at 11.32.24.png"><!-- 2837x1099 -->
	      <img src="Screen Shot 2023-01-25 at 11.32.24_thumb.jpg" width="800" height="310" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	
	<li>github: <a href="https://github.com/camenduru/one-shot-talking-face-colab">camenduru/one-shot-talking-face-colab</a> に戻って、
	  <center>
	    <a href="Screen Shot 2023-01-25 at 11.45.59.png"><!-- 2830x1183 -->
	      <img src="Screen Shot 2023-01-25 at 11.45.59_thumb.jpg" width="800" height="334" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <a href="https://colab.research.google.com/github/camenduru/one-shot-talking-face/blob/main/one_shot_talking_face.ipynb">colab</a> を使ってみる
	</li>
	<li>素材ファイルをアップロードして、ファイル名を指定してやれば、よい
	  <center>
	    <a href="Screen Shot 2023-01-25 at 11.37.12.png"><!-- 2880x1800 -->
	      <img src="Screen Shot 2023-01-25 at 11.37.12_thumb.jpg" width="800" height="500" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2023-01-25 at 11.37.47.png"><!-- 2833x802 -->
	      <img src="Screen Shot 2023-01-25 at 11.37.47_thumb.jpg" width="800" height="226" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>処理時間も１分くらいだった（かな？）
	  <center>
	    <a href="Screen Shot 2023-01-25 at 11.44.01.png"><!-- 2829x1327 -->
	      <img src="Screen Shot 2023-01-25 at 11.44.01_thumb.jpg" width="800" height="375" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>ポイント
	  <ul>
	    <li>colab で実行してたら、エラーや警告が出てた</li>
	    <li>音声ファイルが、ステレオだとダメで、モノラルにする必要があった</li>
	    <li>Linux 環境だと <tt>sox</tt> を入れれば
	      <pre>
sox infile.wav outfile.wav remix 1,2
	      </pre>
	      で OK
	    </li>
	    <li>あと、サンプルレートが 16k じゃない、みたいな警告が出てたが、
	      これは処理には問題なかった（模様）</li>
	  </ul>
	</li>


	<li>結果
	  <center>
	    <video controls
		   width="256" height="256" style="border: 2px #ccc solid;">
	      <source src="one_shot_talking_face.mp4">
	    </video>
	  </center>
	</li>


	<li>ちなみに、 original author の github サイトはこちら<br />
	  github: <a href="https://github.com/FuxiVirtualHuman/AAAI22-one-shot-talking-face">FuxiVirtualHuman/AAAI22-one-shot-talking-face</a>
	  <center>
	    <a href="Screen Shot 2023-01-26 at 10.49.10.png"><!-- 2816x1094 -->
	      <img src="Screen Shot 2023-01-26 at 10.49.10_thumb.jpg" width="800" height="311" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>arxiv: <a href="https://arxiv.org/abs/2112.02749">2112.02749</a>
	      <center>
		<a href="Screen Shot 2023-01-26 at 10.51.15.png"><!-- 2833x1199 -->
		  <img src="Screen Shot 2023-01-26 at 10.51.15_thumb.jpg" width="800" height="339" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	  </ul>
	</li>


      </ul>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">目次に戻る</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2">
    <div style="font-size: 50px; font-weight: bolder;">
      パート２
      <br />
      技術書典１４
    </div>
    <br /><br />
  </center>

  <ul>
    <li><a href="https://twitter.com/techbookfest/status/1611267884969492481">https://twitter.com/techbookfest/status/1611267884969492481</a>
      <center>
	<a href="Screen Shot 2023-01-19 at 17.35.23.png"><!-- 1180x1025 -->
	  <img src="Screen Shot 2023-01-19 at 17.35.23_thumb.jpg" width="400" height="347" style="border: 2px #ccc solid;" /></a>
	<pre>
2023年5月21日(日) 池袋サンシャインシティ 展示ホールDにて技術書典14を開催決定！
同時に技術書典オンラインマーケットでも5月20日(土) から6月4日(日) まで開催📚
出展申込や参加方法などの詳細は後日発表✨
技術に出会えるイベントにみんなで参加しよう🐏 
#技術書典
	</pre>
      </center>
    </li>

    <li>企画
      <ul>
	<li><a href="#part2-ichiki">いちきの個人企画</a></li>
	<li><a href="#part2-ZAM">ZAM 季報 VOL.3</a></li>
      </ul>
    </li><!-- 企画 -->

  </ul>

  <h3 id="part2-ichiki">いちきの個人企画</h3>

  <ul>
    <li>「数理三部作」はどうなった？
      <center>
	<a href="math-trilogy.png"><!-- 3291x1485 -->
	  <img src="math-trilogy_thumb.jpg" width="800" height="361" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>３作目『空間の近似』は後回しにして
	  <center>
	    <a href="math-trilogy-plus-one.png"><!-- 4850x1663 -->
	      <img src="math-trilogy-plus-one_thumb.jpg" width="1000" height="343" style="border: 2px #ccc solid;" /></a>
	  </center>
	  第０作として『μ流体力学』を執筆中ですが……
	  <br /><br />
	  ……難航中……
	</li>
      </ul>
    </li><!-- 「数理三部作」はどうなった？ -->

    <li>新企画！
      <ul>
	<li>『エッセイ - 音楽と数理』</li>
	<li>『音楽と数理 🎼 ♾ ポッドキャスト』
	  の書き起こし原稿をベースに</li>
	<li>縦書きの「文庫本」を出そう！という企画</li>
	<li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202210/ichiki/#part0-3">ZAF-2210</a>
	  <center>
	    <a href="Screen Shot 2023-01-25 at 17.52.59.png"><!-- 2821x1102 -->
	      <img src="Screen Shot 2023-01-25 at 17.52.59_thumb.jpg" width="800" height="313" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

  </ul>

  <h3 id="part2-ZAM">ZAM 季報 VOL.3</h3>

  <ul>
    <li>これまでの『ZAM 季報』
      <ul>
	<li><a href="#part2-ZAM-vol1">ZAM 季報 VOL.1</a>
	  <center>
	    <a href="Screen Shot 2023-01-24 at 23.01.34.png"><!-- 2308x1173 -->
	      <img src="Screen Shot 2023-01-24 at 23.01.34_thumb.jpg" width="800" height="407" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>2018 ZENKEI AI SEMINAR のスタートから
	      2021/06 までの ZAF の内容をベースに</li>
	    <li>メンバーからの書き下ろし原稿で構成</li>
	  </ul>
	</li>
	<li><a href="#part2-ZAM-vol2">ZAM 季報 VOL.2</a>
	  <center>
	    <a href="Screen Shot 2023-01-24 at 23.01.43.png"><!-- 2302x1131 -->
	      <img src="Screen Shot 2023-01-24 at 23.01.43_thumb.jpg" width="800" height="393" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>2021/07/27 から 2022/08/31 までの ZAF の内容をベースに</li>
	    <li>メンバーからの書き下ろし原稿で構成</li>
	  </ul>
	</li>
      </ul>
    </li>

    <li>ZAM 季報 VOL.3
      <ul>
	<li>2022/09 から 2023/04 までの ZAF の内容をベースに</li>
	<li>および、メンバーからの書き下ろし原稿で構成予定！</li>
      </ul>
    </li><!-- ZAM 季報 VOL.3 -->

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">目次に戻る</a>）
  </div>


  
  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="epilogue">
    <div style="font-size: 50px; font-weight: bolder;">
      今日のおわりに
    </div>
  </center>

  <p>……</p>

  <h3>今後の予定</h3>
  <ul>
    <li>次回 ZAF は 2023 年 2 月 22 日開催の予定です。</li>
    <li>ZAF 講演者、 ZAM 執筆者、絶賛、大募集中です！<br />
      お気軽にお問い合わせください！</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />
  <hr />

  <h2 id="detailed-toc">総合目次</h2>
  <ul>
    <li><b>前座</b>
      <a href="#part0">１年の計は１月の ZAF にあり</a>
      <ul>
	<li><a href="#part0-musmath">音楽と数理ポッドキャスト</a></li>
	<li><a href="#part0-zap">ZENKEI AI ポッドキャスト</a></li>
      </ul>
    </li>

    <li><b>第１部</b>
      <a href="#part1">最近の話題から</a>
      <ul>
	<li><a href="#part1-OpenAI">OpenAI 関連</a>
	  <ul>
	    <li><a href="#part1-openai-context">ここ最近の流れ</a></li>
	    <li><a href="#part1-future">未来予想</a></li>
	    <li><a href="#part1-Society">AI と社会</a></li>
	  </ul>
	</li><!-- OpenAI 関連 -->
	<li><a href="#part1-others">その他の話題</a>
	  <ul>
	    <li><a href="#part1-Speech">音声合成関連</a>
	      <ul>
		<li><a href="#part1-valle">VALL-E</a></li>
		<li><a href="#part1-espnet">ESPnet</a></li>
	      </ul>
	    </li>
	    <li><a href="#part1-WiFi">WiFiのシグナルは重要な情報</a></li>
	    <li><a href="#part1-reviews">まとめ系の情報</a></li>
	    <li><a href="#part1-DeepMind">DeepMind</a></li>
	    <li><a href="#part1-misc">そのほか</a></li>
	  </ul>
	</li>
	<li><a href="#part1-demos">デモ</a>
	  <ul>
	    <li><a href="#part1-NeRF">NVIDIA Instant-NGP デモ</a></li>
	    <li><a href="#part1-one-shot-talking-face">One-Shot Talking Face デモ</a></li>
	  </ul>
	</li><!-- デモ -->
      </ul>
    </li>

    <li><b>第２部</b>
      <a href="#part2">技術書典１４</a>
      <ul>
	<li><a href="#part2-ichiki">いちきの個人企画</a></li>
	<li><a href="#part2-ZAM">ZAM 季報 VOL.3</a></li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

</section>

</article>

</body>           
</html>
