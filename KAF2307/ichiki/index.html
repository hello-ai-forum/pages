<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8" />
    <title>HELLO! AI FORUM (2023/07/29)</title>
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+1p:100,400,700&display=swap&subset=japanese" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../bright-M_PLUS_1p.css" />

    <link rel="stylesheet"
	  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="text/javascript" id="MathJax-script" async
	    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

<body style="font-size: 20px;">

<header>
<center><h1>HELLO! AI FORUM 2023/07/29</h1></center>
</header>

<article>

<section id="main">

  <center>
    <a href="HELLO_AI_FORUM_zoom_20230729-2488x1400.jpg"><!-- 2341x1400 -->
      <img src="HELLO_AI_FORUM_zoom_20230729-2488x1400_thumb.jpg" width="800" height="478" style="border: 2px #ccc solid;" /></a>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      こんにちわ！ AI FORUM<br />
      2023 年 7 月 29 日 （土曜日）
    </div>
    <br />
    <div style="font-size: 50px;">＜本日のテーマ＞</div>
    <div style="font-size: 70px;">
      夏休みもＡＩだ！
    </div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <h2 id="toc">目次</h2>
  <ul>

      </ul>
    </li><!-- ネタ -->

    <li>[6:30 - 7:00]
      <b>前座</b>　
      <a href="#part0">2023 年前半の振り返り</a>
    </li><!-- 前座 -->

    <li>[7:00 - 8:00]
      <b>パート１</b>
      <a href="#part1">AI の未来</a>
    </li><!-- パート１ -->

    <li>[8:00 - 9:00]
      <b>パート２</b>
      <a href="#part2">最近の LLMs</a>
    </li><!-- パート２ -->

    <li><a href="#epilogue">今日のおわりに</a></li>

    <li><a href="#detailed-toc">総合目次</a></li>
  </ul>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      前座
      <br />
      2023 年前半の振り返り
    </div>
    <br /><br />
    <div style="font-size: 40px;">
      （本題に入る前のウォーミングアップ）
    </div>
    <br /><br />
  </center>

  <ul>
    <li>今日は７月２９日</li>
    <li>今年ももう半分以上過ぎてしまいました
      <ul>
	<li>本当なら、先月、ちょうど半分ってことでやるべきネタでしたね</li>
      </ul>
    </li>

    <li>なぜ今？
      <ul>
	<li>「こんにちわ！ AI FORUM のポッドキャスト」
	  というものがあります</li>
	<li>週２回、水曜日と日曜日に、過去のフォーラムのイベントから
	  エピソードを切り出して配信しています
	</li>
	<li>それが、今週の水曜日（７月２６日）シーズン３７に入りました</li>
	<li>これが、やっと今年１月のイベントで、<br />
	  <a href="https://hello-ai.seesaa.net/article/499849673.html">
	    S37E01 （前座）１年の計は１月の ZAF にあり - その１、個人の目標</a>
	  <center>
	    <a href="Screen Shot 2023-07-29 at 15.23.54.png"><!-- 2748x1117 -->
	      <img src="Screen Shot 2023-07-29 at 15.23.54_thumb.jpg" width="800" height="325" style="border: 2px #ccc solid;" /></a>
	  </center>
	  というエピソードだったので、<br />
	  今年の目標、半分以上、進んでるかな？とおもったから
	</li>
      </ul>
    </li>

    <li>今年の目標は、
      今年１月のフォーラム (<a href="https://hello-ai-forum.github.io/pages/ZAF202301/ichiki/#part0">ZAF-2301</a>) にて
      <center>
	<a href="https://hello-ai-forum.github.io/pages/ZAF202301/ichiki/#part0">
	  <img src="Screen Shot 2023-07-29 at 15.27.21_thumb.jpg" width="600" height="250" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-1">
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      ピアノ
    </div>
      <br /><br />
    <div style="font-size: 40px; font-weight: bolder;">
      Twitch で頑張ってる！
    </div>
    <br /><br />
  </center>

  <ul>
    <li><a href="https://hello-ai-forum.github.io/pages/KAF2306/ichiki/#part01-2">KAF-2306</a> でもまとめました
      <center>
	<a href="https://hello-ai-forum.github.io/pages/KAF2306/ichiki/#part01-2">
	  <img src="Screen Shot 2023-07-29 at 15.32.06_thumb.jpg" width="800" height="359" style="border: 2px #ccc solid;" /></a>
	<br />
	<a href="https://hello-ai-forum.github.io/pages/KAF2306/ichiki/#part01-2">
	  <img src="Screen Shot 2023-07-29 at 15.31.01_thumb.jpg" width="800" height="205" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-2">
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      ポッドキャスト　その１
    </div>
    <br /><br />
    <div style="font-size: 40px; font-weight: bolder;">
      「こんにちわ！ AI FORUM のポッドキャスト」
    </div>
    <br />
    <div style="font-size: 30px; font-weight:">
      <a href="https://hello-ai.seesaa.net/">https://hello-ai.seesaa.net/</a>
    </div>
    <br /><br />
  </center>

  <center>
    <a href="Screen Shot 2023-07-29 at 15.37.17.png"><!-- 682x491 -->
      <img src="Screen Shot 2023-07-29 at 15.37.17_thumb.jpg" width="240" height="173" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.37.13.png"><!-- 678x495 -->
      <img src="Screen Shot 2023-07-29 at 15.37.13_thumb.jpg" width="240" height="175" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.37.10.png"><!-- 682x496 -->
      <img src="Screen Shot 2023-07-29 at 15.37.10_thumb.jpg" width="240" height="175" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.37.07.png"><!-- 676x542 -->
      <img src="Screen Shot 2023-07-29 at 15.37.07_thumb.jpg" width="240" height="192" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.37.04.png"><!-- 682x493 -->
      <img src="Screen Shot 2023-07-29 at 15.37.04_thumb.jpg" width="240" height="173" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.37.00.png"><!-- 682x494 -->
      <img src="Screen Shot 2023-07-29 at 15.37.00_thumb.jpg" width="240" height="174" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.36.55.png"><!-- 688x563 -->
      <img src="Screen Shot 2023-07-29 at 15.36.55_thumb.jpg" width="240" height="196" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.38.57.png"><!-- 1904x1414 -->
      <img src="Screen Shot 2023-07-29 at 15.38.57_thumb.jpg" width="600" height="446" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.38.50.png"><!-- 1902x1328 -->
      <img src="Screen Shot 2023-07-29 at 15.38.50_thumb.jpg" width="600" height="419" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.38.40.png"><!-- 1902x548 -->
      <img src="Screen Shot 2023-07-29 at 15.38.40_thumb.jpg" width="600" height="173" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.38.34.png"><!-- 1900x1414 -->
      <img src="Screen Shot 2023-07-29 at 15.38.34_thumb.jpg" width="600" height="447" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.38.18.png"><!-- 1905x1328 -->
      <img src="Screen Shot 2023-07-29 at 15.38.18_thumb.jpg" width="600" height="418" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.37.44.png"><!-- 1905x138 -->
      <img src="Screen Shot 2023-07-29 at 15.37.44_thumb.jpg" width="600" height="43" style="border: 2px #ccc solid;" /></a>
  </center>
  <center>
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      ３月から週２本のペースで
      <br />
      今年に入って５２エピソードをリリース
    </div>
    <br /><br />
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-3">
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      ポッドキャスト　その２
    </div>
    <br /><br />
    <div style="font-size: 40px; font-weight: bolder;">
      「音楽と数理 🎼 ♾ ポッドキャスト」
    </div>
    <br />
    <div style="font-size: 30px; font-weight:">
      <a href="https://music0math.wordpress.com/">https://music0math.wordpress.com/</a>
    </div>
    <br /><br />
  </center>

  <center>
    <a href="Screen Shot 2023-07-29 at 15.52.23.png"><!-- 2758x1123 -->
      <img src="Screen Shot 2023-07-29 at 15.52.23_thumb.jpg" width="800" height="326" style="border: 2px #ccc solid;" /></a>
    <br />
    <a href="Screen Shot 2023-07-29 at 15.52.33.png"><!-- 2733x813 -->
      <img src="Screen Shot 2023-07-29 at 15.52.33_thumb.jpg" width="400" height="119" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.52.42.png"><!-- 2739x830 -->
      <img src="Screen Shot 2023-07-29 at 15.52.42_thumb.jpg" width="400" height="121" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.52.48.png"><!-- 2737x945 -->
      <img src="Screen Shot 2023-07-29 at 15.52.48_thumb.jpg" width="400" height="138" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.52.54.png"><!-- 2736x888 -->
      <img src="Screen Shot 2023-07-29 at 15.52.54_thumb.jpg" width="400" height="130" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2023-07-29 at 15.52.58.png"><!-- 2733x967 -->
      <img src="Screen Shot 2023-07-29 at 15.52.58_thumb.jpg" width="400" height="142" style="border: 2px #ccc solid;" /></a>
  </center>
  <center>
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      こちらは週１本のペースで
      <br />
      今年に入って３０エピソードをリリース！
    </div>
    <br /><br />
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-4">
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      「英語の（紙の）本をアマゾンで世界に向けて売る」
    </div>
    <br /><br />
  </center>

  <ul>
    <li>という目標を掲げてました</li>
    <li>要するに、アマゾン Kindle Direct Publishing 使ってみよう、といはなし
      <center>
	(<a href="https://qiita.com/kichiki/items/1d54e3ba66694ac26f49">https://qiita.com/kichiki/items/1d54e3ba66694ac26f49</a>)
	<br />
	<a href="https://qiita.com/kichiki/items/1d54e3ba66694ac26f49">
	  <img src="Screen Shot 2023-07-29 at 16.02.07_thumb.jpg" width="600" height="383" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>５月下旬に開催された「技術書典１４」で出した本も、
      紙の本（Print On Demand）で、アマゾンから買えます
      <center>
	<a href="Screen Shot 2023-07-29 at 16.06.52.png"><!-- 1487x997 -->
	  <img src="Screen Shot 2023-07-29 at 16.06.52_thumb.jpg" width="600" height="402" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>『<a href="https://www.amazon.co.jp/dp/B0C7JG3GYS">エッセイ　音楽と数理　ポッドキャストは自由にする（抄）</a>』</li>
	<li>『<a href="https://www.amazon.co.jp/dp/B0C7JFHTDF">音楽と数理　才能にたよらない耳コピ</a>』</li>
	<li>『<a href="https://www.amazon.co.jp/dp/B0C7JCBC6P">厳密な計算　ふたつの球のなめらかなダンス</a>』</li>
      </ul>
      <center>
	<br />
	<div style="font-size: 40px; font-weight: bolder;">
	  「英語」の本は……
	  <br />
	  今年の後半、がんばるかな……？
	</div>
	<br /><br />
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part0-5">
    <br />
    <div style="font-size: 50px; font-weight: bolder;">
      まとめ
    </div>
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      2023 年前半の振り返り
      <br />
      今年の目標はここまで
      <br />
      順調に進んでいますね！
    </div>
    <br /><br />
  </center>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      パート１
      <br />
      AI の未来
    </div>
  </center>

  <ul>
    <li><a href="#part1-1">Jeremy Howard の「Delightenment」</a></li>
    <li><a href="#part1-2">最近の AI 業界の動き</a></li>
    <li><a href="#part1-3">The Frontier Model Forum</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-1">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      Jeremy Howard の
      <br />
      「Delightenment」論文
    </div>
  </center>

  <ul>
    <li>ぼく自信が、最近ずっと
      <center>
	<br />
	<div style="font-size: 40px; font-weight: bolder;">
	  「AI の将来」とか「人類に未来」とか、<br />
	  この辺が気になってる
	</div>
	<br /><br />
      </center>
      と言ってる手前、<br />
      Jeremy のこのツイート（じゃなくて「𝕏」かな？）<br />
      の記事は読んでおかないとダメだろう、と
      <br /><br />
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">
	<tr>
	  <td>
	    <a href="https://twitter.com/jeremyphoward/status/1678558165712113664">https://twitter.com/jeremyphoward/status/1678558165712113664</a>
	    <div style="float: left; padding-right:1em;">
	      9:13 AM · Jul 11, 2023<br />
	      <a href="Screen Shot 2023-07-11 at 10.58.26.png"><!-- 1179x1052 -->
		<img src="Screen Shot 2023-07-11 at 10.58.26_thumb.jpg" width="400" height="357" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
I’ve spent the last few months interviewing >60 experts
in law, economics, AI, alignment, etc, on the impacts of AI,
and safety interventions.

Today I’m publishing my first article, showing regulation designed
to increase AI safety may backfire badly!

<a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">https://www.fast.ai/posts/2023-11-07-dislightenment.html</a>
	    </pre>
	  </td>
	</tr>
      </table>
    </li>

    <br />

    <li>「<a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">AI Safety and the Age of Dislightenment</a>」
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  AI の安全性と、反啓蒙時代
	</div>
	<br />
	<a href="Screen Shot 2023-07-17 at 18.41.54.png"><!-- 2750x1246 -->
	  <img src="Screen Shot 2023-07-17 at 18.41.54_thumb.jpg" width="800" height="362" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>一通り、目を通す
      <ul>
	<li>regulation も、
	  一旦実施してしまうと、簡単には undo できないので、
	  慎重にすべきだ、と。
	</li>
	<li>"FAR" は、 OpenAI とか Google が言ってるやつ</li>
	<li>ちなみに、 The Enlightment (Age of Enlightment) は「啓蒙時代」
	  <ul>
	    <li>啓蒙時代は、ヨーロッパで啓蒙思想が主流となっていた
	      17世紀後半から18世紀にかけての時代のこと。
	    </li>
	    <li>タイトルの「Dislightenment」は、したがって「啓蒙時代」の反対
	      というニュアンスでの造語かな。
	    </li>
	    <li>読み始めるまで、ずっと「disalignment」だと思い込んでた。</li>
	  </ul>
	</li>
      </ul>
    </li>

    <li>内容をまとめると、
      <ul>
	<li>きちんと心配しよう（心配いらん、という楽観派ではない）</li>
	<li>規制は、かけるならモデルではなく「利用」に</li>
	<li>モデルは公開にすべき（オープンソース的な考え方）</li>
	<li>その意味での「啓蒙時代を思い出そう」というメッセージ</li>
      </ul>
      みたいな感じかな。<br />
      さっと読んだだけなので、あとでゆっくり見返そう。
    </li>

    <li>ジェレミーはいつも前向きで、建設的で、現実的だなと思った。</li>

    <li>ジェレミーとは別な意味で、前向きと言うか、
      素朴な人だと思ってるレックスフリードマンは、
      直近、イスラエルに行ってたみたいで、ぼくは昨日、ユバルハラリのビデオ
      (<a href="https://youtu.be/Mde2q7GFCrw">https://youtu.be/Mde2q7GFCrw</a>) をゆっくり見た。
    </li>
    <li>このジェレミーの話は、間接的に、その内容とも呼応しているように思った。
      もちろん、ユバルハラリも AI のことも念頭に話してたが、
      「今、イスラエルで話している」ことの方が多分、大きな意味があって、
      必然的に、社会とか政治とかが軸足にあった。
      その上で「今、危機に瀕しているのは、民主主義だ」というメッセージだったと思う。
    </li>

    <li>ジェレミーも、啓蒙主義とか、オープンソースとか言ってるが、
      要するに「民主主義」だな、と。
    </li>

    <li>今の日本を振り返ると、実のところ昔からずっと権威主義的社会だったし、
      最近はむしろそれが更に加速してるのだな、と思った。
      あれかね、日本人はコンピュータによる汚染に世界で一番感度高いのかもね。
      「ツイッターの利用率メチャ高」とかイーロンに驚かれてたりしてたし。
      これは、よいことではないと思う。
    </li>

    <li>清書したツイート
      (<a href="https://twitter.com/ichiki_k/status/1681917350441021443">https://twitter.com/ichiki_k/status/1681917350441021443</a>)
      <center>
	<a href="Screen Shot 2023-07-21 at 14.17.09.png"><!-- 1187x1217 -->
	  <img src="Screen Shot 2023-07-21 at 14.17.09_thumb.jpg" width="390" height="400" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2023-07-21 at 14.17.58.png"><!-- 1182x1000 -->
	  <img src="Screen Shot 2023-07-21 at 14.17.58_thumb.jpg" width="400" height="338" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-2">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      AI 業界の動き
    </div>
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/DrJimFan/status/1676650766029963264">https://twitter.com/DrJimFan/status/1676650766029963264</a>
	    <div style="float: left; padding-right:1em;">
	      2:54 AM · Jul 6, 2023<br />
	      <a href="Screen Shot 2023-07-10 at 18.40.50.png"><!-- 1179x1339 -->
		<img src="Screen Shot 2023-07-10 at 18.40.50_thumb.jpg" width="352" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
OpenAI’s alignment strategy says that “humans won’t be able to
reliably supervise AI systems much smarter than us”.
But I think we can move humans up the supervision chain,
i.e. “feedback to feedback”.

Let’s consider the concrete example of writing malware.
It’s very likely that GPT in the next few years can blend in
malicious code so well that even the top engineers can’t detect. 

A well-aligned feedback model studies the code, discusses places
where potential virus may occur,
and explains its findings in intuitive terms.
Then human expert can decide whether the judgements are correct or not,
giving feedback to improve the feedback model,
which in turn steers the main model better.

<a href="https://openai.com/blog/introducing-superalignment">https://openai.com/blog/introducing-superalignment</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/GoogleDeepMind/status/1678767468356210689">https://twitter.com/GoogleDeepMind/status/1678767468356210689</a>
	    <div style="float: left; padding-right:1em;">
	      11:05 PM · Jul 11, 2023<br />
	      <a href="Screen Shot 2023-07-12 at 9.51.58.png"><!-- 1186x1186 -->
		<img src="Screen Shot 2023-07-12 at 9.51.58_thumb.jpg" width="400" height="400" style="border: 2px #ccc solid;" /></a>
	      <br />
	      <a href="Screen Shot 2023-07-12 at 9.52.29.png"><!-- 2880x1800 -->
		<img src="Screen Shot 2023-07-12 at 9.52.29_thumb.jpg" width="400" height="250" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
What could the future of global AI governance look like? 🌐

In our latest white paper, we explore theoretical options for
international institutions to help harness the benefits and
manage the risks of advanced artificial intelligence.

Find out more: <a href="https://dpmd.ai/46LXUdR">https://dpmd.ai/46LXUdR</a>

<a href="https://www.deepmind.com/blog/exploring-institutions-for-global-ai-governance">https://www.deepmind.com/blog/exploring-institutions-for-global-...</a>

<a href="https://arxiv.org/abs/2307.04699">https://arxiv.org/abs/2307.04699</a>

local copy: <a href="arxiv-2307.04699.pdf">arxiv-2307.04699.pdf</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/bioshok3/status/1681480047105019907">https://twitter.com/bioshok3/status/1681480047105019907</a>
	    <div style="float: left; padding-right:1em;">
	      10:44 AM · Jul 19, 2023<br />
	      <a href="Screen Shot 2023-07-19 at 11.00.05.png"><!-- 1177x678 -->
		<img src="Screen Shot 2023-07-19 at 11.00.05_thumb.jpg" width="400" height="230" style="border: 2px #ccc solid;" /></a>
	      <br />
	      <a href="Screen Shot 2023-07-19 at 11.00.15.png"><!-- 1173x1078 -->
		<img src="Screen Shot 2023-07-19 at 11.00.15_thumb.jpg" width="400" height="368" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
メタとマイクロソフトが手を組むってやばくないか？
法人向け消費者向けVR市場を全部取り、
WindowsでPCOSは支配していくことを考えると
バーチャル上のAIモデルの主流はメタ、マイクロソフト、OpenAI産になり、
GoogleDeepMindは昔からやってるロボティクスで現実世界で覇権をにぎるか？

<a href="https://twitter.com/nikkei/status/1681428633083322368">https://twitter.com/nikkei/status/1681428633083322368</a>

メタが生成AIでMicrosoftと提携します。
クラウド大手と生成AIの基盤技術を手掛ける企業の提携が相次ぐワケは。
大規模なデータセンターが開発や利用に必須になっていることが背景にあります。

<a href="https://nikkei.com/article/DGXZQOGN186BL0Y3A710C2000000/?n_cid=SNSTW007">https://nikkei.com/article/DGXZQOGN186BL0Y3A710C2000000/?n_cid=SNSTW007</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/alex_valaitis/status/1681348531834044426">https://twitter.com/alex_valaitis/status/1681348531834044426</a>
	    <div style="float: left; padding-right:1em;">
	      2:01 AM · Jul 19, 2023<br />
	      <a href="Screen Shot 2023-07-19 at 12.53.10.png"><!-- 1182x1200 -->
		<img src="Screen Shot 2023-07-19 at 12.53.10_thumb.jpg" width="394" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
🚨BREAKING: Zuck just announced that Meta will be
open sourcing Llama 2 with Microsoft.

This is a massive announcement for a few reasons...

1) This could kill a number of the open source LLM startups.

Mosaic, Red Pajama, etc. are in major trouble.

The $1.2 billion @MosaicML acquisition
looks especially terrible in the wake of this news.

2) This thrusts Meta onto the AI scene. 

With this announcement,
Zuck is signaling how strong Meta’s AI position is. 

They will now own one of the most widely adopted LLMs
+ have one of the best training data sets in the world.

3) This further strengthens @Microsoft ’s dominant position in the AI space.

With this partnership they now have exclusive partnerships
with the top LLMs (OpenAI, Meta), priority access to @nvidia GPUs,
and strategic assets like GitHub and Azure.

The Game of AI Thrones has just taken another twist. 

Interested to see what happens next!
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/shujisado/status/1682366591462440961">https://twitter.com/shujisado/status/1682366591462440961</a>
	    <div style="float: left; padding-right:1em;">
	      9:27 PM · Jul 21, 2023<br />
	      <a href="Screen Shot 2023-07-24 at 11.15.19.png"><!-- 1177x1071 -->
		<img src="Screen Shot 2023-07-24 at 11.15.19_thumb.jpg" width="400" height="364" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
ふーむ。どうも緊急で
Amazon、Anthropic、Google、Inflection、Meta、Microsoft、OpenAI
のAI企業7社がホワイトハウスに招集され、
AI技術の安全性、透明性の確保にむけて自主的な取り組みを行うことを約束させられたようだ。
リリースで公表された項目は八つ。

<a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/">https://www.whitehouse.gov/briefing-room/statements-releases/...</a>
	    </pre>
	  </td>
	</tr>
      </table>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-3">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      The Frontier Model Forum
    </div>
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">
	<tr>
	  <td>
	    <a href="https://twitter.com/GoogleDeepMind/status/1684143225978724354">https://twitter.com/GoogleDeepMind/status/1684143225978724354</a>
	    <div style="float: left; padding-right:1em;">
	      7:06 PM · Jul 26, 2023<br />
	      <a href="Screen Shot 2023-07-26 at 19.22.09.png"><!-- 1172x1050 -->
		<img src="Screen Shot 2023-07-26 at 19.22.09_thumb.jpg" width="400" height="358" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
We’re excited to support the launch of the Frontier Model Forum
- a new effort to ensure safe and responsible development
of frontier AI systems featuring @Google , @AnthropicAI , @Microsoft
and @OpenAI .

Find out more:

<a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/">https://blog.google/outreach-initiatives/public-policy/google-...</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/OpenAI/status/1684145154628653056">https://twitter.com/OpenAI/status/1684145154628653056</a>
	    <div style="float: left; padding-right:1em;">
	      7:14 PM · Jul 26, 2023<br />
	      <a href="Screen Shot 2023-07-26 at 19.28.27.png"><!-- 1184x981 -->
		<img src="Screen Shot 2023-07-26 at 19.28.27_thumb.jpg" width="400" height="331" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Announcing Frontier Model Forum, an industry body co-founded
with @anthropicAI , @Google , @googledeepmind , and @microsoft
focused on ensuring safe development of future hyperscale AI models:

<a href="https://openai.com/blog/frontier-model-forum">https://openai.com/blog/frontier-model-forum</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/demishassabis/status/1684142735727394817">https://twitter.com/demishassabis/status/1684142735727394817</a>
	    <div style="float: left; padding-right:1em;">
	      7:04 PM · Jul 26, 2023<br />
	      <a href="Screen Shot 2023-07-26 at 19.32.34.png"><!-- 1181x1030 -->
		<img src="Screen Shot 2023-07-26 at 19.32.34_thumb.jpg" width="400" height="349" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Thrilled to bring @GoogleDeepMind ’s expertise on responsible AI
to the new Frontier Model Forum and look forward to collaborating
with policymakers, academics, civil society & companies
to advance AI safety.
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/bioshok3/status/1684150211604803584">https://twitter.com/bioshok3/status/1684150211604803584</a>
	    <div style="float: left; padding-right:1em;">
	      7:34 PM · Jul 26, 2023<br />
	      <a href="Screen Shot 2023-07-27 at 17.40.43.png"><!-- 1180x530 -->
		<img src="Screen Shot 2023-07-27 at 17.40.43_thumb.jpg" width="400" height="180" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
なるほど、国際的な高度なAI規制機関が作成される前に業界団体
（Google、GoogleDeepMind、Anthropic、Open AI、Microsoft）
として取り組みやすい形で設立した模様だ。METAも入ってほしいな。
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

    <br />

    <li>「The Frontier Model Forum」というのは、
      <ul>
	<li>Google + DeepMind, Microsoft + OpenAI, AnthropicAI</li>
      </ul>
    </li>

    <li>先の White House に呼ばれた話ってのは、また別のはなし
      <ul>
	<li>こっちは「Amazon、Anthropic、Google、Inflection、Meta、Microsoft、OpenAI のAI企業7社」と</li>
      </ul>
    </li>

  </ul>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2">
    <div style="font-size: 50px; font-weight: bolder;">
      パート２
      <br />
      最近の LLMs
    </div>
    <br /><br />
  </center>

  <ul>
    <li><a href="#part2-0">ポスト Transformer 時代の LLMs</a></li>
    <li><a href="#part2-1">Llama 2 発表</a></li>
    <li><a href="#part2-2">LlaMA.cpp で Llama 2</a></li>
    <li><a href="#part2-3">Andrej Karpathy の llama2.c</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-0">
    <div style="font-size: 50px; font-weight: bolder;">
      ポスト Transformer 時代の LLMs
    </div>
    <br /><br />
  </center>

  <center id="part2-0-1">
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      RWKV (Receptance Weighted Key Value)
    </div>
    <br /><br />
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/_akhaliq/status/1660816265454419969">https://twitter.com/_akhaliq/status/1660816265454419969</a>
	    <div style="float: left; padding-right:1em;">
	      10:13 AM · May 23, 2023<br />
	      <a href="Screen Shot 2023-05-23 at 10.45.47.png"><!-- 1184x1208 -->
		<img src="Screen Shot 2023-05-23 at 10.45.47_thumb.jpg" width="392" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
RWKV: Reinventing RNNs for the Transformer Era

propose a novel model architecture,
Receptance Weighted Key Value (RWKV),
that combines the efficient parallelizable training of Transformers
with the efficient inference of RNNs.
Our approach leverages a linear attention mechanism and allows us
to formulate the model as either a Transformer or an RNN,
which parallelizes computations during training
and maintains constant computational
and memory complexity during inference,
leading to the first non-transformer architecture to be scaled
to tens of billions of parameters. Our experiments reveal
that RWKV performs on par with similarly sized Transformers,
suggesting that future work can leverage this architecture
to create more efficient models.
This work presents a significant step
towards reconciling the trade-offs between computational efficiency
and model performance in sequence processing tasks.

paper page: <a href="https://huggingface.co/papers/2305.13048">https://huggingface.co/papers/2305.13048</a>

<a href="https://arxiv.org/abs/2305.13048">https://arxiv.org/abs/2305.13048</a>

local copy: <a href="arxiv-2305.13048.pdf">arxiv-2305.13048.pdf</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/__genzitsu__/status/1669158661313400832">https://twitter.com/__genzitsu__/status/1669158661313400832</a>
	    <div style="float: left; padding-right:1em;">
	      10:43 AM · Jun 15, 2023<br />
	      <a href="Screen Shot 2023-06-15 at 15.56.30.png"><!-- 1185x1036 -->
		<img src="Screen Shot 2023-06-15 at 15.56.30_thumb.jpg" width="400" height="350" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
思ったよりもすごいなRWKV

RWKV（Receptance Weighted Key Value）をつかってみた

<a href="https://blog.brainpad.co.jp/entry/2023/06/14/144554">https://blog.brainpad.co.jp/entry/2023/06/14/144554</a>
	    </pre>
	  </td>
	</tr>

      </table>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-0-2">
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      FlashAttention-2
    </div>
    <br /><br />
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/ImAI_Eruel/status/1681144116229578752">https://twitter.com/ImAI_Eruel/status/1681144116229578752</a>
	    <div style="float: left; padding-right:1em;">
	      12:29 PM · Jul 18, 2023<br />
	      <a href="Screen Shot 2023-07-18 at 13.44.13.png"><!-- 1177x1123 -->
		<img src="Screen Shot 2023-07-18 at 13.44.13_thumb.jpg" width="400" height="382" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
２倍以上のスピードで大規模言語モデルを学習できるようになるかもしれません．
ハードウェア自体に着目した高速化手法FlashAttentionの新作が出ました．
"FlashAttention-2:
 Faster Attention with Better Parallelism and Work Partitioning"
<a href="https://tridao.me/publications/flash2/flash2.pdf">https://tridao.me/publications/flash2/flash2.pdf</a>
ドラゴンボール並みのインフレ
	    </pre>
	  </td>
	</tr>
      </table>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-0-3">
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      Retentive Network (RetNet)
    </div>
    <br /><br />
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/RosaRugosaBeach/status/1681138056106233858">https://twitter.com/RosaRugosaBeach/status/1681138056106233858</a>
	    <div style="float: left; padding-right:1em;">
	      12:05 PM · Jul 18, 2023<br />
	      <a href="Screen Shot 2023-07-18 at 13.44.46.png"><!-- 1183x550 -->
		<img src="Screen Shot 2023-07-18 at 13.44.46_thumb.jpg" width="400" height="186" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Transformerの後継となるべく新たに提案されたRetentive Network、面白い

並列処理と再帰構造をうまく組み合わせた仕組みで、
メモリ消費や推論効率が改善しているほか、
2B以上の規模になると精度も上回り始める
（昨今のLLMとしてのベンチマークがどうなるかは気になるが）

<a href="https://arxiv.org/abs/2307.08621">https://arxiv.org/abs/2307.08621</a>

local copy: <a href="arxiv-2307.08621.pdf">arxiv-2307.08621.pdf</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/arankomatsuzaki/status/1681113977500184576">https://twitter.com/arankomatsuzaki/status/1681113977500184576</a>
	    <div style="float: left; padding-right:1em;">
	      10:29 AM · Jul 18, 2023<br />
	      <a href="Screen Shot 2023-07-18 at 13.48.19.png"><!-- 1172x1165 -->
		<img src="Screen Shot 2023-07-18 at 13.48.19_thumb.jpg" width="400" height="398" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs,
simultaneously achieving training parallelism, low-cost inference,
and good performance.

<a href="https://arxiv.org/abs/2307.08621">https://arxiv.org/abs/2307.08621</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/_kaiinui/status/1681169026658217985">https://twitter.com/_kaiinui/status/1681169026658217985</a>
	    <div style="float: left; padding-right:1em;">
	      2:08 PM · Jul 18, 2023<br />
	      <a href="Screen Shot 2023-07-18 at 17.33.23.png"><!-- 1185x1050 -->
		<img src="Screen Shot 2023-07-18 at 17.33.23_thumb.jpg" width="400" height="354" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
話題の(?) RetNet, O(1) ということでグラフを見てみたらすごい

1. Context 長に対してメモリがスケールしない
2. Context 長に対して処理時間がスケールしない
3. 入力長に対して処理時間がスケールしない
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/nkmry_/status/1681235176574324736">https://twitter.com/nkmry_/status/1681235176574324736</a>
	    <div style="float: left; padding-right:1em;">
	      6:31 PM · Jul 18, 2023<br />
	      <a href="Screen Shot 2023-07-18 at 18.44.50.png"><!-- 1178x742 -->
		<img src="Screen Shot 2023-07-18 at 18.44.50_thumb.jpg" width="400" height="252" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
<a href="https://arxiv.org/abs/2307.08621">https://arxiv.org/abs/2307.08621</a>
RetNet (Retentive Network) は RWKV と同等に高速でメモリ効率が良く、
Transformer 並の性能が出せるアーキテクチャで、Microsoft と清華大学が開発。

RWKV や H3 など高速なアーキテクチャは出てきていたが、
更に性能まで Transformer に追いついたのはすごい！
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/hillbig/status/1681417687380152320">https://twitter.com/hillbig/status/1681417687380152320</a>
	    <div style="float: left; padding-right:1em;">
	      6:36 AM · Jul 19, 2023<br />
	      <a href="Screen Shot 2023-07-19 at 8.15.25.png"><!-- 1184x512 -->
		<img src="Screen Shot 2023-07-19 at 8.15.25_thumb.jpg" width="400" height="173" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Retentive Network (RetNet)は、
並列学習、Context長に依存しない効率的な推論、高い表現力を同時に達成する。
線形RNNの遷移行列を対角化して並列に計算できるようにしたRetentionを、
チャンク毎にも適用し長距離依存を捉える。
Transformerに速度、性能面で並ぶか上回る

<a href="https://arxiv.org/abs/2307.08621">https://arxiv.org/abs/2307.08621</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/heat_1nt/status/1684447745808171008">https://twitter.com/heat_1nt/status/1684447745808171008</a>
	    <div style="float: left; padding-right:1em;">
	      3:16 PM · Jul 27, 2023<br />
	      <a href="Screen Shot 2023-07-28 at 11.20.56.png"><!-- 1175x325 -->
		<img src="Screen Shot 2023-07-28 at 11.20.56_thumb.jpg" width="400" height="111" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
transformerの後継と称される
RetNetもう実装出てた ワクワク

<a href="https://github.com/microsoft/unilm/tree/master/retnet">https://github.com/microsoft/unilm/tree/master/retnet</a>
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-0-4">
    <br />
    <div style="font-size: 40px; font-weight: bolder;">
      「Textbooks Are All You Need」論文
      <br />
      phi-1
    </div>
    <br /><br />
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/EldanRonen/status/1658321669407248387">https://twitter.com/EldanRonen/status/1658321669407248387</a>
	    <div style="float: left; padding-right:1em;">
	      1:01 PM · May 16, 2023<br />
	      <a href="Screen Shot 2023-07-28 at 22.11.51.png"><!-- 1178x1310 -->
		<img src="Screen Shot 2023-07-28 at 22.11.51_thumb.jpg" width="360" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Will future LLMs be based almost entirely on synthetic training data?
In a new paper, we introduce TinyStories,
a dataset of short stories generated by GPT-3.5&4.
We use it to train tiny LMs (&lt; 10M params)
that produce fluent stories and exhibit reasoning.
<a href="https://arxiv.org/abs/2305.07759">https://arxiv.org/abs/2305.07759</a>

local copy: <a href="arxiv-2305.07759.pdf">arxiv-2305.07759.pdf</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/SebastienBubeck/status/1671326369626853376">https://twitter.com/SebastienBubeck/status/1671326369626853376</a>
	    <div style="float: left; padding-right:1em;">
	      10:17 AM · Jun 21, 2023<br />
	      <a href="Screen Shot 2023-07-28 at 22.12.09.png"><!-- 1188x943 -->
		<img src="Screen Shot 2023-07-28 at 22.12.09_thumb.jpg" width="400" height="318" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
New LLM in town:

***phi-1 achieves 51% on HumanEval
w. only 1.3B parameters & 7B tokens training dataset***

Any other &gt;50% HumanEval model is &gt;1000x bigger
(e.g., WizardCoder from last week is 10x in model size
and 100x in dataset size).

How?

***Textbooks Are All You Need***
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/SebastienBubeck/status/1671326371921154049">https://twitter.com/SebastienBubeck/status/1671326371921154049</a>
	    <div style="float: left; padding-right:1em;">
	      10:17 AM · Jun 21, 2023<br />
	      <a href="Screen Shot 2023-07-28 at 21.24.15.png"><!-- 1181x641 -->
		<img src="Screen Shot 2023-07-28 at 21.24.15_thumb.jpg" width="400" height="217" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Full details in the paper: <a href="https://arxiv.org/abs/2306.11644">https://arxiv.org/abs/2306.11644</a>

Awesome collaboration with our (also awesome) @MSFTResearch team!

Cc a few authors with an active twitter account: 
@EldanRonen (we follow-up on his TinyStories w. Yuanzhi Li!) 
@JyotiAneja @sytelus @AdilSlm @YiZhangZZZ @xinw_ai
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/_akhaliq/status/1671360619986010112">https://twitter.com/_akhaliq/status/1671360619986010112</a>
	    <div style="float: left; padding-right:1em;">
	      12:33 PM · Jun 21, 2023<br />
	      <a href="Screen Shot 2023-06-21 at 13.19.29.png"><!-- 1182x1029 -->
		<img src="Screen Shot 2023-06-21 at 13.19.29_thumb.jpg" width="400" height="348" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Textbooks Are All You Need

paper page: <a href="https://huggingface.co/papers/2306.11644">https://huggingface.co/papers/2306.11644</a>

introduce phi-1, a new large language model for code,
with significantly smaller size than competing models:
phi-1 is a Transformer-based model with 1.3B parameters,
trained for 4 days on 8 A100s, using a selection of
``textbook quality'' data from the web (6B tokens)
and synthetically generated textbooks and exercises
with GPT-3.5 (1B tokens). Despite this small scale,
phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.
It also displays surprising emergent properties
compared to phi-1-base, our model before our finetuning stage
on a dataset of coding exercises,
and phi-1-small, a smaller model with 350M parameters
trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/karpathy/status/1671587087542530049">https://twitter.com/karpathy/status/1671587087542530049</a>
	    <div style="float: left; padding-right:1em;">
	      3:33 AM · Jun 22, 2023<br />
	      <a href="Screen Shot 2023-07-28 at 21.20.29.png"><!-- 1181x558 -->
		<img src="Screen Shot 2023-07-28 at 21.20.29_thumb.jpg" width="400" height="189" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
"Textbooks Are All You Need" is making rounds:
<a href="https://twitter.com/SebastienBubeck/status/1671326369626853376">https://twitter.com/SebastienBubeck/status/1671326369626853376</a>
reminding me of my earlier tweet :).
TinyStories is also an inspiring read:
<a href="https://twitter.com/EldanRonen/status/1658321669407248387">https://twitter.com/EldanRonen/status/1658321669407248387</a>
We’ll probably see a lot more creative "scaling down" work:
prioritizing data quality and diversity over quantity,
a lot more synthetic data generation,
and small but highly capable expert models.

<a href="https://twitter.com/karpathy/status/1509289133637832705">https://twitter.com/karpathy/status/1509289133637832705</a>

Seems likely we’ll have custom (and partially auto-generated) “textbooks”
but for teaching language models, not humans, to help them “grok” concepts.
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/hillbig/status/1671643297616654342">https://twitter.com/hillbig/status/1671643297616654342</a>
	    <div style="float: left; padding-right:1em;">
	      7:16 AM · Jun 22, 2023<br />
	      <a href="Screen Shot 2023-06-22 at 16.51.26.png"><!-- 1180x574 -->
		<img src="Screen Shot 2023-06-22 at 16.51.26_thumb.jpg" width="400" height="195" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
LLMの学習ではデータの質が重要であり、
明確、自己完結、有益でバランスされている「教科書」のようなデータと
微調整用「練習問題」を既存LLMによるフィルタリングと生成で用意。
結果のphi-1は1/10のモデルサイズ、1/100のデータ量で
コード向け既存OSS LLMを超える性能を達成

<a href="https://arxiv.org/abs/2306.11644">https://arxiv.org/abs/2306.11644</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/Beluuuuuuga/status/1684529301293776897">https://twitter.com/Beluuuuuuga/status/1684529301293776897</a>
	    <div style="float: left; padding-right:1em;">
	      8:40 PM · Jul 27, 2023<br />
	      <a href="Screen Shot 2023-07-27 at 21.41.51.png"><!-- 1185x396 -->
		<img src="Screen Shot 2023-07-27 at 21.41.51_thumb.jpg" width="400" height="134" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Microsoftからやばそうな論文がでてた

<a href="https://arxiv.org/abs/2306.11644">https://arxiv.org/abs/2306.11644</a>

local copy: <a href="arxiv-2306.11644.pdf">arxiv-2306.11644.pdf</a>
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-1">
    <div style="font-size: 50px; font-weight: bolder;">
      Llama 2 発表
    </div>
    <br /><br />
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

    <tr>
      <td>
	<a href="https://twitter.com/tmiyatake1/status/1669495679633485824">https://twitter.com/tmiyatake1/status/1669495679633485824</a>
	<div style="float: left; padding-right:1em;">
	  <a href="Screen Shot 2023-06-20 at 19.29.49.png"><!-- 1179x1102 -->
	    <img src="Screen Shot 2023-06-20 at 19.29.49_thumb.jpg" width="400" height="374" style="border: 2px #ccc solid;" /></a>
	</div>
	<pre>
Metaが自社開発したLLM「LLaMA」の次のバージョンを開発しているが、
今回は商業利用出来るように作っている。

現在のLLaMAは研究利用でしか使えないが、
これを商業利用で使えるとかなり面白い展開になりそう。

<a href="https://www.theinformation.com/articles/meta-wants-companies-to-make-money-off-its-open-source-ai-in-challenge-to-google">https://www.theinformation.com/articles/meta-wants-companies-to-...</a>
	</pre>
      </td>
    </tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/MetaAI/status/1681363272484945921">https://twitter.com/MetaAI/status/1681363272484945921</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-19 at 8.14.16.png"><!-- 1182x946 -->
		<img src="Screen Shot 2023-07-19 at 8.14.16_thumb.jpg" width="400" height="320" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
We believe an open approach is the right one
for the development of today’s Al models.

Today, we’re releasing Llama 2,
the next generation of Meta’s open source Large Language Model,
available for free for research & commercial use.

Details ➡️ <a href="https://bit.ly/3Dh9hNp">https://bit.ly/3Dh9hNp</a>

<a href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/hillbig/status/1681436336451125257">https://twitter.com/hillbig/status/1681436336451125257</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-19 at 10.57.12.png"><!-- 1181x352 -->
		<img src="Screen Shot 2023-07-19 at 10.57.12_thumb.jpg" width="400" height="119" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Llama2は学習データを2Tトークンに増やしコンテキスト長を4KにしGQAを採用。
報告書では有用性と安全性の向上に向けたSFTとRLHFの詳細が充実している。

SFTは量より質が大事。
2万程度作るとSFT向けアノテーションデータは生成と人手が区別できないレベルとなる。

5章の議論が興味深い
1) 研究コミュニティではSFTが注目されているが、SFTよりRLHFが効果的。
SFTアノテーターがたまに低品質な教師データを作り、SFTがそれに引っ張られるが、
Rewordフィードバックはそれが少ない。
またSFTは学習対象がアノテーターの能力に限定されしまうのに対し、
RLHFはLLMが持つ創作能力がアノテーターを超えていくことができることができる。

2) サンプリング時の温度は、
創作的なことを生成する場合は温度を反映し多様的なのを作るのに対し、
事実を述べる場合は温度を無視し同じのを生成するように勝手になる

3) 次の単語予測しかしていないにも関わらず、時間を理解しており、
例えば今の時代をSFTで調整するとそれに合わせて回答し、
例えば今の時代が852年と調整すると地球は平らか丸いかを知らないと答える
（LLMが空間も把握しているのは既報）

4) ツールを使う能力は追加学習せずとも事前学習時で発現する

<a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-...</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/snakajima/status/1682164910359461888">https://twitter.com/snakajima/status/1682164910359461888</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-21 at 10.27.28.png"><!-- 1176x1320 -->
		<img src="Screen Shot 2023-07-21 at 10.27.28_thumb.jpg" width="356" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Metaが、LLM（大規模言語モデル）であるLlama2をオープンソース化しましたが、
業界全体に大きな激震を与えています。
なぜそんなに大きな意味を持つのかを連投で書きます。（続く）
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-1-1">
    <div style="font-size: 40px; font-weight: bolder;">
      Llama 2 をダウンロード
    </div>
    <br /><br />
  </center>

  <ul>
    <li><a href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a>
      <center>
	<a href="Screen Shot 2023-07-27 at 22.02.52.png"><!-- 2751x1403 -->
	  <img src="Screen Shot 2023-07-27 at 22.02.52_thumb.jpg" width="400" height="204" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>論文：
      (<a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/</a>)
      <center>
	<a href="Screen Shot 2023-07-27 at 22.15.32.png"><!-- 2755x1416 -->
	  <img src="Screen Shot 2023-07-27 at 22.15.32_thumb.jpg" width="400" height="206" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>"Llama 2: Open Foundation and Fine-Tuned Chat Models"
	  <br />
	  (local copy: <a href="10000000_662098952474184_2584067087619170692_n.pdf">PDF</a>)
	  <center>
	    <object data="10000000_662098952474184_2584067087619170692_n.pdf"
		    type="application/pdf"
		    width="500" height="700">
	      link to PDF:
	      <a href="10000000_662098952474184_2584067087619170692_n.pdf">PDF</a>
	    </object>
	  </center>
	</li>
      </ul>
    </li>

    <li>ダウンロードは手順通りにすれば、問題ない
      <ul>
	<li><a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a> からダウンロードスクリプトをクローン</li>
	<li>Mac の場合、 md5sum がデフォルトでは入ってないので、
	  適宜、インストール
	  <ul>
	    <li>ぼくは macports を使ってるので
	      <pre>
$ sudo port install -v md5sha1sum
	      </pre>
	    </li>
	  </ul>
	</li>
	<li>download.sh を実行して、要求される URL をメールからコピペして、<br />
	  ダウンロードしたいモデルを指定すれば、<br />
	  ダウンロード開始される
	  <ul>
	    <li>途中でトラブって、ファイルの残骸がある状態でも、<br />
	      やり直すと、全部ダウンロードし直します</li>
	    <li>（これ、チェックサムあるので、大丈夫なファイルはスキップすればいいのに……<br />
	      って、それなら自分でシェルクスリプトにパッチ書け、ということだな<br />
	      （シェルスクリプト、詳しくないので、ごめんなさい））</li>
	  </ul>
	</li>
	<li>モデルは（今のところ）以下の６種類
	  <pre>
llama-2-13b
llama-2-13b-chat
llama-2-70b
llama-2-70b-chat
llama-2-7b
llama-2-7b-chat
	  </pre>
	  フォルダの中身は、例えば llama-2-7b だと
	  <pre>
llama-2-7b:
total 26322160
drwxr-xr-x   5 ichiki  staff          170 Jul 27 23:08 ./
drwxr-xr-x  25 ichiki  staff          850 Jul 28 13:14 ../
-rw-r--r--   1 ichiki  staff          100 Jul 14 08:00 checklist.chk
-rw-r--r--   1 ichiki  staff  13476925163 Jul 14 08:00 consolidated.00.pth
-rw-r--r--   1 ichiki  staff          102 Jul 14 08:00 params.json
	  </pre>
	</li>
	<li>params.json がモデルのパラメータを含む
	  <pre>
$ cat llama-2-7b/params.json 
{"dim": 4096,
"multiple_of": 256,
"n_heads": 32,
"n_layers": 32,
"norm_eps": 1e-05,
"vocab_size": -1}
	  </pre>
	</li>
      </ul>
    </li><!-- ダウンロードは手順通りにすれば、問題ない -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-2">
    <div style="font-size: 50px; font-weight: bolder;">
      LlaMA.cpp で Llama 2
    </div>
    <br /><br />
    <a href="https://github.com/ggerganov/llama.cpp">
      <img src="Screen Shot 2023-07-29 at 18.05.18_thumb.jpg" width="600" height="306" style="border: 2px #ccc solid;" /></a>
  </center>

  <ul>
    <li>ここからは、先月 (<a href="https://hello-ai-forum.github.io/pages/KAF2306/ichiki/#part2">KAF-2306</a>) のノリになります……つまり
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  貧乏人の AI
	  <br />
	  古いパソコンでも Llama 2 を使いたい！
	</div>
	<br /><br />
      </center>
    </li>

    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/ItakGol/status/1681684771833884675">https://twitter.com/ItakGol/status/1681684771833884675</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-24 at 11.21.54.png"><!-- 1183x1186 -->
		<img src="Screen Shot 2023-07-24 at 11.21.54_thumb.jpg" width="399" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Llama2 weights have  already been quantized
and available in cpp for local inference! 👼

Weights: <a href="http://huggingface.co/TheBloke">http://huggingface.co/TheBloke</a>
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

    <li>みなさんご承知の通り、
      python なくても Llama が走る <a href="https://github.com/ggerganov/llama.cpp">LlaMA.cpp</a> を使うには、
      モデルを（いわゆる GGML 形式とよばれるものに）変換する必要があります
      <ul>
	<li>（変換については、後述）</li>
      </ul>
    </li>
    <li>上のツイート（じゃなくて「𝕏」……しつこい？）が言ってるのは、
      <ul>
	<li>TheBloke さん（？）が、既に GGML 形式に変換してくれて、
	  それが HuggingFace にあるよ、
	  という知らせ
	</li>
      </ul>
    </li>
    
    <li>実験
      <ul>
	<li>前に clone したディレクトリに行って、
	  <pre>
$ cd somewhere/llama.cpp
	    
$ git fetch
$ git merge origin/master

$ make clean
$ make
	  </pre>
	</li>

	<li>モデルは llama-2-7b-chat.ggmlv3.q4_K_M.bin を使ってみる
	  <pre>
$ ./main -m ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin\
-p "### Instruction: What is the height of Mount Fuji?\
> ### Response:"
main: build = 926 (8a88e58)
main: seed  = 1690618277
llama.cpp: loading model from ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_head_kv  = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: n_gqa      = 1
llama_model_load_internal: rnorm_eps  = 5.0e-06
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: freq_base  = 10000.0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0.08 MB
llama_model_load_internal: mem required  = 4193.33 MB (+  256.00 MB per state)
llama_new_context_with_model: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 0 | AVX512 = 0 |
AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 |
F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |
sampling: repeat_last_n = 64, repeat_penalty = 1.100000,
presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,
tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,
mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


### Instruction: What is the height of Mount Fuji?
### Response: The height of Mount Fuji, located on Honshu Island in Japan,
is 3,776 meters (12,480 feet) above sea level.
This is based on the mountain’s summit elevation,
which is the highest point on the mountain. However,
it’s important to note that the height of Mount Fuji
can vary slightly depending on how it is measured
and where the measurement is taken along the mountain.
For example, some sources may give a height of 3,791 meters (12,438 feet)
or 3,780 meters (12,400 feet), based on different methods of measurement.
Nonetheless, 3,776 meters is the most commonly cited
and accepted height for Mount Fuji. [end of text]

llama_print_timings:        load time = 157994.23 ms
llama_print_timings:      sample time =   338.27 ms
/   169 runs   (    2.00 ms per token,   499.61 tokens per second)
llama_print_timings: prompt eval time = 17858.14 ms
/    18 tokens (  992.12 ms per token,     1.01 tokens per second)
llama_print_timings:        eval time = 45131.00 ms
/   168 runs   (  268.64 ms per token,     3.72 tokens per second)
llama_print_timings:       total time = 63353.35 ms
	  </pre>
	</li>

	<li>結構、はやい！</li>

      </ul>
    </li><!-- 実験 -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3">
    <div style="font-size: 50px; font-weight: bolder;">
      Andrej Karpathy の llama2.c
    </div>
    <br /><br />
    <a href="https://github.com/karpathy/llama2.c">
      <img src="Screen Shot 2023-07-29 at 18.04.57_thumb.jpg" width="600" height="277" style="border: 2px #ccc solid;" /></a>
  </center>

  <ul>
    <li>最近のツイッター（いや、「𝕏」と言うべきか？）から
      <table border="1" style="border: 1px solid black; border-collapse: collapse; table-layout: fixed;" width="100%">

	<tr>
	  <td>
	    <a href="https://twitter.com/karpathy/status/1683143097604243456">https://twitter.com/karpathy/status/1683143097604243456</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-24 at 11.06.44.png"><!-- 1179x1309 -->
		<img src="Screen Shot 2023-07-24 at 11.06.44_thumb.jpg" width="360" height="400" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
My fun weekend hack: llama2.c 🦙🤠
<a href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a>
Lets you train a baby Llama 2 model in PyTorch,
then inference it with one 500-line file with no dependencies, in pure C.
My pretrained model (on TinyStories) samples stories in fp32
at 18 tok/s on my MacBook Air M1 CPU.
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/karpathy/status/1683698478080466944">https://twitter.com/karpathy/status/1683698478080466944</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-25 at 14.03.13.png"><!-- 1181x997 -->
		<img src="Screen Shot 2023-07-25 at 14.03.13_thumb.jpg" width="400" height="338" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Yay, llama2.c can now load and inference the Meta released models! :)
E.g. here inferencing the smallest 7B model at ~3 tokens/s
on 96 OMP threads on a cloud Linux box.
Still just CPU, fp32, one single .c file of 500 lines:
<a href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a>
expecting ~300 tok/s tomorrow :)
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/ggerganov/status/1683574709470875649">https://twitter.com/ggerganov/status/1683574709470875649</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-27 at 17.47.44.png"><!-- 1179x947 -->
		<img src="Screen Shot 2023-07-27 at 17.47.44_thumb.jpg" width="400" height="321" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Lets add support for llama2.c models to llama.cpp

<a href="https://github.com/ggerganov/llama.cpp/issues/2379">https://github.com/ggerganov/llama.cpp/issues/2379</a>
	    </pre>
	  </td>
	</tr>

	<tr>
	  <td>
	    <a href="https://twitter.com/karpathy/status/1684612972034011136">https://twitter.com/karpathy/status/1684612972034011136</a>
	    <div style="float: left; padding-right:1em;">
	      <a href="Screen Shot 2023-07-28 at 11.27.27.png"><!-- 1177x760 -->
		<img src="Screen Shot 2023-07-28 at 11.27.27_thumb.jpg" width="400" height="258" style="border: 2px #ccc solid;" /></a>
	    </div>
	    <pre>
Neat, didn’t realize llama2.c made it to the top of Github trending.
Also more generally Github trending is a great place
to keep an eye on for projects that are seeing traction,
either as following this account and its xeets, or as bookmark.

<a href="https://twitter.com/trending_repos/status/1684488232862732289">https://twitter.com/trending_repos/status/1684488232862732289</a>

Trending repository of the day 📈
  
llama2.c by @karpathy

Inference Llama 2 in one file of pure C

Last 24h: 2214 ⭐
Total: 8367 ⭐️
	    </pre>
	  </td>
	</tr>

      </table>
    </li>

    <br />

    <li>andrej karpathy の <a href="https://github.com/karpathy/llama2.c">llama2.c</a>
      <ul>
	<li>これ、やっぱり面白いプロジェクトだ</li>

	<li>Llama 2 モデルを、自分で学習しよう（pytorch で）と言うプロジェクト</li>
	<li>というより、その結果（つまり Llama 2 モデル）を実行（推論）する
	  プログラムを C だけで（なんの依存性もなく）書いたよ、というのがポイント</li>

	<li>だからプロジェクト名が llama2.c</li>

	<li>以前、ぼく自身も手を動かした karpathy の nanoGPT
	  (<a href="https://hello-ai-forum.github.io/pages/ZAF202302/ichiki/#part1-gpt-from-scratch">ZAF-2302</a>)
	  <center>
      	    <a href="Screen Shot 2023-07-29 at 17.28.17.png"><!-- 2732x1188 -->
	      <img src="Screen Shot 2023-07-29 at 17.28.17_thumb.jpg" width="600" height="261" style="border: 2px #ccc solid;" /></a>
	  </center>
	  を思い出す
	</li>

	<center>
	  <br />
	  <div style="font-size: 40px; font-weight: bolder;">
	    遊びたいことリスト
	  </div>
	  <br /><br />
	</center>

	<li>（今回は時間がなかったので、できなかったこと）
	  <ul>
	    <li>Llama 2 の論文を読んで、 GPT との違いを理解し、
	      自分で実装してみる
	    </li>
	    <li>いずれのモデルも、推論用に C 版を書いてみる</li>

	    <li>上で述べた RWKV とか RetNet とか、実装しても面白そう</li>
	  </ul>
	</li><!-- 遊び方（今回は時間がなかったので、できなかったこと） -->

      </ul>
    </li><!-- llama2.c by karpathy -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-1">
    <div style="font-size: 40px; font-weight: bolder;">
      モデルファイルの変換について
    </div>
    <br /><br />
  </center>

  <ul>
    <li>じゃぁ、
      <center>
	<br />
	<div style="font-size: 40px; font-weight: bolder;">
	  今日はなにをやったのか？
	</div>
	<br /><br />
      </center>
      それは（上にちょこっと言及した）
      pytorch のモデルファイルの変換について、
      ちょっと実験してみた
      <ul>
	<li>やろうとしたことは、
	  変換プログラムも（python を使わず）
	  C で書こうってこと
	</li>
	<li>で、いろいろ調べた結果、
	  まぁ、 python で書く方が無難だね、
	  という結論に落ち着いた、と
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-1a">
    <div style="font-size: 40px; font-weight: bolder;">
      （０） llama2.c の変換プログラム
    </div>
    <br /><br />
  </center>
  <ul>
    <li>llama2.c プロジェクトに
      Meta の Llama2 モデルの変換スクリプト
      (export_meta_llama_bin.py) が追加された！
    </li>
    <li>つまり、本物の Llama2 が CPU で動く！
      <ul>
	<li>（まぁ、上で見た通り llaMA.cpp で、動くのは既に動くんだけど）</li>
      </ul>
    </li>
    <li>しかし llama2.c で使うためには、
      （ライセンスを遵守する場合は、なのかな？）
      各自が元の pytorch の weight を「llama2.c のバイナリー」形式
      に変換する必要がある
      <ul>
	<li>そのための python script が追加された、と</li>
      </ul>
    </li>
    <li>しかし！
      <ul>
	<li>貧乏人のパソコン環境は、今、まともな python がない
	  <ul>
	    <li>それに、依存性見ると torch が要求されている……</li>
	  </ul>
	</li>
	<li>そもそも、なんで llama2.c プロジェクトに
	  python script が入ってるんだ？
	  <ul>
	    <li>（C なら、基本、何でも書けるし、
	      なんなら python だって C で書かれている、よね）
	    </li>
	  </ul>
	</li>
      </ul>
    </li><!-- しかし！ -->

    <li>ということで、しばらく（半日くらい）
      pytorch の weight ファイル pth を
      C で開くためにはどうしたらいいか、
      ちょっと調べてみた
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-1b">
    <div style="font-size: 40px; font-weight: bolder;">
      （１） pth を C で開くには？
    </div>
    <br /><br />
  </center>
  <ul>
    <li>本家の情報によると、
      学習は python 環境でやるが、
      推論など production での計算では python に依存しない環境で使いたい、
      というニーズがあるようで、
      そのための workflow が準備されているようだ
    </li>
    <li>TorchScript
      <ul>
	<li>...</li>
      </ul>
    </li>
    <li>でも、モデルを一度 python の環境で TorchScript 形式（？）にして、
      それを C++ などで使うような形らしい
      <ul>
	<li>それは、オレの欲しいものではない</li>
      </ul>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-1c">
    <div style="font-size: 40px; font-weight: bolder;">
      （２） そもそも pth ファイルって何？
    </div>
    <br /><br />
  </center>
  <ul>
    <li>llama.cpp にある変換スクリプト convert.py を読んでみると
      <ul>
	<li>ちなみに、こいつは、 torch には依存してなかったので、
	  Mac の native 環境（macports）に python38 を準備してみた</li>
      </ul>
    </li>

    <li>すると、 pth ファイルは zip 圧縮されてた zip ファイル</li>

    <li>例えば Llama2 の 7B-Chat ファイルを展開すると
      consolidated というフォルダ下に
      <pre>
$ ll consolidated/
total 96
drwxr-xr-x    5 ichiki  staff    170 Jul 29 11:41 ./
drwxr-xr-x   16 ichiki  staff    544 Jul 29 11:41 ../
drwxr-xr-x  294 ichiki  staff   9996 Jul 29 11:41 data/
-rw-rw-r--    1 ichiki  staff  34124 Nov 30  1979 data.pkl
-rw-rw-r--    1 ichiki  staff      2 Nov 30  1979 version
      </pre>
      というファイル、 data 下にもいっぱいファイル
      <pre>
$ ll consolidated/data
total 26321952
drwxr-xr-x  294 ichiki  staff       9996 Jul 29 11:41 ./
drwxr-xr-x    5 ichiki  staff        170 Jul 29 11:41 ../
-rw-rw-r--    1 ichiki  staff  262144000 Nov 30  1979 0
-rw-rw-r--    1 ichiki  staff       8192 Nov 30  1979 1
-rw-rw-r--    1 ichiki  staff  262144000 Nov 30  1979 2
-rw-rw-r--    1 ichiki  staff   33554432 Nov 30  1979 3
-rw-rw-r--    1 ichiki  staff   33554432 Nov 30  1979 4

...

-rw-rw-r--    1 ichiki  staff   90177536 Nov 30  1979 286
-rw-rw-r--    1 ichiki  staff   90177536 Nov 30  1979 287
-rw-rw-r--    1 ichiki  staff   90177536 Nov 30  1979 288
-rw-rw-r--    1 ichiki  staff       8192 Nov 30  1979 289
-rw-rw-r--    1 ichiki  staff       8192 Nov 30  1979 290
-rw-rw-r--    1 ichiki  staff        128 Nov 30  1979 291
      </pre>
    </li>

    <li>大事なのは pickle ファイル、 python のシリアライザですな
      （中身はよく知らないけれど……）
    </li>

    <li>pickle を python を使わずに C だけで取り扱う方法は？
      <ul>
	<li>stackoverflow さんとか曰く、
	  素直に python 使え、
	  なんで python 使わないの？
	  みたいな状況
	</li>
	<li>ま、いろいろな状況に対するサポートや、
	  様々なフェイルセーフなど、
	  キッチンシク的にその部分に押し込まれてて、<br />
	  入り口（のデータ）と出口（のデータ）の
	  中立性を担保しているが故、なんだろうな
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-1d">
    <div style="font-size: 40px; font-weight: bolder;">
      結論： pth ファイルは python で扱うのが現実的
    </div>
    <br /><br />
  </center>
  <ul>
    <li>考えれば、 llama.cpp も llama2.c も、<br />
      python で書いているには、それだけの理由があるわけで
    </li>
    <li>いい勉強になりました。（オレの半日は……）</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-2">
    <div style="font-size: 40px; font-weight: bolder;">
      ローカルで python の変換スクリプトを動かす
    </div>
    <br /><br />
  </center>
  <ul>
    <li>（なんか、最初言ってたことと１８０度反対のことだけど……）</li>

    <br />

    <li>さっき準備しかけた macports の python38 環境に torch も入れてしまう
      <ul>
	<li>macports でインストールするスタイル
	  <ul>
	    <li>（pip も macports から入れれば、
	      pip で行けたのかもしれないが）</li>
	  </ul>
	</li>
	<li>あと sentencepiece も入れる必要があった</li>
	<li>でも、それだけで export_meta_llama_bin.py は通った</li>

	<li>7B と 7B-Chat を変換しておく
	  <pre>
$ ll *.bin
-rw-r--r--  1 ichiki  staff  26954711068 Jul 29 13:29 llama-2-7b-chat.bin
-rw-r--r--  1 ichiki  staff  26954711068 Jul 29 14:37 llama-2-7b.bin
-rw-r--r--  1 ichiki  staff       432717 Jul 29 12:53 tokenizer.bin
	  </pre>
	</li>

      </ul>
    </li><!-- さっき準備しかけた macports の python38 環境に ... -->
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-3-3">
    <div style="font-size: 40px; font-weight: bolder;">
      llama2.c で Llama2 モデルを動かす
    </div>
    <br /><br />
  </center>

  <ul>
    <li>今、変換した 7B モデルを動かしてみよう！
      <pre>
Kengos-MacBook-Pro:llama2.c ichiki$ ./run llama-2-7b.bin 0 64 "Hello! How are you doing today?"
&lt;s&gt;
Hello! How are you doing today??
I hope you are doing well.
I am doing well.
I am happy to be here with you.
I am happy to be here with you.
I am happy to be here with
      </pre>
      なんか、ハマっとるな……
    </li>
    <li>それに、めちゃくちゃ遅い……
      <ul>
	<li>fp32 のまま、というのが、やっぱり重たい原因かな</li>
	<li>（LlaMA.cpp の 7B-Chat は llama-2-7b-chat.ggmlv3.q4_K_M.bin だった）</li>
      </ul>
    </li>

  </ul>

  <center>
    <div style="font-size: 40px; font-weight: bolder;">
      次までに、 RetNet マスターするぞー！
    </div>
    <br /><br />
  </center>

  <br /><br />
  <div align="right">
    （<a href="#toc">トップに戻る</a>、<a href="#detailed-toc">詳細目次へ</a>）
  </div>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="epilogue">
    <div style="font-size: 50px; font-weight: bolder;">
      今日のおわりに
    </div>
  </center>

  <p>……</p>

  <h3>今後の予定</h3>
  <ul>
    <li>次回「こんにちわ！ AI FORUM」は
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  2023 年 8 月 26 日（土曜日）<br />
	  開催の予定です！
	  <br /><br />
	</div>
      </center>
      <ul>
	<li>ご意見、ご希望など、お気軽に！</li>
      </ul>
    </li>
    <li>フォーラム講演者、サークル同人誌活動への執筆者、絶賛、大募集中です！<br />
      お気軽にお問い合わせください！</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />
  <hr />

  <h2 id="detailed-toc">総合目次</h2>
  <ul>
    <li><b>前座</b>
      <a href="#part0">2023 年前半の振り返り</a>
      <ul>
	<li><a href="#part0-1">ピアノ</a></li>
	<li><a href="#part0-2">「こんにちわ！ AI FORUM のポッドキャスト」</a></li>
	<li><a href="#part0-3">「音楽と数理 🎼 ♾ ポッドキャスト」</a></li>
	<li><a href="#part0-4">英語の（紙の）本をアマゾンで世界に向けて売る</a></li>
	<li><a href="#part0-5">まとめ</a></li>
      </ul>
    </li>

    <li><b>パート１</b>
      <a href="#part1">AI の未来</a>
      <ul>
	<li><a href="#part1-1">Jeremy Howard の「Delightenment」</a></li>
	<li><a href="#part1-2">最近の AI 業界の動き</a></li>
	<li><a href="#part1-3">The Frontier Model Forum</a></li>
      </ul>
    </li>

    <li><b>パート２</b>
      <a href="#part2">最近の LLMs</a>
      <ul>
	<li><a href="#part2-0">ポスト Transformer 時代の LLMs</a>
	  <ul>
	    <li><a href="#part2-0-1">RWKV (Receptance Weighted Key Value)</a></li>
	    <li><a href="#part2-0-2">FlashAttention-2</a></li>
	    <li><a href="#part2-0-3">Retentive Network (RetNet)</a></li>
	    <li><a href="#part2-0-4">phi-1 / 「Textbooks Are All You Need」論文</a></li>
	  </ul>
	</li>
	<li><a href="#part2-1">Llama 2 発表</a>
	  <ul>
	    <li><a href="#part2-1-1">ダウンロード</a></li>
	  </ul>
	</li>
	<li><a href="#part2-2">LlaMA.cpp で Llama 2</a></li>
	<li><a href="#part2-3">Andrej Karpathy の llama2.c</a>
	  <ul>
	    <li><a href="#part2-3-1">モデルファイルの変換について</a>
	      <ul>
		<li><a href="#part2-3-1a">（０） llama2.c の変換プログラム</a></li>
		<li><a href="#part2-3-1b">（１） pth を C で開くには？</a></li>
		<li><a href="#part2-3-1c">（２） そもそも pth ファイルって何？</a></li>
		<li><a href="#part2-3-1d">結論： pth ファイルは python で扱うのが現実的</a></li>
	      </ul>
	    </li>
	    <li><a href="#part2-3-2">ローカルで python の変換スクリプトを動かす</a></li>
	    <li><a href="#part2-3-3">llama2.c で Llama2 モデルを動かす</a></li>
	  </ul>
	</li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

</section>

</article>

</body>           
</html>
