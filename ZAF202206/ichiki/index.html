<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8" />
    <title>ZENKEI AI FORUM (2022/06/29)</title>
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+1p:100,400,700&display=swap&subset=japanese" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../bright-M_PLUS_1p.css" />

    <link rel="stylesheet"
	  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="text/javascript" id="MathJax-script" async
	    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

<body style="font-size: 20px;">

<header>
<center><h1>ZENKEI AI FORUM 2022/06/29</h1></center>
</header>

<article>

<section id="main">

  <center>
    <a href="ZENKEI_AI_FORUM_zoom_20220629-2488x1400.jpg"><!-- 2488x1400 -->
      <img src="ZENKEI_AI_FORUM_zoom_20220629-2488x1400_thumb.jpg" width="800" height="450" style="border: 2px #ccc solid;" /></a>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      ZAF ２０２２年６月２９日
    </div>
    <div style="font-size: 40px;">＜本日のテーマ＞</div>
    <div style="font-size: 80px;">
      技術書典１３企画会議
    </div>
    <div style="font-size: 60px;">ほか</div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <h2>目次</h2>
  <ul>
    <li>[6:30 - 7:00]
      <b>前座</b>
      <ul>
	<li><a href="#part0-twitch">最近の私（いちき）はどこに向かっているのか？</a></li>
	<li><a href="#part0-dance">踊ってみた-その後</a></li>
      </ul>
    </li>

    <li>[7:00 - 8:00]
      <b>パート１</b>
      <a href="#part1">最近の AI は本当すごいね</a>
      <ul>
	<li><a href="#part1-community-oriented-life">（前座）令和時代のコミュニティ志向な生き方</a></li>
	<li><a href="#part1-portrait">顔の 3D 再構成</a></li>
	<li><a href="#part1-NeRF">3D 再構成、 NeRF 関係</a></li>
	<li><a href="#part1-photogrammetry">フォトグラメトリ、LiDAR 関係</a></li>
	<li><a href="#part1-text-to-image">Text to Image など</a></li>
	<li><a href="#part1-super-resolution">超解像など</a></li>
	<li><a href="#part1-demos">デモ</a></li>
	<li><a href="#part1-pose">姿勢推定</a></li>
	<li><a href="#part1-misc">その他</a></li>
      </ul>
    </li>

    <li>[8:00 - 9:00]
      <b>パート２</b>
      <a href="#part2">技術書典１３企画会議！</a>
      <ul>
	<li><a href="#part2-invitation">技術書典（イベント）へのお誘い</a></li>
	<li><a href="#part2-mission-statement">ZENKEI AI FORUM の Mission Statement</a></li>
	<li><a href="#part2-fandom-economy">同人誌活動は今のビジネスの最先端！</a></li>
	<li><a href="#part2-line-up">これまでのラインナップ</a></li>
        <li><a href="#part2-my-personal-project">いちきのソロ企画</a></li>
	<li><a href="#part2-as-a-circle">ZAF のサークル企画</a></li>
	<li><a href="#part2-lets-discuss">企画会議！</a></li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>

    <li><a href="#detailed-toc">総合目次</a></li>
  </ul>

  <hr />

  <center>
    <br />
    <div style="font-size: 30px;">
      YouTube のアーカイブ・ビデオはこちら
    </div>
    (<a href="https://youtube.com/live/3BmmpshE2eI">
      https://youtube.com/live/3BmmpshE2eI</a>)
    <br /><br />
    <a href="https://youtube.com/live/3BmmpshE2eI">
      <img src="Screen Shot 2023-03-31 at 10.46.54_thumb.jpg" width="500" height="282" style="border: 2px #ccc solid;" /></a>
    <br /><br />
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="part0-twitch">
    <div style="font-size: 50px; font-weight: bolder;">
      前座、その１
      <br />
      最近の私（いちき）はどこに向かっているのか？
    </div>
  </center>

  <ul>
    <li>これまでの、「最近、どこに向かっていたのか？」</li>
    <li>今年の ZAF の前座を振り返ってみると、
      <ul>
	<li><a href="https://hello-ai-forum.github.io/pages/ZAF202201/ichiki/">ZAF-2201</a> 同人の方向、芭蕉とかに引っかかったり
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.41.20.png"><!-- 2650x655 -->
	      <img src="Screen Shot 2022-06-28 at 23.41.20_thumb.jpg" width="800" height="198" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://hello-ai-forum.github.io/pages/ZAF202202/ichiki/">ZAF-2202</a> アーティストの方向、 AUDIUS デビュー
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.43.12.png"><!-- 2669x739 -->
	      <img src="Screen Shot 2022-06-28 at 23.43.12_thumb.jpg" width="800" height="222" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <ul>
	    <li>（2016~2017 あたりに一度、作曲ブーム来てた）</li>
	  </ul>
	</li>
	<li><a href="https://hello-ai-forum.github.io/pages/ZAF202203/ichiki/">ZAF-2203</a> ジャズは、きちんとみると、緻密だった（ピアノの方向）
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.46.11.png"><!-- 2678x1066 -->
	      <img src="Screen Shot 2022-06-28 at 23.46.11_thumb.jpg" width="800" height="318" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://hello-ai-forum.github.io/pages/ZAF202204/ichiki/">ZAF-2204</a> 音楽と数理ポッドキャスト - 自分自身をまな板の上にのせる
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.48.15.png"><!-- 2677x1073 -->
	      <img src="Screen Shot 2022-06-28 at 23.48.15_thumb.jpg" width="800" height="321" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://hello-ai-forum.github.io/pages/ZAF202205/ichiki/">ZAF-2205</a> 音楽と数理ポッドキャスト - 自分の可能性を模索、
	  ピアノを晒し、英語で語りかけた
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.50.08.png"><!-- 2680x588 -->
	      <img src="Screen Shot 2022-06-28 at 23.50.08_thumb.jpg" width="800" height="176" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-28 at 23.50.11.png"><!-- 2677x602 -->
	      <img src="Screen Shot 2022-06-28 at 23.50.11_thumb.jpg" width="800" height="180" style="border: 2px #ccc solid;" /></a>
	  </center>
	  で、踊った
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.53.30.png"><!-- 2675x654 -->
	      <img src="Screen Shot 2022-06-28 at 23.53.30_thumb.jpg" width="800" height="196" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>そして、先日、歌ってみた (<a href="https://youtube.com/shorts/LuK4loO4ffM">https://youtube.com/shorts/LuK4loO4ffM</a>)
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.56.18.png"><!-- 2278x1206 -->
	      <img src="Screen Shot 2022-06-28 at 23.56.18_thumb.jpg" width="800" height="424" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 0.00.14.png"><!-- 2313x464 -->
	      <img src="Screen Shot 2022-06-29 at 0.00.14_thumb.jpg" width="800" height="160" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>
    <li>まとめると、
      <ul>
	<li>コミュニティ志向（一人でできること、みんなでできること）</li>
	<li>自分自身を晒す（世間に問う、と同時に、信頼してもらう）</li>
	<li>自分を高める（成長する、楽しむ）</li>
	<li>自分の限界を超える</li>
      </ul>
    </li>

    <li>この流れの、ある意味、当然の帰結として、
      <center>
	<div style="font-size: 50px; font-weight: bolder;">
	  いちき、 Twitch をはじめました！
	</div>
      </center>
    </li>
    <li><a href="https://www.twitch.tv/kengoichiki">https://www.twitch.tv/kengoichiki</a>
  </ul>

  <center>
    <a href="Screen Shot 2022-06-26 at 13.25.22.png"><!-- 2341x1157 -->
      <img src="Screen Shot 2022-06-26 at 13.25.22_thumb.jpg" width="1000" height="494" style="border: 2px #ccc solid;" /></a>
    <a href="Screen Shot 2022-06-26 at 13.25.32.png"><!-- 2343x820 -->
      <img src="Screen Shot 2022-06-26 at 13.25.32_thumb.jpg" width="1000" height="350" style="border: 2px #ccc solid;" /></a>
  </center>

  <ul>
    <li>ってことで、</li>
  </ul>

  <center>
    <div style="font-size: 50px; font-weight: bolder;">
      今後とも、よろしくお願いします！
    </div>
  </center>
  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="part0-dance">
    <div style="font-size: 50px; font-weight: bolder;">
      前座、その２
      <br />
      踊ってみた-その後
    </div>
  </center>

  <ul>
    <li>前回 (<a href="https://hello-ai-forum.github.io/pages/ZAF202205/ichiki/">ZAF-2205</a>)
      <center>
	<a href="ZENKEI_AI_FORUM_zoom_20220525-2488x1400.jpg"><!-- 2488x1400 -->
	  <img src="ZENKEI_AI_FORUM_zoom_20220525-2488x1400_thumb.jpg" width="800" height="450" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>踊ったビデオを YouTube にアップ
      <center>
	<a href="https://youtu.be/AJi6SzSCDXA">
	  <img src="Screen Shot 2022-06-26 at 14.20.11_thumb.jpg" width="1000" height="575" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
    <li>
      <center>
	<div style="font-size: 50px; font-weight: bolder;">
	  その後、すごいことになりました
	  <br />
	  （あくまで、当社比ってことなんですが）
	</div>
      </center>
    </li>

    <li>
      <center>
	<a href="Screen Shot 2022-06-26 at 14.10.46.png"><!-- 2067x1188 -->
	  <img src="Screen Shot 2022-06-26 at 14.10.46_thumb.jpg" width="800" height="460" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-26 at 14.11.18.png"><!-- 2015x1348 -->
	  <img src="Screen Shot 2022-06-26 at 14.11.18_thumb.jpg" width="800" height="535" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-26 at 14.11.34.png"><!-- 2772x1259 -->
	  <img src="Screen Shot 2022-06-26 at 14.11.34_thumb.jpg" width="800" height="363" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-26 at 14.11.44.png"><!-- 2766x1198 -->
	  <img src="Screen Shot 2022-06-26 at 14.11.44_thumb.jpg" width="800" height="346" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-26 at 14.13.02.png"><!-- 777x520 -->
	  <img src="Screen Shot 2022-06-26 at 14.13.02_thumb.jpg" width="800" height="535" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="part1">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      パート１
      最近の AI は本当すごいね
    </div>
  </center>

  <ul>
    <li>先日まで、ちょうど <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>
      (computer vision and pattern recognition)
      が New Orleans で開催されていた
      <center>
	<a href="Screen Shot 2022-06-27 at 21.22.21.png"><!-- 2687x979 -->
	  <img src="Screen Shot 2022-06-27 at 21.22.21_thumb.jpg" width="1000" height="364" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li>と、その前に、ちょっと（これは、前座ネタかな？）</li>
  </ul>

  <center id="part1-community-oriented-life">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      令和時代のコミュニティ志向な生き方
    </div>
  </center>

  <ul>
    <li>最近、注目している（注目されている）人物が２人</li>

    <li>その１： AK さん
      <center>
	<a href="Screen Shot 2022-06-27 at 12.07.26.png"><!-- 1319x637 -->
	  <img src="Screen Shot 2022-06-27 at 12.07.26_thumb.jpg" width="600" height="290" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>この人 AI の研究者の界隈で、これまでも Twitter でいろいろ話題になってた
	  <ul>
	    <li>ほぼ毎日のように、大量のある arxiv に投稿される
	      AI のプレプリントに目を通していて、<br />
	      重要そうな論文をピックアップしてツイートしていた。
	    </li>
	    <li>それが、くる日も来る日もなので、
	      一部に「あれは AI で作られた bot に違いない」とか、<br />
	      「AK って Andrej Karpathy じゃね？」とか、<br />
	      そんな話題があがってた。（ネタ、ですね）
	    </li>
	  </ul>
	</li>
	<li>その人が、今回、 covid 明けの CVPR のリアルのカンファレンスに行く、
	  というので、話題になってた、と。
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.29.34.png"><!-- 1315x937 -->
	      <img src="Screen Shot 2022-06-27 at 12.29.34_thumb.jpg" width="600" height="428" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

	<li>会場での反応 - 「本物だった！」「ボットじゃなかった！」と
	  <center>
	    <table border="0">
	      <tr>
		<td valign="top">
		  <a href="Screen Shot 2022-06-27 at 12.14.49.png"><!-- 1306x435 -->
		    <img src="Screen Shot 2022-06-27 at 12.14.49_thumb.jpg" width="600" height="200" style="border: 2px #ccc solid;" /></a>
		</td>
		<td valign="top">		
		  <a href="Screen Shot 2022-06-27 at 12.11.59.png"><!-- 1313x1156 -->
		    <img src="Screen Shot 2022-06-27 at 12.11.59_thumb.jpg" width="600" height="528" style="border: 2px #ccc solid;" /></a>
		</td>
	      </tr>
	    </table>
	  </center>
	</li>
	<li>会場での反応、その２ - どうも何らかのフィールドが発生しているようだ
	  <center>
	    <table border="0">
	      <tr>
		<td valign="top">
		  <a href="Screen Shot 2022-06-27 at 12.13.58.png"><!-- 1345x556 -->
		    <img src="Screen Shot 2022-06-27 at 12.13.58_thumb.jpg" width="600" height="248" style="border: 2px #ccc solid;" /></a>
		</td>
		<td valign="top">		
		  <a href="Screen Shot 2022-06-27 at 12.10.46.png"><!-- 1319x929 -->
		    <img src="Screen Shot 2022-06-27 at 12.10.46_thumb.jpg" width="600" height="423" style="border: 2px #ccc solid;" /></a>
		</td>
	      </tr>
	      <tr>
		<td valign="top">
		  <a href="Screen Shot 2022-06-27 at 12.10.41.png"><!-- 1344x819 -->
		    <img src="Screen Shot 2022-06-27 at 12.10.41_thumb.jpg" width="600" height="366" style="border: 2px #ccc solid;" /></a>
		</td>
		<td valign="top">		
		  <a href="Screen Shot 2022-06-27 at 12.10.38.png"><!-- 1311x660 -->
		    <img src="Screen Shot 2022-06-27 at 12.10.38_thumb.jpg" width="600" height="302" style="border: 2px #ccc solid;" /></a>
		</td>
	      </tr>
	    </table>
	  </center>
	</li>

      </ul>
    </li>

    <li>その２： iwama さん
      <ul>
	<li><a href="https://twitter.com/iwamah1/status/1540223070321152001">https://twitter.com/iwamah1/status/1540223070321152001</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.48.45.png"><!-- 1317x1006 -->
	      <img src="Screen Shot 2022-06-27 at 12.48.45_thumb.jpg" width="600" height="458" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
登壇DONE
NianticのLightship Summit Tokyo 2022にiPhone 3Dスキャンで登壇した男の実績解除です
登壇中ずっと手が震えていました
皆さんありがとうございます！！！
#Lightship #Niantic
	  </pre>
	</li>

	<li><a href="https://twitter.com/ShogoNu/status/1540248857551917056">https://twitter.com/ShogoNu/status/1540248857551917056</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.49.09.png"><!-- 1297x778 -->
	      <img src="Screen Shot 2022-06-27 at 12.49.09_thumb.jpg" width="600" height="360" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
iPhone LiDARを叫んだ男がテック業界のど真ん中にきてるの控えめに言ってスゴイ。叫び続けるの大事！
	  </pre>
	</li>

      </ul>
    </li><!-- その２： iwama さん -->

  </ul>


  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li>ということで、本題に戻りましょう</li>
  </ul>

  <center>
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      最近の AI は本当すごいね！
    </div>
  </center>

  <ul>
    <li>プレプリやコードの時代は終わって、
      <center>
	<div style="font-size: 40px; font-weight: bolder;">
	  <br />
	  デモを発表しないと注目されない時代
	  <br /><br />
	</div>
      </center>
      になった（みたい）
      <ul>
	<li><a href="https://twitter.com/abidlabs/status/1539618131139772416">https://twitter.com/abidlabs/status/1539618131139772416</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.20.19.png"><!-- 1316x984 -->
	      <img src="Screen Shot 2022-06-27 at 12.20.19_thumb.jpg" width="600" height="449" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Slides for my @CVPR 2022 talk: 

"Papers and Code Aren't Enough: Why Demos are Critical to ML Research and How to Build Them"

Thank you to @humphrey_shi and #CVPR2022 for organizing!
<a href="https://docs.google.com/presentation/d/1TXw48MjZFvkVur6rE0tUrEECf76SgUrglTjv0Kb0SWQ/edit#slide=id.g10d8aca05c4_0_10">https://docs.google.com/presentation/d/1TXw...</a>
	  </pre>
	</li>
	<li>ということで、今日は、すぐ動くデモを中心に、紹介していきましょう</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-portrait">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      顔の 3D 再構成
    </div>
  </center>

  <ul>
    <li><b>Thin-Plate Spline Motion Model for Image Animation</b> (<a href="https://twitter.com/ak92501/status/1539066735965380608">https://twitter.com/ak92501/status/1539066735965380608</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.25.25.png"><!-- 1300x624 -->
	  <img src="Screen Shot 2022-06-27 at 12.25.25_thumb.jpg" width="600" height="288" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
a @Gradio Demo for Thin-Plate Spline Motion Model for Image Animation
 on @huggingface Spaces for @CVPR 2022 

demo: <a href="https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model">https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model</a>
      </pre>
      <ul>
	<li><a href="https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model">https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 22.15.30.png"><!-- 2673x1057 -->
	      <img src="Screen Shot 2022-06-27 at 22.15.30_thumb.jpg" width="800" height="316" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>
	  <center>
	    <div style="font-size: 50px; font-weight: bolder;">
	      やってみよう！
	    </div>
	  </center>
	  <ul>
	    <li>準備
	      <center>
		<a href="KI20220401.jpg"><!-- 1893x1070 -->
		  <img src="KI20220401_thumb.jpg" width="600" height="339" style="border: 2px #ccc solid;" /></a>
		<br />
		<a href="KI20220401-orig.jpg"><!-- 956x957 -->
		  <img src="KI20220401-orig_thumb.jpg" width="400" height="400" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	    <li>結果
	      <center>
		<video controls autoplay muted loop
		       width="256" height="256" style="border: 2px #ccc solid;">
		  <source src="ThinPlateSplineMotionModel.mp4">
		</video>
		<img src="vox.gif" />
	      </center>
	    </li>
	  </ul>
	</li><!-- やってみよう！ -->

      </ul>
    </li><!-- Thin-Plate Spline Motion Model for Image Animation -->

    <li><b>DaGAN</b> (<a href="https://twitter.com/danxuhk/status/1537785692201177088">https://twitter.com/danxuhk/status/1537785692201177088</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.33.08.png"><!-- 1298x943 -->
	  <img src="Screen Shot 2022-06-27 at 12.33.08_thumb.jpg" width="600" height="436" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Please check our paper and project for talking head video generation at the incoming CVPR 22 😃😃😃
@harlan_hong
 
You may also try our model on the hugging face:
<a href="https://huggingface.co/spaces/HarlanHong/DaGAN">https://huggingface.co/spaces/HarlanHong/DaGAN</a>

Depth-Aware Generative Adversarial Network for Talking Head Video Generation
abs: <a href="https://arxiv.org/abs/2203.06605">https://arxiv.org/abs/2203.06605</a>
github: <a href="https://github.com/harlanhong/CVPR2022-DaGAN">https://github.com/harlanhong/CVPR2022-DaGAN</a>
      </pre>
      <ul>
	<li>DEMO:
	  <a href="https://huggingface.co/spaces/HarlanHong/DaGAN">https://huggingface.co/spaces/HarlanHong/DaGAN</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 22.41.54.png"><!-- 2678x1037 -->
	      <img src="Screen Shot 2022-06-28 at 22.41.54_thumb.jpg" width="800" height="310" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- DaGAN -->

    <li>付記：イベント時に、その場で
      <center id="part1-portrait-others">
	<div style="font-size: 50px; font-weight: bolder;">
	  <br />
	  やってみた
	</div>
      </center>
      <ul>
	<li>DaGAN のデモに、前に知ってた（浮世絵の顔のデモで使われてた）
	  変顔の女の子のビデオがあったので、
	  <center id="part1-portrait-others">
	    <div style="font-size: 50px; font-weight: bolder;">
	      <br />
	      急遽、企画
	      <br />
	      いちき、変顔してみた
	    </div>
	  </center>
	</li>
	<li>driving video
	  <center>
	    <video controls autoplay muted loop
		   width="256" height="256" style="border: 2px #ccc solid;">
	      <source src="video2.mp4">
	    </video>
	  </center>
	</li>
	<li>結果
	  <center>
	    <table border="0">
	      <tr>
		<th>DaGAN の結果</th>
		<th>ThinPlateSpline</th>
	      </tr>
	      <tr>
		<td>
		  <video controls autoplay muted loop
			 width="1024" height="256" style="border: 2px #ccc solid;">
		    <source src="DaGAN-result.mp4">
		  </video>
		</td>
		<td>
		  <video controls autoplay muted loop
			 width="256" height="256" style="border: 2px #ccc solid;">
		    <source src="ThinPlateSpline-result.mp4">
		  </video>
		</td>
	      </tr>
	    </table>
	  </center>
	</li>
      </ul>

    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-portrait-others">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      顔の 3D 再構成、つづき
    </div>
  </center>

  <ul>
    <li><b>LIA (and FOMM)</b> (<a href="https://twitter.com/kym384/status/1540957782505320449">https://twitter.com/kym384/status/1540957782505320449</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.42.24.png"><!-- 1317x975 -->
	  <img src="Screen Shot 2022-06-27 at 12.42.24_thumb.jpg" width="600" height="444" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-27 at 12.42.48.png"><!-- 1312x1108 -->
	  <img src="Screen Shot 2022-06-27 at 12.42.48_thumb.jpg" width="600" height="507" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
コード公開されたので動かしてみた
<a href="https://github.com/wyhsirius/LIA">https://github.com/wyhsirius/LIA</a>

比較としてFirst Order Motion Modelの出力
Driving videoの動きが激しいサンプルに対してはFOMMよりも自然な動きをしてるように感じる

<a href="https://github.com/AliaksandrSiarohin/first-order-model">https://github.com/AliaksandrSiarohin/first-order-model</a>
      </pre>
      <ul>
	<li><a href="https://github.com/wyhsirius/LIA">https://github.com/wyhsirius/LIA</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.12.52.png"><!-- 2663x753 -->
	      <img src="Screen Shot 2022-06-28 at 18.12.52_thumb.jpg" width="800" height="226" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://github.com/AliaksandrSiarohin/first-order-model">https://github.com/AliaksandrSiarohin/first-order-model</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.13.01.png"><!-- 2661x1014 -->
	      <img src="Screen Shot 2022-06-28 at 18.13.01_thumb.jpg" width="800" height="305" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- LIA (and FOMM) -->

    <li><b>GRAM-HD</b> (<a href="https://twitter.com/Buntworthy/status/1537810017449041920">https://twitter.com/Buntworthy/status/1537810017449041920</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 13.05.15.png"><!-- 1306x526 -->
	  <img src="Screen Shot 2022-06-27 at 13.05.15_thumb.jpg" width="600" height="242" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
A horrible nightmare of infinite people shaking their head at me:

GRAM-HD: 3D-Consistent Image Generation at High Resolution
with Generative Radiance Manifolds
abs: <a href="https://arxiv.org/abs/2206.07255">https://arxiv.org/abs/2206.07255</a>
project page: <a href="https://jeffreyxiang.github.io/GRAM-HD/">https://jeffreyxiang.github.io/GRAM-HD/</a>

3D-aware GAN that can generate high resolution images
while keeping strict 3D consistency as in volume rendering
      </pre>
    </li><!-- GRAM-HD -->

    <li><b>RigNeRF</b> (<a href="https://twitter.com/shahrukh_athar/status/1540218740461142022">https://twitter.com/shahrukh_athar/status/1540218740461142022</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.10.52.png"><!-- 1308x957 -->
	  <img src="Screen Shot 2022-06-27 at 12.10.52_thumb.jpg" width="600" height="439" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Really excited to present RigNeRF today at Poster Session 4.2 of #CVPR2022 (@CVPR)!!
Drop by PosterID 161b to discuss RigNeRF, human face modeling,
facial expressions and the future of human avatars!

RigNeRF: Fully Controllable Neural 3D Portraits
abs: <a href="https://arxiv.org/abs/2206.06481">https://arxiv.org/abs/2206.06481</a>
project page: <a href="http://shahrukhathar.github.io/2022/06/06/RigNeRF.html">http://shahrukhathar.github.io/2022/06/06/RigNeRF.html</a>

Using only a smartphone-captured short video of a subject for training,
demonstrate the effectiveness of method on free view synthesis
of a portrait scene
      </pre>
    </li><!-- RigNeRF -->

    <li><b>I M Avatar</b> (<a href="https://twitter.com/ak92501/status/1470971332062031880">https://twitter.com/ak92501/status/1470971332062031880</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.09.11.png"><!-- 1295x656 -->
	  <img src="Screen Shot 2022-06-27 at 12.09.11_thumb.jpg" width="600" height="304" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
I M Avatar: Implicit Morphable Head Avatars from Videos
abs: <a href="https://arxiv.org/abs/2112.07471">https://arxiv.org/abs/2112.07471</a>

github: <a href="https://github.com/zhengyuf/IMavatar">https://github.com/zhengyuf/IMavatar</a>
      </pre>
    </li><!-- I M Avatar -->

    <li>以下は、コードがまだ公開されていない</li>

    <li><b>GAN2X</b> (<a href="https://twitter.com/ak92501/status/1539442569733718016">https://twitter.com/ak92501/status/1539442569733718016</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.22.52.png"><!-- 1317x794 -->
	  <img src="Screen Shot 2022-06-27 at 12.22.52_thumb.jpg" width="600" height="362" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
GAN2X: Non-Lambertian Inverse Rendering of Image GANs
abs: <a href="https://arxiv.org/abs/2206.09244">https://arxiv.org/abs/2206.09244</a>
project page: <a href="https://people.mpi-inf.mpg.de/~xpan/GAN2X/">https://people.mpi-inf.mpg.de/~xpan/GAN2X/</a>
      </pre>
      <ul>
	<li>コード、デモは、未公開</li>
      </ul>
    </li><!-- GAN2X -->

    <li><b>EpiGRAF</b> (<a href="https://twitter.com/ak92501/status/1539459120667021312">https://twitter.com/ak92501/status/1539459120667021312</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.22.19.png"><!-- 1322x659 -->
	  <img src="Screen Shot 2022-06-27 at 12.22.19_thumb.jpg" width="600" height="299" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
EpiGRAF: Rethinking training of 3D GANs
abs: <a href="https://arxiv.org/abs/2206.10535">https://arxiv.org/abs/2206.10535</a>
project page: <a href="https://universome.github.io/epigraf">https://universome.github.io/epigraf</a>
      </pre>
      <ul>
	<li>コードは近日公開（リファクタ前のコードはあるみたい）</li>
      </ul>
    </li><!-- EpiGRAF -->

    <li><b>ROME</b> (<a href="https://twitter.com/ak92501/status/1537639309888610305">https://twitter.com/ak92501/status/1537639309888610305</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.34.30.png"><!-- 1301x633 -->
	  <img src="Screen Shot 2022-06-27 at 12.34.30_thumb.jpg" width="600" height="292" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Realistic One-shot Mesh-based Head Avatars
abs: <a href="https://arxiv.org/abs/2206.08343">https://arxiv.org/abs/2206.08343</a>
project page: <a href="https://samsunglabs.github.io/rome/">https://samsunglabs.github.io/rome/</a>
      </pre>
      <center>
	<a href="Screen Shot 2022-06-28 at 22.44.07.png"><!-- 2673x1027 -->
	  <img src="Screen Shot 2022-06-28 at 22.44.07_thumb.jpg" width="800" height="307" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>コードはまだ</li>
      </ul>
    </li><!-- ROME -->

    <li><b>VoxGRAF</b> (<a href="https://twitter.com/K_S_Schwarz/status/1537442208982237193">https://twitter.com/K_S_Schwarz/status/1537442208982237193</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.38.06.png"><!-- 1315x906 -->
	  <img src="Screen Shot 2022-06-27 at 12.38.06_thumb.jpg" width="600" height="413" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Sparse voxel grids have proven super useful for speeding up novel view synthesis.
Inspired by this, our latest work uses a sparse voxel grid representation
for fast and 3D-consistent generative modeling.

Paper: <a href="https://arxiv.org/abs/2206.07695">https://arxiv.org/abs/2206.07695</a>
Project page: <a href="https://katjaschwarz.github.io/voxgraf/">https://katjaschwarz.github.io/voxgraf/</a>


VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids
abs: <a href="https://arxiv.org/abs/2206.07695">https://arxiv.org/abs/2206.07695</a>
      </pre>
      <center>
	<a href="Screen Shot 2022-06-28 at 22.46.03.png"><!-- 2677x870 -->
	  <img src="Screen Shot 2022-06-28 at 22.46.03_thumb.jpg" width="800" height="260" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li>コードは coming soon...</li>
      </ul>
    </li><!-- VoxGRAF -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-NeRF">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      3D 再構成、 NeRF 関係
    </div>
  </center>

  <ul>
    <li><b>Instant NGP</b> (<a href="https://twitter.com/smallfly/status/1540795797142372353">https://twitter.com/smallfly/status/1540795797142372353</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.43.26.png"><!-- 1332x744 -->
	  <img src="Screen Shot 2022-06-27 at 12.43.26_thumb.jpg" width="600" height="335" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Series of NeRF trained using drone-selfie datasets.
I've a bunch of those - one of the side effects of the social distancing
we went through. Synthesized using @NVIDIAAIDev 's Instant-ngp.⁣⁣⁣⁣⁣⁣⁣⁣

#selfportrait #AI #NeRF #nerfies #synthesis #neuralrendering #volumetric
      </pre>
      <ul>
	<li><a href="https://twitter.com/jonstephens85/status/1540384170412634112">https://twitter.com/jonstephens85/status/1540384170412634112</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.45.32.png"><!-- 1319x767 -->
	      <img src="Screen Shot 2022-06-27 at 12.45.32_thumb.jpg" width="600" height="349" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Ferraris are cool, but there is something about American muscle
that always draws me in. I would LOVE to drive a car like this one day...
This was created with @NVIDIAAIDev 's instant NeRF!
Who's watching the @NASCAR Ally 400 this weekend?
#NeuralNetworks #3D
	  </pre>
	</li>
	<li><a href="https://twitter.com/gradeeterna/status/1540380257634631680">https://twitter.com/gradeeterna/status/1540380257634631680</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.45.57.png"><!-- 1315x750 -->
	      <img src="Screen Shot 2022-06-27 at 12.45.57_thumb.jpg" width="600" height="342" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Trying out the 360 video to AnimeGAN to @NVIDIAAIDev
Instant NeRF workflow on a nature scene.
Turned out a bit fuzzy, but look at those trees and reflections😍
#InstantNeRFSweepstakes #NeRF #instantNGP #volumetric #synthesis
#AI #neuralrendering #neuralradiancefields
	  </pre>
	</li>
	<li><a href="https://twitter.com/jonstephens85/status/1539840525670354944">https://twitter.com/jonstephens85/status/1539840525670354944</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.47.38.png"><!-- 1317x840 -->
	      <img src="Screen Shot 2022-06-27 at 12.47.38_thumb.jpg" width="600" height="383" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
I think I just walked down a path I cannot come back from. 
@Luxottica / @RealityLabs Ray-Ban Stories plus @NVIDIAAIDev Instant-NeRFs.

Imagine me running around a @NASCAR event with these one @Scobleizer 😎

Notice the bounce is gone without cropping the scene. Get it now?

#3D #AI
	  </pre>
	</li>
	<li><a href="https://twitter.com/van_eng622/status/1540148501887488000">https://twitter.com/van_eng622/status/1540148501887488000</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.46.20.png"><!-- 1331x789 -->
	      <img src="Screen Shot 2022-06-27 at 12.46.20_thumb.jpg" width="600" height="356" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Facebookのスマートグラス「Ray-Ban Stories」で撮影した移動中の動画を
Instant NeRFで3D化したもの

この動画から、ここまでの3Dを生成できるのか...
Instant NeRFやばいな

時代が動画から3Dに移行していくことが肌で感じられる事例
	  </pre>
	</li>

	<li><a href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.16.26.png"><!-- 2672x787 -->
	      <img src="Screen Shot 2022-06-28 at 18.16.26_thumb.jpg" width="800" height="236" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/">Getting Started with NVIDIA Instant NeRFs (By Jonathan Stephens May 12, 2022)</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.19.27.png"><!-- 2677x986 -->
	      <img src="Screen Shot 2022-06-28 at 18.19.27_thumb.jpg" width="800" height="295" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li><a href="https://youtu.be/z3-fjYzd0BA">https://youtu.be/z3-fjYzd0BA</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.21.52.png"><!-- 1777x902 -->
	      <img src="Screen Shot 2022-06-28 at 18.21.52_thumb.jpg" width="800" height="406" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>


	<li><a href="https://speakerdeck.com/itagakim/run-instant-nerf-on-docker">Run Instant NeRF on Docker (masa-ita April 26, 2022)</a>
	</li>

      </ul>
    </li><!-- Instant NGP -->


    <li><b>EyeNeRF</b> (<a href="https://twitter.com/ak92501/status/1538689482534309890">https://twitter.com/ak92501/status/1538689482534309890</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.27.53.png"><!-- 1318x654 -->
	  <img src="Screen Shot 2022-06-27 at 12.27.53_thumb.jpg" width="600" height="298" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes
abs: <a href="https://arxiv.org/abs/2206.08428">https://arxiv.org/abs/2206.08428</a>
      </pre>
    </li><!-- EyeNeRF -->

    <li><b>IRON</b> (<a href="https://twitter.com/Jimantha/status/1539599469305643011">https://twitter.com/Jimantha/status/1539599469305643011</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.20.55.png"><!-- 1306x915 -->
	  <img src="Screen Shot 2022-06-27 at 12.20.55_thumb.jpg" width="600" height="420" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
To all the CVPR-heads out there -- check out @KaiZhang9546 's work on
inverse rendering in this morning's oral session!
Relightable 3D meshes from photos, with really beautiful results.

IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images
abs: <a href="https://arxiv.org/abs/2204.02232">https://arxiv.org/abs/2204.02232</a>
project page: <a href="https://kai-46.github.io/IRON-website/">https://kai-46.github.io/IRON-website/</a>
      </pre>
      <ul>
	<li>github: <a href="https://github.com/Kai-46/IRON">https://github.com/Kai-46/IRON</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 22.03.05.png"><!-- 2673x1122 -->
	      <img src="Screen Shot 2022-06-27 at 22.03.05_thumb.jpg" width="800" height="336" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- IRON -->

    <li><b>EventNeRF</b> (<a href="https://twitter.com/ak92501/status/1540134704057294848">https://twitter.com/ak92501/status/1540134704057294848</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.14.28.png"><!-- 1294x619 -->
	  <img src="Screen Shot 2022-06-27 at 12.14.28_thumb.jpg" width="600" height="287" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
EventNeRF: Neural Radiance Fields from a Single Colour Event Camera
abs: <a href="https://arxiv.org/abs/2206.11896">https://arxiv.org/abs/2206.11896</a>
project page: <a href="https://4dqv.mpi-inf.mpg.de/EventNeRF/">https://4dqv.mpi-inf.mpg.de/EventNeRF/</a>
      </pre>
    </li><!-- EventNeRF -->

    <li><b>VBA</b> (<a href="https://twitter.com/ronnieclark__/status/1537789877265252352">https://twitter.com/ronnieclark__/status/1537789877265252352</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 13.04.47.png"><!-- 1319x639 -->
	  <img src="Screen Shot 2022-06-27 at 13.04.47_thumb.jpg" width="600" height="291" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Looking for super fast neural volume reconstruction and rendering? 

At #CVPR2022 I'll be presenting Volumetric Bundle Adjustment (VBA).
VBA enables fast and clean reconstruction for live 3D capture
while a video is being recorded.

Project page: <a href="https://r0nn13.github.io/volumetric-bundle-adjustment/">https://r0nn13.github.io/volumetric-bundle-adjustment/</a>
      </pre>
    </li><!-- VBA -->

    <li><b>Multimodal Colored Point Cloud to Image Alignment</b> (<a href="https://twitter.com/ak92501/status/1539820424376320000">https://twitter.com/ak92501/status/1539820424376320000</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.17.22.png"><!-- 1301x528 -->
	  <img src="Screen Shot 2022-06-27 at 12.17.22_thumb.jpg" width="600" height="244" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Multimodal Colored Point Cloud to Image Alignment
paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rotstein_Multimodal_Colored_Point_Cloud_to_Image_Alignment_CVPR_2022_paper.pdf">pdf</a>
colab: <a href="https://colab.research.google.com/drive/1M_FX4BNDZdSvyzG-htjs-g_IlSvn1mwM?usp=sharing">https://colab.research.google.com/...</a>
      </pre>
    </li><!-- Multimodal Colored Point Cloud to Image Alignment -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-photogrammetry">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      フォトグラメトリ、LiDAR 関係
    </div>
  </center>

  <ul>
    <li><a href="https://twitter.com/DuckbillStudio/status/1537723495370858496">https://twitter.com/DuckbillStudio/status/1537723495370858496</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 13.05.45.png"><!-- 1314x633 -->
	  <img src="Screen Shot 2022-06-27 at 13.05.45_thumb.jpg" width="600" height="289" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
愛媛松山の重要文化財「萬翠荘」の3DCG動画です。
実写ではありません！！フォトグラメトリで制作した3Dモデルです。
是非YouTubeの4K動画も見てください～。
#EHIME3D #photogrammetry
      </pre>
    </li>

    <li><a href="https://twitter.com/view0608/status/1540557654262632448">https://twitter.com/view0608/status/1540557654262632448</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 12.44.43.png"><!-- 1332x631 -->
	  <img src="Screen Shot 2022-06-27 at 12.44.43_thumb.jpg" width="600" height="284" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
JR三ノ宮駅の北口から南口を散歩しながらiPhoneでスキャン。
高架下は飲食店が並び、南口の広場ではキッチンカーやパークエリアが
広がりにぎわいのあるまちづくりが進められている。
アプリは @EveryPointIO を使用。
      </pre>
      <ul>
	<li><a href="https://twitter.com/jonstephens85/status/1540106031665033216">https://twitter.com/jonstephens85/status/1540106031665033216</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.47.15.png"><!-- 1316x644 -->
	      <img src="Screen Shot 2022-06-27 at 12.47.15_thumb.jpg" width="600" height="294" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
What is you could produce a 3d flythrough of a #concrete production plant
every 15 minutes, every hour, on demand?
No need to deploy cameras, they are already there.
When it happens, expect @EveryPointIO and @stockpilereport
to be blazing that trail!
#Mining #ComputerVision
	  </pre>
	</li>

	<li><a href="https://everypoint.io/">https://everypoint.io/</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.25.36.png"><!-- 2674x665 -->
	      <img src="Screen Shot 2022-06-28 at 18.25.36_thumb.jpg" width="800" height="199" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-text-to-image">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      Text to Image など
    </div>
  </center>

  <ul>
    <li><b>CogView2</b>
      <ul>
	<li><a href="https://twitter.com/gclue_akira/status/1540088475113754624">https://twitter.com/gclue_akira/status/1540088475113754624</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.15.51.png"><!-- 1317x569 -->
	      <img src="Screen Shot 2022-06-27 at 12.15.51_thumb.jpg" width="600" height="259" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
CogView2のWebデモ
<a href="https://huggingface.co/spaces/THUDM/CogView2">https://huggingface.co/spaces/THUDM/CogView2</a>
	  </pre>
	</li>
	<li><a href="https://twitter.com/ak92501/status/1540033980128436226">https://twitter.com/ak92501/status/1540033980128436226</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.16.34.png"><!-- 1321x883 -->
	      <img src="Screen Shot 2022-06-27 at 12.16.34_thumb.jpg" width="600" height="401" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
a @Gradio Demo for CogView2: Faster and Better Text-to-Image Generation
via Hierarchical Transformers on @huggingface Spaces by
<a href="https://huggingface.co/hysts">https://huggingface.co/hysts</a>

demo: <a href="https://huggingface.co/spaces/THUDM/CogView2">https://huggingface.co/spaces/THUDM/CogView2</a>
	  </pre>
	</li>
	<li><a href="https://huggingface.co/spaces/THUDM/CogView2">https://huggingface.co/spaces/THUDM/CogView2</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 21.48.33.png"><!-- 2682x1254 -->
	      <img src="Screen Shot 2022-06-27 at 21.48.33_thumb.jpg" width="800" height="374" style="border: 2px #ccc solid;" /></a>
	    <br />
	    <a href="Screen Shot 2022-06-27 at 21.51.03.png"><!-- 2678x1271 -->
	      <img src="Screen Shot 2022-06-27 at 21.51.03_thumb.jpg" width="400" height="190" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-27 at 21.52.08.png"><!-- 2677x1274 -->
	      <img src="Screen Shot 2022-06-27 at 21.52.08_thumb.jpg" width="400" height="190" style="border: 2px #ccc solid;" /></a>
	    
	  </center>
	</li>
      </ul>
    </li><!-- CogView2 -->

    <li><b>RegionCLIP</b> (<a href="https://twitter.com/YiwuZhong/status/1540253720377667584">https://twitter.com/YiwuZhong/status/1540253720377667584</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.12.13.png"><!-- 1328x752 -->
	  <img src="Screen Shot 2022-06-27 at 12.12.13_thumb.jpg" width="600" height="340" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
#CVPR2022 We just released a web demo for RegionCLIP
(<a href="https://huggingface.co/spaces/CVPR/regionclip-demo">https://huggingface.co/spaces/CVPR/regionclip-demo</a>).
The pre-trained RegionCLIP demonstrates interesting results in zero-shot object detection.
Check it out!
If you’re interested in our work, please stop by
our poster 77a on Friday morning!

a @Gradio Demo for RegionCLIP: Region-based Language-Image Pretraining
on @huggingface Spaces for @CVPR 2022 by @jw2yang4ai
      </pre>
      <ul>
	<li><a href="https://huggingface.co/spaces/CVPR/regionclip-demo">https://huggingface.co/spaces/CVPR/regionclip-demo</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 21.43.33.png"><!-- 2679x1257 -->
	      <img src="Screen Shot 2022-06-27 at 21.43.33_thumb.jpg" width="800" height="375" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- RegionCLIP -->

    <li><b>FLAVA</b> (<a href="https://twitter.com/ak92501/status/1539749459915149313">https://twitter.com/ak92501/status/1539749459915149313</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.18.41.png"><!-- 1316x641 -->
	  <img src="Screen Shot 2022-06-27 at 12.18.41_thumb.jpg" width="600" height="292" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
a @Gradio Demo for FLAVA: A Foundation Language And Vision Alignment Model on 
@huggingface Spaces for @CVPR 2022 by @apsdehal

demo: <a href="https://huggingface.co/spaces/CVPR/flava-multimodal-zero-shot">https://huggingface.co/spaces/CVPR/flava-multimodal-zero-shot</a>
      </pre>
      <ul>
	<li><a href="https://huggingface.co/spaces/CVPR/flava-multimodal-zero-shot">https://huggingface.co/spaces/CVPR/flava-multimodal-zero-shot</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 21.57.30.png"><!-- 2679x1246 -->
	      <img src="Screen Shot 2022-06-27 at 21.57.30_thumb.jpg" width="800" height="372" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- FLAVA -->

    <li><b>Parti</b> (<a href="https://twitter.com/ak92501/status/1539672920456298498">https://twitter.com/ak92501/status/1539672920456298498</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.19.16.png"><!-- 1319x891 -->
	  <img src="Screen Shot 2022-06-27 at 12.19.16_thumb.jpg" width="600" height="405" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Scaling Autoregressive Models for Content-Rich Text-to-Image Generation
paper: <a href="https://gweb-research-parti.web.app/parti_paper.pdf">https://gweb-research-parti.web.app/parti_paper.pdf</a>
project page: <a href="https://parti.research.google">https://parti.research.google</a>
github: <a href="https://github.com/google-research/parti">https://github.com/google-research/parti</a>

sota zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO
      </pre>
    </li><!-- Parti -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-super-resolution">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      超解像など
    </div>
  </center>

  <ul>
    <li><b>VideoINR</b> (<a href="https://twitter.com/xiaolonw/status/1537508196557811713">https://twitter.com/xiaolonw/status/1537508196557811713</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 13.06.26.png"><!-- 1317x781 -->
	  <img src="Screen Shot 2022-06-27 at 13.06.26_thumb.jpg" width="600" height="356" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
We are presenting VideoINR in #CVPR2022 ,
a new continuous Video Implicit Neural Representation,
that allows any scale super-resolution / interpolation in both space and time.

Paper: <a href="https://arxiv.org/abs/2206.04647">https://arxiv.org/abs/2206.04647</a>
Project page: <a href="http://zeyuan-chen.com/VideoINR/">http://zeyuan-chen.com/VideoINR/</a>
      </pre>
      <center>
	<a href="Screen Shot 2022-06-28 at 23.02.49.png"><!-- 2678x983 -->
	  <img src="Screen Shot 2022-06-28 at 23.02.49_thumb.jpg" width="800" height="294" style="border: 2px #ccc solid;" /></a>
      </center>
      <ul>
	<li><a href="https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution">https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.02.42.png"><!-- 2662x1150 -->
	      <img src="Screen Shot 2022-06-28 at 23.02.42_thumb.jpg" width="800" height="346" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- VideoINR -->

    <li><b>Come-Closer-Diffuse-Faster</b> (<a href="https://twitter.com/hyungjin_chung/status/1539983653186543616">https://twitter.com/hyungjin_chung/status/1539983653186543616</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.14.54.png"><!-- 1308x1010 -->
	  <img src="Screen Shot 2022-06-27 at 12.14.54_thumb.jpg" width="600" height="463" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
For those interested diffusion models and inverse problems,
come check out our poster on 174a #CVPR2022 ! Joint work with 
@Byeongsu9

Come-Closer-Diffuse-Faster:
Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction
abs: <a href="https://arxiv.org/abs/2112.05146">https://arxiv.org/abs/2112.05146</a>

method can achieve sota reconstruction performance at significantly reduced sampling steps
      </pre>
    </li><!-- Come-Closer-Diffuse-Faster -->

    <li><b>Megapixel Image Generation...</b> (<a href="https://twitter.com/ak92501/status/1541219433259175937">https://twitter.com/ak92501/status/1541219433259175937</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.07.42.png"><!-- 1303x917 -->
	  <img src="Screen Shot 2022-06-27 at 12.07.42_thumb.jpg" width="600" height="422" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Megapixel Image Generation with Step-Unrolled Denoising Autoencoders
abs: <a href="https://arxiv.org/abs/2206.12351">https://arxiv.org/abs/2206.12351</a>

obtain FID scores of 10.56 on FFHQ256—close to the original VQ-GAN
in less than half the sampling steps—and 21.85 on FFHQ1024
in only 100 sampling steps
      </pre>
    </li><!-- Megapixel Image Generation... -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-demos">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      デモ
    </div>
  </center>

  <ul>
    <li><b>SketchEdit</b> (<a href="https://twitter.com/zengxianyu18/status/1539471341266542593">https://twitter.com/zengxianyu18/status/1539471341266542593</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.21.34.png"><!-- 1315x984 -->
	  <img src="Screen Shot 2022-06-27 at 12.21.34_thumb.jpg" width="600" height="449" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Thanks for sharing our work😀 I will be presenting SketchEdit @CVPR 2022.
If you are interested in our work or just want to discuss with me,
don’t hesitate to come by on Wednesday to talk and play the demo,
I am happy to discuss with anyone and make new connections #CVPR2022

SketchEdit: Mask-Free Local Image Manipulation with Partial Sketches
abs: <a href="https://arxiv.org/abs/2111.15078">https://arxiv.org/abs/2111.15078</a>
project page: <a href="https://zengxianyu.github.io/sketchedit/">https://zengxianyu.github.io/sketchedit/</a>
github: <a href="https://github.com/zengxianyu/sketchedit">https://github.com/zengxianyu/sketchedit</a>
      </pre>
      <ul>
	<li><a href="https://github.com/zengxianyu/sketchedit">https://github.com/zengxianyu/sketchedit</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 22.06.50.png"><!-- 2664x1114 -->
	      <img src="Screen Shot 2022-06-27 at 22.06.50_thumb.jpg" width="800" height="335" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>interactive demo (<a href="http://47.57.135.203:8001/">http://47.57.135.203:8001/</a>)
	  <center>
	    <a href="Screen Shot 2022-06-27 at 22.06.29.png"><!-- 2669x1026 -->
	      <img src="Screen Shot 2022-06-27 at 22.06.29_thumb.jpg" width="800" height="308" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- SketchEdit -->

    <li><b>Analog Clock Reading in the Wild</b> (<a href="https://twitter.com/ak92501/status/1537989713256099848">https://twitter.com/ak92501/status/1537989713256099848</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.32.00.png"><!-- 1304x865 -->
	  <img src="Screen Shot 2022-06-27 at 12.32.00_thumb.jpg" width="600" height="398" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
a @Gradio Demo for It's About Time: Analog Clock Reading in the Wild
on @huggingface Spaces for @CVPR 2022
 
demo: <a href="https://huggingface.co/spaces/CVPR/time">https://huggingface.co/spaces/CVPR/time</a>
      </pre>
      <ul>
	<li><a href="https://twitter.com/chaaarig/status/1537586593279909888">https://twitter.com/chaaarig/status/1537586593279909888</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.36.46.png"><!-- 1308x1014 -->
	      <img src="Screen Shot 2022-06-27 at 12.36.46_thumb.jpg" width="600" height="465" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Also have a try at our demo on @Gradio/@huggingface !

Demo: <a href="https://huggingface.co/spaces/CVPR/time">https://huggingface.co/spaces/CVPR/time</a>

and do join the CVPR 2022 organization page to check out other demos
/ make one https://huggingface.co/CVPR
	  </pre>
	</li>
	<li>実験<br />
	  <a href="https://twitter.com/DearWatchLover/status/1541673141507829761">https://twitter.com/DearWatchLover/status/1541673141507829761</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 23.20.19.png"><!-- 1305x845 -->
	      <img src="Screen Shot 2022-06-28 at 23.20.19_thumb.jpg" width="600" height="389" style="border: 2px #ccc solid;" /></a>
	    <br />
	    <a href="Screen Shot 2022-06-28 at 23.18.37.png"><!-- 2676x925 -->
	      <img src="Screen Shot 2022-06-28 at 23.18.37_thumb.jpg" width="800" height="277" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>

      </ul>
    </li><!-- Analog Clock Reading in the Wild -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-pose">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      姿勢推定
    </div>
  </center>

  <ul>
    <li><a href="https://twitter.com/goto_yuta_/status/1539472619585830912">https://twitter.com/goto_yuta_/status/1539472619585830912</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 12.50.56.png"><!-- 1320x626 -->
	  <img src="Screen Shot 2022-06-27 at 12.50.56_thumb.jpg" width="600" height="285" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
NVIDIAが姿勢推定でブレークスルーを起こした理由を簡潔にまとめてる記事。
既存の技術だと物理法則を無視した推定結果が多かったことを踏まえて
損失に物理制約を盛り込んで定式化して精度を跳ね上げたらしい。
<a href="https://medium.com/aiguys/pose-estimation-and-nvidias-breakthrough-16fb6263bd48">https://medium.com/aiguys/pose-estimation-and-nvidias-breakthrough-16fb6263bd48</a>
      </pre>
      <ul>
	<li><a href="https://medium.com/aiguys/pose-estimation-and-nvidias-breakthrough-16fb6263bd48">Pose estimation and NVIDIA’s breakthrough (Vishal Rajput Jan 2)</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.38.38.png"><!-- 2681x1147 -->
	      <img src="Screen Shot 2022-06-28 at 18.38.38_thumb.jpg" width="800" height="342" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li><b>Strided Transformer for Pose 3D</b> (<a href="https://twitter.com/PINTO03091/status/1540937546792075264">https://twitter.com/PINTO03091/status/1540937546792075264</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.41.08.png"><!-- 1324x621 -->
	  <img src="Screen Shot 2022-06-27 at 12.41.08_thumb.jpg" width="600" height="281" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
実際に動かしてかなりキレイに三次元推定できたのだけど、処理に時間掛かりすぎｗ
<a href="https://github.com/Vegetebird/StridedTransformer-Pose3D">https://github.com/Vegetebird/StridedTransformer-Pose3D</a>
      </pre>
      <ul>
	<li><a href="https://github.com/Vegetebird/StridedTransformer-Pose3D">https://github.com/Vegetebird/StridedTransformer-Pose3D</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.09.50.png"><!-- 2854x776 -->
	      <img src="Screen Shot 2022-06-28 at 18.09.50_thumb.jpg" width="800" height="218" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li>

    <li><b>TAVA</b> (<a href="https://twitter.com/ak92501/status/1538719219818409994">https://twitter.com/ak92501/status/1538719219818409994</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.26.26.png"><!-- 1295x691 -->
	  <img src="Screen Shot 2022-06-27 at 12.26.26_thumb.jpg" width="600" height="320" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
TAVA: Template-free Animatable Volumetric Actors
abs: <a href="https://arxiv.org/abs/2206.08929">https://arxiv.org/abs/2206.08929</a>
project page: <a href="https://liruilong.cn/projects/tava/">https://liruilong.cn/projects/tava/</a>
      </pre>
    </li><!-- TAVA -->

    <li><b>MediaPipe</b> (<a href="https://twitter.com/yeemachine/status/1539639359657447424">https://twitter.com/yeemachine/status/1539639359657447424</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.46.43.png"><!-- 1322x1059 -->
	  <img src="Screen Shot 2022-06-27 at 12.46.43_thumb.jpg" width="600" height="481" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
It still feels like a dream. 
Yes, I am working on #Vtuber tech at Google AI! 
💪😎💪
Our new BlazePose GHUM Holistic model predicts full body
and hand joint rotations for 3D avatars.
Available soon at <a href="http://mediapipe.dev">http://mediapipe.dev</a>. 🐎💃🎵

Paper and live demo at @CVPR for #CVPR2022.
      </pre>
    </li><!-- MediaPipe -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part1-misc">
    <div style="font-size: 50px; font-weight: bolder;">
      <br />
      その他
    </div>
  </center>

  <ul>
    <li><b>ShapeFormer</b> (<a href="https://twitter.com/yan_xg/status/1539109905743101952">https://twitter.com/yan_xg/status/1539109905743101952</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.24.24.png"><!-- 1310x911 -->
	  <img src="Screen Shot 2022-06-27 at 12.24.24_thumb.jpg" width="600" height="417" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Code/pretained model is released, please have a try! 😁
<a href="https://github.com/QhelDIV/ShapeFormer">https://github.com/QhelDIV/ShapeFormer</a>

ShapeFormer: Transformer-based Shape Completion via Sparse Representation
abs: <a href="https://arxiv.org/abs/2201.10326">https://arxiv.org/abs/2201.10326</a>
project page: <a href="https://shapeformer.github.io">https://shapeformer.github.io</a>
      </pre>
      <ul>
	<li><a href="https://github.com/QhelDIV/ShapeFormer">https://github.com/QhelDIV/ShapeFormer</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 22.13.12.png"><!-- 2675x1161 -->
	      <img src="Screen Shot 2022-06-27 at 22.13.12_thumb.jpg" width="800" height="347" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- ShapeFormer -->


    <li><b>iSEE</b> (<a href="https://twitter.com/ak92501/status/1538706936211951617">https://twitter.com/ak92501/status/1538706936211951617</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.27.14.png"><!-- 1310x762 -->
	  <img src="Screen Shot 2022-06-27 at 12.27.14_thumb.jpg" width="600" height="349" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
What do navigation agents learn about their environment?
abs: <a href="https://arxiv.org/abs/2206.08500">https://arxiv.org/abs/2206.08500</a>
github: <a href="https://github.com/allenai/iSEE">https://github.com/allenai/iSEE</a>
      </pre>
    </li><!-- iSEE -->

    <li><b>PlanarRecon</b> (<a href="https://twitter.com/gadelha_m/status/1537546360622157824">https://twitter.com/gadelha_m/status/1537546360622157824</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.37.19.png"><!-- 1312x859 -->
	  <img src="Screen Shot 2022-06-27 at 12.37.19_thumb.jpg" width="600" height="393" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Always nice to see the work in AK’s feed! Congrats, @YimingXie4!

PlanarRecon: Real-time 3D Plane Detection and Reconstruction from Posed Monocular Videos
abs: <a href="https://arxiv.org/abs/2206.07710">https://arxiv.org/abs/2206.07710</a>
project page: <a href="https://neu-vi.github.io/planarrecon/">https://neu-vi.github.io/planarrecon/</a>
      </pre>
    </li><!-- PlanarRecon -->


    <li><b>NU-Wave 2</b> (<a href="https://twitter.com/ak92501/status/1538686489491648514">https://twitter.com/ak92501/status/1538686489491648514</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.28.29.png"><!-- 1324x826 -->
	  <img src="Screen Shot 2022-06-27 at 12.28.29_thumb.jpg" width="600" height="374" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates
abs: <a href="https://arxiv.org/abs/2206.08545">https://arxiv.org/abs/2206.08545</a>

a diffusion model for neural audio upsampling that enables the generation of
48 kHz audio signals from inputs of various sampling rates with a single model
      </pre>
    </li><!-- NU-Wave 2 -->

    <li><b>AI Song Contest 2022 - the finalists</b> (<a href="https://twitter.com/keunwoochoi/status/1537825500864839683">https://twitter.com/keunwoochoi/status/1537825500864839683</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.30.10.png"><!-- 1350x466 -->
	  <img src="Screen Shot 2022-06-27 at 12.30.10_thumb.jpg" width="600" height="207" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
<a href="https://aisongcontest.com/the-2022-finalists">https://aisongcontest.com/the-2022-finalists</a>

AI Song Contest 2022 - the finalists 🔥🔥🔥
      </pre>
      <ul>
	<li><a href="https://aisongcontest.com/the-2022-finalists">https://aisongcontest.com/the-2022-finalists</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 17.55.13.png"><!-- 2681x778 -->
	      <img src="Screen Shot 2022-06-28 at 17.55.13_thumb.jpg" width="800" height="232" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- AI Song Contest 2022 - the finalists -->

    <li><b>Virtual Correspondences</b> (<a href="https://twitter.com/ShenlongWang/status/1537904063127224320">https://twitter.com/ShenlongWang/status/1537904063127224320</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 13.02.28.png"><!-- 1313x720 -->
	  <img src="Screen Shot 2022-06-27 at 13.02.28_thumb.jpg" width="600" height="329" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Excited to share our #CVPR2022 work on
associating images with extreme viewpoint differences!
We propose to find "virtual correspondences"
and use them to reason relative camera poses. 

Please check the project website for more details: 
<a href="https://virtual-correspondence.github.io">https://virtual-correspondence.github.io</a>
      </pre>
      <ul>
	<li><a href="https://twitter.com/weichiuma/status/1537875561745223680">https://twitter.com/weichiuma/status/1537875561745223680</a>
	  <center>
	    <a href="Screen Shot 2022-06-27 at 12.31.07.png"><!-- 1305x1221 -->
	      <img src="Screen Shot 2022-06-27 at 12.31.07_thumb.jpg" width="600" height="561" style="border: 2px #ccc solid;" /></a>
	  </center>
	  <pre>
Can you match images with little or no overlaps?
 
Humans can🧠but most existing methods fail😰
 
Our #CVPR2022 paper shoots camera rays through the scene to form
“virtual correspondences” & uses epipolar geometry.
 
w/ @ajyang99 @ShenlongWang @RaquelUrtasun Antonio Torralba
	  </pre>
	</li>
      </ul>
    </li><!-- Virtual Correspondences -->


    <li><b>BYOL-Explore</b> (<a href="https://twitter.com/ak92501/status/1537621348339572736">https://twitter.com/ak92501/status/1537621348339572736</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.35.20.png"><!-- 1331x909 -->
	  <img src="Screen Shot 2022-06-27 at 12.35.20_thumb.jpg" width="600" height="410" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
BYOL-Explore: Exploration by Bootstrapped Prediction
abs: <a href="https://arxiv.org/abs/2206.08332">https://arxiv.org/abs/2206.08332</a>

BYOL-Explore achieves superhuman performance on
the ten hardest exploration games in Atari
while having a much simpler design than other competitive agents
      </pre>
    </li><!-- BYOL-Explore -->


    <li><b>HuggingFace Hub Client Library</b> (<a href="https://twitter.com/mervenoyann/status/1540470328111030274">https://twitter.com/mervenoyann/status/1540470328111030274</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.42.02.png"><!-- 1309x823 -->
	  <img src="Screen Shot 2022-06-27 at 12.42.02_thumb.jpg" width="600" height="377" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
automate your workflows with @huggingface Hub client library 😏
      </pre>
    </li><!-- HuggingFace Hub Client Library -->


    <li><b>Open-source large language models</b> (<a href="https://twitter.com/awnihannun/status/1540055549156372480">https://twitter.com/awnihannun/status/1540055549156372480</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 12.48.18.png"><!-- 1312x731 -->
	  <img src="Screen Shot 2022-06-27 at 12.48.18_thumb.jpg" width="600" height="334" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Open-source large language models is moving fast.
Just today, three big releases:
* Google's UL2 20B on HuggingFace <a href="https://huggingface.co/google/ul2">https://huggingface.co/google/ul2</a>
* Meta's OPT 65B <a href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">https://github.com/facebookresearch/metaseq/tree/main/projects/OPT</a>
* Yandex 100B (!) LM <a href="https://github.com/yandex/YaLM-100B">https://github.com/yandex/YaLM-100B</a>
      </pre>
    </li><!-- Open-source large language models -->

    <li><a href="https://twitter.com/AlexandreDevaux/status/1539946052824055808">https://twitter.com/AlexandreDevaux/status/1539946052824055808</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 12.49.33.png"><!-- 1315x665 -->
	  <img src="Screen Shot 2022-06-27 at 12.49.33_thumb.jpg" width="600" height="303" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Push your walls à la Spike Jonze in #MixedReality 🙂
In #WebXR (hololens) :
  Detect Wall (Hit-Test)
+ Capture and project (UserMediaAPI)
+ Vertex displacement
@NYTimesRD #AR
      </pre>
    </li>

    <li><a href="https://twitter.com/jaguring1/status/1539913772063543296">https://twitter.com/jaguring1/status/1539913772063543296</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 12.50.07.png"><!-- 1332x777 -->
	  <img src="Screen Shot 2022-06-27 at 12.50.07_thumb.jpg" width="600" height="350" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
2021年
1月：DALL-E、CLIP
5月：LaMDA
6月：Codex
8月：Jurassic-1
9月：FLAN
10月：MT-NLG
11月：Florence、ExT5
12月：Gopher、GaLM、GLIDE

2022年
2月：AlphaCode、Perceiver AR
3月：Chinchilla
4月：PaLM、DALL-E2、Flamingo
5月：OPT-175B、CoCa、UL2、Gato、Imagen
6月：Unified-IO、Parti
      </pre>
    </li>

    <li><a href="https://twitter.com/ogawa_yutaro_22/status/1538661505000173568">https://twitter.com/ogawa_yutaro_22/status/1538661505000173568</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 13.01.13.png"><!-- 1314x733 -->
	  <img src="Screen Shot 2022-06-27 at 13.01.13_thumb.jpg" width="600" height="335" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Google全体として、TensorFlowからJAXへと移行する気配です。
今後のTFのメンテはどうなるんだろう

確かにJAXは使いやすい部分もあり、PyTorchもJAXの機能を取り込んでいます
（FUNCTORCH：<a href="https://pytorch.org/functorch/stable/">https://pytorch.org/functorch/stable/</a>）

2階微分のヘシアンの計算とか不便なので

今後はPyTorchかJAXかになりそうです


After losing out to PyTorch, Google is quietly moving
to roll out a new AI framework internally called JAX.
It's expected to become the underpinning of Google's products,
fixing some of TensorFlow's biggest pain points
that frustrate Googlers internally
      </pre>
    </li>

    <li><b>Project Aria</b> (<a href="https://twitter.com/TweetEdMiller/status/1537495397509349378">https://twitter.com/TweetEdMiller/status/1537495397509349378</a>)
      <center>
	<a href="Screen Shot 2022-06-27 at 13.03.46.png"><!-- 1324x636 -->
	  <img src="Screen Shot 2022-06-27 at 13.03.46_thumb.jpg" width="600" height="288" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
Ahead of #CVPR2022 ,
I’m excited to share the open dataset of Project Aria data
from Meta Reality Labs,
along with accompanying open research tools
designed to accelerate AI and ML research.
<a href="https://about.facebook.com/realitylabs/projectaria/datasets">https://about.facebook.com/realitylabs/projectaria/datasets</a>
A little about the dataset and why I think it’s so exciting…
      </pre>
      <ul>
	<li><a href="https://about.facebook.com/realitylabs/projectaria/datasets">Project Aria Pilot Dataset</a>
	  <center>
	    <a href="Screen Shot 2022-06-28 at 18.43.23.png"><!-- 2678x1155 -->
	      <img src="Screen Shot 2022-06-28 at 18.43.23_thumb.jpg" width="800" height="345" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- Project Aria -->

    <li><a href="https://twitter.com/psi_ni_phi/status/1537784531553353729">https://twitter.com/psi_ni_phi/status/1537784531553353729</a>
      <center>
	<a href="Screen Shot 2022-06-27 at 13.03.09.png"><!-- 1323x723 -->
	  <img src="Screen Shot 2022-06-27 at 13.03.09_thumb.jpg" width="600" height="328" style="border: 2px #ccc solid;" /></a>
      </center>
      <pre>
MeshDeformで顔のパース補正できるか検証したらあっさりと上手くいってビックリ。
カメラの視線に合うよう立方体をスクショのように手前側を絞るだけ。
要は、平行投影で設定画から3D起こすと、
パースがかかった時に不自然な顔になる現象を補正する手法です。
お試しあれ。
#b3d #blender3d #ゆるキャンΔ
      </pre>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="part2">
    <div style="font-size: 50px; font-weight: bolder;">
      パート２
      技術書典１３企画会議！
    </div>
  </center>

  <ul>
    <li>参加申し込みをしました！ (<a href="https://twitter.com/zenkeiaif/status/1542038873936924672">https://twitter.com/zenkeiaif/status/1542038873936924672</a>)
      <center>
	<a href="Screen Shot 2022-06-29 at 15.55.00.png"><!-- 1319x930 -->
	  <img src="Screen Shot 2022-06-29 at 15.55.00_thumb.jpg" width="600" height="423" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>これまで参加した技術書典
      <center>
	<a href="Screen Shot 2022-06-29 at 16.02.14.png"><!-- 2780x919 -->
	  <img src="Screen Shot 2022-06-29 at 16.02.14_thumb.jpg" width="800" height="264" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-invitation">
    <div style="font-size: 50px; font-weight: bolder;">
      技術書典（イベント）へのお誘い
    </div>
    <br /><br />
    <div style="font-size: 40px; font-weight: bolder;">
      同人誌活動は<br />
      今のビジネスの最先端！
    </div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li id="part2-mission-statement">コロナで ZOOM 化した後の
      <center>
	<div style="font-size: 50px; font-weight: bolder;">
	  「ZENKEI AI FORUM とは何か？」
	</div>
      </center>
      について思索に耽っていた頃
      <ul>
	<li><a href="https://youtu.be/tWvm7LmXKhE?t=2776">ZAF-2006</a> - 瀧本哲史</li>
	<li><a href="https://youtu.be/JzggEa_tPE4?t=659">ZAF-2008</a>「ZAF のミッション・ステートメント」</li>
      </ul>
      <center>
	<a href="Screen Shot 2022-06-29 at 17.39.27.png"><!-- 1777x1120 -->
	  <img src="Screen Shot 2022-06-29 at 17.39.27_thumb.jpg" width="500" height="315" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 17.36.24.png"><!-- 1779x1117 -->
	  <img src="Screen Shot 2022-06-29 at 17.36.24_thumb.jpg" width="500" height="314" style="border: 2px #ccc solid;" /></a>
      </center>
      <center>
	<a href="Screen Shot 2022-06-29 at 17.21.41.png"><!-- 2546x743 -->
	  <img src="Screen Shot 2022-06-29 at 17.21.41_thumb.jpg" width="1000" height="292" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 17.22.04.png"><!-- 2547x1425 -->
	  <img src="Screen Shot 2022-06-29 at 17.22.04_thumb.jpg" width="1000" height="559" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 17.22.11.png"><!-- 2544x1419 -->
	  <img src="Screen Shot 2022-06-29 at 17.22.11_thumb.jpg" width="1000" height="558" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="part2-fandom-economy">
    <div style="font-size: 50px; font-weight: bolder;">
      同人誌活動は<br />
      今のビジネスの最先端！
    </div>
  </center>

  <ul>
    <li>若林恵の本『<a href="https://www.amazon.co.jp/dp/499112607X">はりぼて王国年代記 【週刊だえん問答 第2集】</a>』から
      <center>
	<a href="Screen Shot 2022-06-29 at 16.59.53.png"><!-- 2754x903 -->
	  <img src="Screen Shot 2022-06-29 at 16.59.53_thumb.jpg" width="1000" height="328" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 17.00.34.png"><!-- 2761x992 -->
	  <img src="Screen Shot 2022-06-29 at 17.00.34_thumb.jpg" width="1000" height="359" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

    <li>この話は、多分、以下の本『<a href="https://www.amazon.co.jp/dp/4833441292">ファンダムエコノミー入門</a>』の内容に続いていくんだと思う
      <center>
	<a href="Screen Shot 2022-06-29 at 17.05.03.png"><!-- 1761x1193 -->
	  <img src="Screen Shot 2022-06-29 at 17.05.03_thumb.jpg" width="800" height="542" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li id="part2-line-up">これまでのラインナップ
      <center>
	<a href="Screen Shot 2022-06-29 at 16.26.31.png"><!-- 2784x511 -->
	  <img src="Screen Shot 2022-06-29 at 16.26.31_thumb.jpg" width="1000" height="184" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 16.26.39.png"><!-- 2772x1100 -->
	  <img src="Screen Shot 2022-06-29 at 16.26.39_thumb.jpg" width="1000" height="397" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 16.26.51.png"><!-- 2780x1016 -->
	  <img src="Screen Shot 2022-06-29 at 16.26.51_thumb.jpg" width="1000" height="365" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 16.27.03.png"><!-- 2789x942 -->
	  <img src="Screen Shot 2022-06-29 at 16.27.03_thumb.jpg" width="1000" height="338" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 16.27.13.png"><!-- 2784x1063 -->
	  <img src="Screen Shot 2022-06-29 at 16.27.13_thumb.jpg" width="1000" height="382" style="border: 2px #ccc solid;" /></a>
	<a href="Screen Shot 2022-06-29 at 16.27.22.png"><!-- 2667x1419 -->
	  <img src="Screen Shot 2022-06-29 at 16.27.22_thumb.jpg" width="1000" height="532" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li id="part2-my-personal-project">自分の企画
      <ul>
	<li>三部作の前日譚としてのナンバー・ゼロ
	  <ul>
	    <li>まずは表紙からツイート (<a href="https://twitter.com/ichiki_k/status/1386981834223886339">https://twitter.com/ichiki_k/status/1386981834223886339</a>)
	      <center>
		<a href="Screen Shot 2022-06-29 at 16.15.11.png"><!-- 1238x1277 -->
		  <img src="Screen Shot 2022-06-29 at 16.15.11_thumb.jpg" width="582" height="600" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	    <li>宣言ツイート (<a href="https://twitter.com/ichiki_k/status/1388740363091726338">https://twitter.com/ichiki_k/status/1388740363091726338</a>)
	      <center>
		<a href="Screen Shot 2022-06-29 at 16.13.51.png"><!-- 1235x1332 -->
		  <img src="Screen Shot 2022-06-29 at 16.13.51_thumb.jpg" width="556" height="600" style="border: 2px #ccc solid;" /></a>
	      </center>
	    </li>
	    <li>進んでない進捗の懺悔ツイート
	      (<a href="https://twitter.com/ichiki_k/status/1487400479110082560">https://twitter.com/ichiki_k/status/1487400479110082560</a>)
	      <center>
		<table border="0">
		  <tr>
		    <td valign="top">
		      <a href="Screen Shot 2022-06-29 at 16.09.32.png"><!-- 1223x497 -->
			<img src="Screen Shot 2022-06-29 at 16.09.32_thumb.jpg" width="600" height="244" style="border: 2px #ccc solid;" /></a>
		    </td>
		    <td valign="top">
		      <a href="Screen Shot 2022-06-29 at 16.09.41.png"><!-- 1220x1291 -->
			<img src="Screen Shot 2022-06-29 at 16.09.41_thumb.jpg" width="567" height="600" style="border: 2px #ccc solid;" /></a>
		    </td>
		  </tr>
		</table>
	      </center>
	    </li>
	  </ul>
	</li>

      </ul>
    </li><!-- 自分の企画 -->
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <ul>
    <li id="part2-as-a-circle">サークルの企画
      <ul>
	<li>ZAM 構想（既刊 - 印刷版は <a href="https://zenkei.booth.pm/">BOOTH</a> で頒布中）
	  <center>
	    <a href="Screen Shot 2022-06-29 at 16.47.20.png"><!-- 2789x699 -->
	      <img src="Screen Shot 2022-06-29 at 16.47.20_thumb.jpg" width="1000" height="251" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.47.23.png"><!-- 2792x804 -->
	      <img src="Screen Shot 2022-06-29 at 16.47.23_thumb.jpg" width="1000" height="288" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.47.29.png"><!-- 2762x791 -->
	      <img src="Screen Shot 2022-06-29 at 16.47.29_thumb.jpg" width="1000" height="286" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
	<li>未完の月刊 ZAM どうなってるのか？
	  (cf. <a href="https://hello-ai-forum.github.io/pages/ZAF202205/ichiki/">ZAF-2205</a>)
	  <center>
	    <a href="Screen Shot 2022-06-29 at 16.40.43.png"><!-- 2671x704 -->
	      <img src="Screen Shot 2022-06-29 at 16.40.43_thumb.jpg" width="1000" height="264" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.40.22.png"><!-- 2670x812 -->
	      <img src="Screen Shot 2022-06-29 at 16.40.22_thumb.jpg" width="1000" height="304" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.40.29.png"><!-- 2671x787 -->
	      <img src="Screen Shot 2022-06-29 at 16.40.29_thumb.jpg" width="1000" height="295" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.40.34.png"><!-- 2668x1184 -->
	      <img src="Screen Shot 2022-06-29 at 16.40.34_thumb.jpg" width="1000" height="444" style="border: 2px #ccc solid;" /></a>
	    <a href="Screen Shot 2022-06-29 at 16.40.39.png"><!-- 2671x801 -->
	      <img src="Screen Shot 2022-06-29 at 16.40.39_thumb.jpg" width="1000" height="300" style="border: 2px #ccc solid;" /></a>
	  </center>
	</li>
      </ul>
    </li><!-- サークルの企画 -->

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />


  <center id="part2-lets-discuss">
    <div style="font-size: 50px; font-weight: bolder;">
      技術書典１３<br />
      企画会議！
    </div>
  </center>

  <ul>
    <li>付記：当日の企画会議の模様
      <center>
	<a href="Screen Shot 2022-06-30 at 11.52.58.png"><!-- 2767x1091 -->
	  <img src="Screen Shot 2022-06-30 at 11.52.58_thumb.jpg" width="1000" height="394" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
  </ul>


  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center id="epilogue">
    <div style="font-size: 50px; font-weight: bolder;">
      今日のおわりに
    </div>
  </center>

  <p>……</p>

  <h3>今後の予定</h3>
  <ul>
    <li>次回 ZAF は 2022 年７月２７日開催の予定です。</li>
    <li>ZAF 講演者、 ZAM 執筆者、絶賛、大募集中です！<br />
      お気軽にお問い合わせください！</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />
  <hr />

  <h2 id="detailed-toc">総合目次</h2>
  <ul>
    <li><b>前座</b>
      <ul>
	<li><a href="#part0-twitch">最近の私（いちき）はどこに向かっているのか？</a></li>
	<li><a href="#part0-dance">踊ってみた-その後</a></li>
      </ul>
    </li>

    <li><b>第１部</b>
      <a href="#part1">最近の AI は本当すごいねLet's Dance!</a>
      <ul>
	<li><a href="#part1-community-oriented-life">（前座）令和時代のコミュニティ志向な生き方</a></li>
	<li><a href="#part1-portrait">顔の 3D 再構成</a></li>
	<li><a href="#part1-NeRF">3D 再構成、 NeRF 関係</a></li>
	<li><a href="#part1-photogrammetry">フォトグラメトリ、LiDAR 関係</a></li>
	<li><a href="#part1-text-to-image">Text to Image など</a></li>
	<li><a href="#part1-super-resolution">超解像など</a></li>
	<li><a href="#part1-demos">デモ</a></li>
	<li><a href="#part1-pose">姿勢推定</a></li>
	<li><a href="#part1-misc">その他</a></li>
      </ul>
    </li>

    <li><b>第２部</b>
      <a href="#part2">技術書典１３企画会議！</a>
      <ul>
	<li><a href="#part2-invitation">技術書典（イベント）へのお誘い</a></li>
	<li><a href="#part2-mission-statement">ZENKEI AI FORUM の Mission Statement</a></li>
	<li><a href="#part2-fandom-economy">同人誌活動は今のビジネスの最先端！</a></li>
	<li><a href="#part2-line-up">これまでのラインナップ</a></li>
        <li><a href="#part2-my-personal-project">いちきのソロ企画</a></li>
	<li><a href="#part2-as-a-circle">ZAF のサークル企画</a></li>
	<li><a href="#part2-lets-discuss">企画会議！</a></li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

</section>

</body>           
</html>
