<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8" />
    <title>ZENKEI AI FORUM (2021/11/24)</title>
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+1p:100,400,700&display=swap&subset=japanese" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../bright-M_PLUS_1p.css" />

    <link rel="stylesheet"
	  href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
  </head>

<body style="font-size: 20px;">

<header>
<center><h1>ZENKEI AI FORUM 2021/11/24</h1></center>
</header>

<article>

<section id="main">

  <center>
    <a href="ZENKEI_AI_FORUM_zoom_20211124-2488x1400.jpg"><!-- 2488x1400 -->
      <img src="ZENKEI_AI_FORUM_zoom_20211124-2488x1400_thumb.jpg" width="800" height="450" style="border: 2px #ccc solid;" /></a>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      ZAF ２０２１年１１月２４日
    </div>
    <div style="font-size: 40px;">＜今回のテーマ＞</div>
    <div style="font-size: 80px;">
      無題
    </div>
  </center>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <h2>目次</h2>
  <ul>
    <li><b>前座</b> [6:30 - 7:00]
      <ul>
	<li>技術書典１２</li>
	<li>ピアノ</li>
	<li>この１ヶ月の ZENKEI AI ポッドキャスト
	  （と ZENKEI AI FORUM SELECTIONS)</li>
      </ul>
    </li>

    <li><b>第１部</b> [7:00 - 8:00]
      <a href="#news1">最近の話題から、パート１</a>
      <ul>
	<li>文字通りの、最近の話題から</li>
      </ul>
    </li>

    <li><b>第２部</b> [8:00 - 9:00]
      <a href="#news2">最近の話題から、パート２</a>
      <ul>
	<li>copilot のレビュー、しようかな（できるかな？）</li>
      </ul>
    </li>

    <li><a href="#epilogue">今日のおわりに</a></li>

    <li><a href="#detailed-toc">総合目次</a></li>
  </ul>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="part0">
    <div style="font-size: 50px; font-weight: bolder;">
      前座
    </div>
  </center>

  <ul>
    <li>関係ないけど、暗号解読、好きですか？
      <ul>
	<li>ツイッター (<a href="https://twitter.com/WASEMARSHI/status/1460615164890206211">https://twitter.com/WASEMARSHI/status/1460615164890206211</a>) から
	  <br />
	  <a href="Screen Shot 2021-11-20 at 18.06.03.png"><!-- 2670x1517 -->
	    <img src="Screen Shot 2021-11-20 at 18.06.03_thumb.jpg" width="800" height="455" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>
  </ul>

  <h2 id="techbookfest12">技術書典１２</h2>

  <center>
    <a href="https://techbookfest.org/">
      <img src="Screen Shot 2021-11-24 at 15.52.10_thumb.jpg" width="800" height="386" style="border: 2px #ccc solid;" /></a>
    <br /><br />
    <a href="https://counting.hatelabo.jp/count/198345">
      <img src="Screen Shot 2021-11-24 at 15.54.37_thumb.jpg" width="600" height="386" style="border: 2px #ccc solid;" /></a>
  </center>

  <h3 id="techbookfest12-solo">自分のプロジェクト</h3>
  <ul>
    <li>(<a href="https://zenkei-ai-forum.github.io/pages/ZAF202104/ichiki/#techbookfest11">技術書典１１、自分の執筆プラン</a>)
      <ul>
	<li>今回は新刊出すぞっと</li>
	<li>これを出さないと、３部作の完結編（なのかな？３番目）に着手できない<br />
	  <a href="trilogy-plus-alpha.jpg"><!-- 4368x2945 -->
	    <img src="trilogy-plus-alpha_thumb.jpg" width="800" height="539" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>

    <li>１冊目（2020年9月）の『音楽と数理』から
      <ul>
	<li>Qiita: <a href="https://qiita.com/kichiki/items/6d14c7bfea7ef95f4e1a#%E4%BB%98%E9%8C%B2%EF%BC%92%E9%9F%B3%E6%A5%BD%E3%81%A8%E6%95%B0%E7%90%86%E3%81%A7%E5%8F%96%E3%82%8A%E4%B8%8A%E3%81%92%E3%81%9F%E7%B4%A0%E6%95%B5%E3%81%AA%E3%83%8F%E3%83%BC%E3%83%A2%E3%83%8B%E3%83%BC">【付録２】『音楽と数理』で取り上げた「素敵なハーモニー」</a>
	  <a href="https://qiita.com/kichiki/items/6d14c7bfea7ef95f4e1a#%E4%BB%98%E9%8C%B2%EF%BC%92%E9%9F%B3%E6%A5%BD%E3%81%A8%E6%95%B0%E7%90%86%E3%81%A7%E5%8F%96%E3%82%8A%E4%B8%8A%E3%81%92%E3%81%9F%E7%B4%A0%E6%95%B5%E3%81%AA%E3%83%8F%E3%83%BC%E3%83%A2%E3%83%8B%E3%83%BC">
	    <img src="Screen Shot 2021-11-24 at 15.05.03_thumb.jpg" width="800" height="623" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>

    <li>このときの ZAF で、ピアノ、ちょこっと弾きましたね<br />
      <a href="https://zenkei-ai-forum.github.io/youtube-selections.html#zaf-2009">
	<img src="Screen Shot 2021-11-24 at 14.42.42_thumb.jpg" width="800" height="381" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li>ZAP S09E01: <a href="https://zenkei.seesaa.net/article/483127966.html">（前座）『音楽と数理』でとりあげた「素敵なハーモニー」</a></li>
	<li>SELECTIONS: <a href="https://youtu.be/pdxIfm7jzpc?t=810">ZAF-2009-01</a>
	  <br />
	  <a href="https://youtu.be/pdxIfm7jzpc?t=810">
	    <img src="Screen Shot 2021-11-24 at 15.31.13_thumb.jpg" width="800" height="500" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>

    <li>LFE の該当号は、実は最終号でしたね<br />
      <a href="http://www2.southeastern.edu/orgs/34skid/html/26.pdf">http://www2.southeastern.edu/orgs/34skid/html/26.pdf</a>
      <br />
      <a href="LFE-26.pdf">
	<img src="Screen Shot 2021-11-24 at 15.40.20_thumb.jpg" width="504" height="600" style="border: 2px #ccc solid;" /></a>
    </li>

    <li>その後、どうなったか？
      <ul>
	<li>こんな感じ</li>
	<li>LFE の譜面に、全部書いてあった（というオチ）
	  <ul>
	    <li>もちろん、ヘッドの部分が採譜されてなくて、<br />
	      そこが一番弾きたい部分だったりした訳ですが</li>
	  </ul>
	</li>
	<li>手元にある情報（資料）を、きちんと勉強しましょう
	  （２０年以上の遠回り）</li>
      </ul>
    </li>
    <li>理論、よくわからん（けれど、理解したい）
      <ul>
	<li>詳しい人、教えて！</li>
	<li>「Maj 7 に半音上からアプローチする」ってこと？</li>
	<li>cf. My Foolish Heart のエンディング</li>
      </ul>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="techbookfest12-solo">自分のプロジェクト</h3>
  <ul>
    <li>(<a href="https://zenkei-ai-forum.github.io/pages/ZAF202104/ichiki/#techbookfest11">技術書典１１、自分の執筆プラン</a>)
      <ul>
	<li>今回は新刊出すぞっと</li>
	<li>これを出さないと、３部作の完結編（なのかな？３番目）に着手できない<br />
	  <a href="trilogy-plus-alpha.jpg"><!-- 4368x2945 -->
	    <img src="trilogy-plus-alpha_thumb.jpg" width="800" height="539" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>
    <li>とか言いながら、<br />
      数理シリーズ（という、自分の研究の落穂拾い）の
      もう１つのネタを思いついた（というか、思い出した）
      <ul>
	<li>ダイナミクスと統計、というか<br />
	  マクロと統計、というか<br />
	  その辺りのアイデア</li>
      </ul>
    </li>
    <li>ってことで、今の構想（構想ばかりですが）
      <ul>
	<li>（０）μ流体力学 - （技術書典１２予定）</li>
	<li>（１）音楽と数理 - （技術書典９ 既刊）</li>
	<li>（２）厳密な計算 - （技術書典１０ 既刊）</li>
	<li>（３）空間の近似</li>
	<li>（４）因果と統計（仮）</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="techbookfest12-group">ZAM 季報</h3>
  <ul>
    <li>思案中
      <ul>
	<li>サークル活動なので、このままの状態だと、季報は出ないかな、と</li>
	<li>（ぼくだけ執筆するのなら、それはソロ活動なので）</li>
      </ul>
    </li><!-- 思案中 -->
    <li>つまり、原稿を絶賛募集中！って意味です
      <ul>
	<li>「誰がタダ働きすると思ってるのですか？」<br />
	  という風に考える人も、いるかな？</li>
	<li>ZAM 季報は、サークルとして販売ってことになってます
	  <ul>
	    <li>つまり、執筆者に個別に原稿料なりロイヤリティは出ません</li>
	  </ul>
	</li>
	<li>なら、何の見返りがあるのか？
	  <ul>
	    <li>一人で書くのが大変だ、という人に、書いて、
	      出版する体験をしてもらうこと</li>
	    <li>それが継続して、単行本化した暁には
	      （普通のソロ活動として）ロイヤリティ出ます</li>
	  </ul>
	</li>
	<li>それじゃ弱い（寄稿者は来ない）、ということなのかなぁ…</li>
      </ul>
    </li><!-- つまり、原稿を絶賛募集中！って意味です -->

    <li>月刊 ZAM の刊行が滞っていますが、
      そちらはこれを機会にまとめておきたいとは思っています
      <br />
      <a href="Screen Shot 2021-11-24 at 14.22.04.png"><!-- 1679x1410 -->
	<img src="Screen Shot 2021-11-24 at 14.22.04_thumb.jpg" width="800" height="672" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li>少なくとも１年間は継続するつもりでいたので</li>
	<li>（それが、なかなか大変だったわけですが）</li>
      </ul>
    </li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h2 id="podcast">毎日更新、ポッドキャスト</h2>

  <ul>
    <li>先月の ZAF の時（2021年10月27日）は<br />
      ちょうどシーズン１５
      （ZAF 2021年3月）が全 15 エピソードで終わったところでした<br />
      <a href="https://zenkei-ai-forum.github.io/podcasts-list.html">
	<img src="Screen Shot 2021-11-23 at 14.03.02_thumb.jpg" width="800" height="422" style="border: 2px #ccc solid;" /></a>
    </li>
    <li>その後も毎日更新続いてます</li>

    <li>シーズン１６は ZAF 2021年4月の内容
      <br />
      <a href="https://zenkei.seesaa.net/article/484101182.html">
	<img src="ZAF-2104-01_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484101252.html">
	<img src="ZAF-2104-02_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484101376.html">
	<img src="ZAF-2104-03_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484101439.html">
	<img src="ZAF-2104-04_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484101512.html">
	<img src="ZAF-2104-05_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484101566.html">
	<img src="ZAF-2104-06_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134639.html">
	<img src="ZAF-2104-07_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134673.html">
	<img src="ZAF-2104-08_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134705.html">
	<img src="ZAF-2104-09_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134739.html">
	<img src="ZAF-2104-10_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134795.html">
	<img src="ZAF-2104-11_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134882.html">
	<img src="ZAF-2104-12_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484134942.html">
	<img src="ZAF-2104-13_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484135023.html">
	<img src="ZAF-2104-14_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268139.html">
	<img src="ZAF-2104-15_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268197.html">
	<img src="ZAF-2104-16_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268242.html">
	<img src="ZAF-2104-17_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268289.html">
	<img src="ZAF-2104-18_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268366.html">
	<img src="ZAF-2104-19_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484268403.html">
	<img src="ZAF-2104-20_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li><a href="https://zenkei.seesaa.net/article/484101182.html">S16E01 ゴールデンウィークも AI</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484101252.html">S16E02 ZAM ２月号できました！</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484101376.html">S16E03 コピー本の作り方</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484101439.html">S16E04 次号 ZAM ３月号の紹介</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484101512.html">S16E05 技術書典１１に向けて - ソロ活動</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484101566.html">S16E06 技術書典１１に向けて - グループ活動『ZAM 季報』爆誕</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134639.html">S16E07 『数理クイズ』解答編（１）githubアカウントとgithubページ</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134673.html">S16E08 『数理クイズ』解答編（２）出題内容の復習</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134705.html">S16E09 『数理クイズ』解答編（３）元ネタのQuantaの記事</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134739.html">S16E10 『数理クイズ』解答編（４）レーザー・メソッド？</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134795.html">S16E11 『数理クイズ』解答編（５）Strassen Algorithm</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134882.html">S16E12 『数理クイズ』解答編（６）デジャヴュ？</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484134942.html">S16E13 『数理クイズ』解答編（７）I ❤ Numerical Recipes</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484135023.html">S16E14 『数理クイズ』解答編（８）ディスカッション</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268139.html">S16E15 コンピュータ会話教室 第２回（１）Nedさんのビデオを解説</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268197.html">S16E16 コンピュータ会話教室 第２回（２）index は敵</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268242.html">S16E17 コンピュータ会話教室 第２回（３）python 流の for loop</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268289.html">S16E18 コンピュータ会話教室 第２回（４）多重ループからの break</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268366.html">S16E19 コンピュータ会話教室 第２回（５）Ned さんのビデオのまとめ</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484268403.html">S16E20 コンピュータ会話教室 第２回（６）DataLoader の使い方</a></li>
      </ul>
    </li>
    <li>ZENKEI AI FORUM SELECTIONS ZAF-2104
      <ul>
	<li>ZAF-2104-01 <a href="https://youtu.be/HXZ9d6TPY48">ゴールデンウィークも AI</a></li>
	<li>ZAF-2104-02 <a href="https://youtu.be/XuvrrgJ9_aQ">ZAM ２月号できました！</a></li>
	<li>ZAF-2104-03 <a href="https://youtu.be/lXMDJsYtIVc">コピー本の作り方</a></li>
	<li>ZAF-2104-04 <a href="https://youtu.be/zbGOXhk3W0g">次号 ZAM ３月号の紹介</a></li>
	<li>ZAF-2104-05 <a href="https://youtu.be/q2smF6fAvZw">技術書典１１に向けて - ソロ活動</a></li>
	<li>ZAF-2104-06 <a href="https://youtu.be/A3A5PtAnYcs">技術書典１１に向けて - グループ活動『ZAM 季報』爆誕</a></li>
	<li>ZAF-2104-07 <a href="https://youtu.be/0InOq-PfAmY">『数理クイズ』解答編（１）githubアカウントとgithubページ</a></li>
	<li>ZAF-2104-08 <a href="https://youtu.be/fwpf22kjVWo">『数理クイズ』解答編（２）出題内容の復習</a></li>
	<li>ZAF-2104-09 <a href="https://youtu.be/3FTBdZSzKF8">『数理クイズ』解答編（３）元ネタのQuantaの記事</a></li>
	<li>ZAF-2104-10 <a href="https://youtu.be/37rFzOOOfE8">『数理クイズ』解答編（４）レーザー・メソッド？</a></li>
	<li>ZAF-2104-11 <a href="https://youtu.be/9Ki_TnSPn_Y">『数理クイズ』解答編（５）Strassen Algorithm</a></li>
	<li>ZAF-2104-12 <a href="https://youtu.be/DtAGU3Msjfc">『数理クイズ』解答編（６）デジャヴュ？</a></li>
	<li>ZAF-2104-13 <a href="https://youtu.be/7oDmYuO9oDo">『数理クイズ』解答編（７）I ❤ Numerical Recipes</a></li>
	<li>ZAF-2104-14 <a href="https://youtu.be/Dfe_Tkw_7-g">『数理クイズ』解答編（８）ディスカッション</a></li>
	<li>ZAF-2104-15 <a href="https://youtu.be/1jc1IjLplCg">コンピュータ会話教室 第２回（１）Nedさんのビデオを解説</a></li>
	<li>ZAF-2104-16 <a href="https://youtu.be/mtbhUFxcuEQ">コンピュータ会話教室 第２回（２）index は敵</a></li>
	<li>ZAF-2104-17 <a href="https://youtu.be/-6-Kog2YvwE">コンピュータ会話教室 第２回（３）python 流の for loop</a></li>
	<li>ZAF-2104-18 <a href="https://youtu.be/LESeG7R8KyI">コンピュータ会話教室 第２回（４）多重ループからの break</a></li>
	<li>ZAF-2104-19 <a href="https://youtu.be/N8vGmuhm4SY">コンピュータ会話教室 第２回（５）Ned さんのビデオのまとめ</a></li>
	<li>ZAF-2104-20 <a href="https://youtu.be/fza4OPowPK0">コンピュータ会話教室 第２回（６）DataLoader の使い方</a></li>

      </ul>
    </li>
    <li>現在シーズン１７ ZAF 2021年5月を配信中<br />
      <a href="https://zenkei.seesaa.net/article/484376090.html">
	<img src="ZAF-2105-01_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484376130.html">
	<img src="ZAF-2105-02_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484376218.html">
	<img src="ZAF-2105-03_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484447640.html">
	<img src="ZAF-2105-04_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484447740.html">
	<img src="ZAF-2105-05_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484447984.html">
	<img src="ZAF-2105-06_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <a href="https://zenkei.seesaa.net/article/484448080.html">
	<img src="ZAF-2105-07_thumb.jpg" width="400" height="225" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li><a href="https://zenkei.seesaa.net/article/484376090.html">S17E01 ZAF2021年５月のイントロ</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484376130.html">S17E02 ZAM３月号できました！</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484376218.html">S17E03 コピー本、再び（失敗）</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484447640.html">S17E04 アイリス VS ペンギン（古川郁衣さん）</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484447740.html">S17E05 Rを使ってみた（古川郁衣さん）</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484447984.html">S17E06 アイリス VS ペンギン - 質疑応答（１）（古川郁衣さん）</a></li>
	<li><a href="https://zenkei.seesaa.net/article/484448080.html">S17E07 アイリス VS ペンギン - 質疑応答（２）（古川郁衣さん）</a></li>
      </ul>
    </li>
    <li>ZENKEI AI FORUM SELECTIONS ZAF-2105
      <ul>
	<li>ZAF-2105-01 <a href="https://youtu.be/xiFj6elQm2A">ZAF2021年５月のイントロ</a></li>
	<li>ZAF-2105-02 <a href="https://youtu.be/arlTSEl4ov0">ZAM３月号できました！</a></li>
	<li>ZAF-2105-03 <a href="https://youtu.be/ZT4w2qIQV-E">コピー本、再び（失敗）</a></li>
	<li>ZAF-2105-04 <a href="https://youtu.be/NdOKAkhx_1E">アイリス VS ペンギン（古川郁衣さん）</a></li>
	<li>ZAF-2105-05 <a href="https://youtu.be/JRWUqWFUTNk">Rを使ってみた（古川郁衣さん）</a></li>
	<li>ZAF-2105-06 <a href="https://youtu.be/QlMgfZC4boc">アイリス VS ペンギン - 質疑応答（１）（古川郁衣さん）</a></li>
	<li>ZAF-2105-07 <a href="https://youtu.be/wopo_p58O84">アイリス VS ペンギン - 質疑応答（２）（古川郁衣さん）</a></li>
      </ul>
    </li>
  </ul>

  <center>
    <div style="font-size: 60px; font-weight: bold;">
      祝！１００エピソード！！
    </div>
  </center>
  <ul>
    <li>実は、今回祈念すべき１００エピソードを達成しました！<br />
      <center>
	<a href="https://zenkei.seesaa.net/article/484134673.html">
	  <img src="Screen Shot 2021-11-24 at 15.59.47_thumb.jpg" width="600" height="319" style="border: 2px #ccc solid;" /></a>
	<a href="https://zenkei.seesaa.net/article/484134673.html">
	  <img src="Screen Shot 2021-11-24 at 15.59.38_thumb.jpg" width="600" height="319" style="border: 2px #ccc solid;" /></a>
      </center>
    </li>
  </ul>
    
  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center>
    <div id="news1">
    <div style="font-size: 60px; font-weight: bold;">
      最近の話題から、パート１
    </div>
    </div>
  </center>

  <h3>目次</h3>
  <ul>
    <li><a href="#whats-new-history">最近の話題からの歴史</a></li>
    <li><a href="#convmixer">ConvMixer</a></li>
    <li><a href="#vit">ViT 関連</a></li>
    <li><a href="#audio">音関係</a></li>
    <li><a href="#transformers">Transformer 関連</a></li>
    <li><a href="#nlp">NLP</a></li>
    <li><a href="#stylegan">StyleGAN3</a></li>
    <li><a href="#images">画像系、そのほか</a></li>
    <li><a href="#self-supervised">Self-Supervised</a></li>
    <li><a href="#misc">そのほか</a></li>
  </ul>

  <h3 id="whats-new-history">最近の話題からの歴史</h3>
  <ul>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202108/ichiki/#whats-new">ZAF 2021年8月</a>
      最近の話題から 2021年夏</li>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202106/ichiki/#ichiki">ZAF 2021年6月</a>
      GPT-3 で遊んでみた！</li>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202102/ichiki/#whatsnew">ZAF 2021年２月</a>
      画像分類でSOTA更新した NFNets</li>
    <li>ZAF 2020年11月 画像分類の新手法 ViT と BYOL</li>
    <li>ZAF 2020年7月 GPT-3 と StyleGAN2</li>
    <li>ZAF 2020年1月 2020年版　最近の話題から</li>
    <li>ZAF 2019年9月 最近の話題から</li>
  </ul>


  <h3 id="convmixer">ConvMixer</h3>
  <ul>
    <li>"Patches are all you need" 論文<br />
      <table border="0">
	<tr>
	  <td valign="top">
      <a href="Screen Shot 2021-11-24 at 16.04.58.png"><!-- 1615x1081 -->
	<img src="Screen Shot 2021-11-24 at 16.04.58_thumb.jpg" width="600" height="402" style="border: 2px #ccc solid;" /></a>
	  </td>
	  <td valign="top">
      <a href="Screen Shot 2021-11-24 at 16.05.07.png"><!-- 1622x829 -->
	<img src="Screen Shot 2021-11-24 at 16.05.07_thumb.jpg" width="600" height="307" style="border: 2px #ccc solid;" /></a>
	  </td>
	</tr>
      </table>
    </li>
    <li><a href="https://twitter.com/cHHillee/status/1445888512918704131">https://twitter.com/cHHillee/status/1445888512918704131</a>
      <pre>
	<a href="https://openreview.net/forum?id=TVHS5Y4dNvM">openreview.net/forum?id=TVHS5Y4dNvM</a>
	A very ... interesting 4 page paper at ICLR. I’m curious to see the reviewers’ reactions.

	8:07 AM · Oct 7, 2021·Twitter Web App
      </pre>
    </li>
    <li>PDF: <a href="patches_are_all_you_need_.pdf">patches_are_all_you_need_.pdf</a>
      <ul>
	<li>2021/10/7 にダウンロードしたもの：<br />
	  <a href="patches_are_all_you_need-for-ICLR2022-ConvMixer.pdf">patches_are_all_you_need-for-ICLR2022-ConvMixer.pdf</a></li>
      </ul>
    </li>

    <li><a href="https://twitter.com/icoxfog417/status/1446072881029341185">icoxfog417/status/1446072881029341185</a>
      <pre>
	画像にTransformerを適用した時の精度が、
	モデル構造ではなく入力をパッチにすることに起因するとした研究。
	カーネルサイズ9、ストライド7というかなり大きい値でパッチ入力を模倣し、
	Depthwise/Pointwiseでパッチ間のMixを行うConvMixerを提案。
	同パラメータ数でViTより高精度。
      </pre>
      8:20 PM · Oct 7, 2021
    </li>

    <li><a href="https://twitter.com/yu4u/status/1445986165773897729">yu4u/status/1445986165773897729</a>
      <pre>
	文字数SOTA競争が始まっとるｗｗｗ
      </pre>
      2:35 PM · Oct 7, 2021
    </li>

    <li><a href="https://twitter.com/karpathy/status/1445915220644229124">karpathy/status/1445915220644229124</a>
      <pre>
	Errr ok wow, I am shook by the new ConvMixer architecture
	https://openreview.net/forum?id=TVHS5Y4dNvM
	"the first model that achieves the elusive dual goals of
	80%+ ImageNet top-1 accuracy while also fitting into a tweet"
	Neutral face
      </pre>
      9:53 AM · Oct 7, 2021
    </li>

    <li><a href="https://twitter.com/icoxfog417/status/1451301927006863362">https://twitter.com/icoxfog417/status/1451301927006863362</a>
      <pre>
	Transformer/MLP-MixerのコンセプトをCNNで再現した
	ConvMixer(Patches Are All You Need)をKerasで実装するチュートリアルが公開。
	CIFAR-10でなかなかの精度が出ているよう。
	実装は数十行といったところ。可視化の実装も解説されている。

	6:38 AM · Oct 22, 2021·Twitter Web App
      </pre>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="vit">ViT 関連</h3>

  <ul>
    <li><a href="https://twitter.com/cihangxie/status/1458627156582432771">https://twitter.com/cihangxie/status/1458627156582432771</a>
      <pre>
	Thanks for tweeting, 
	@ak92501
	! Regarding robustness, we surprisingly find that

	1) ViTs are NO MORE robust than CNNs on adversarial examples;
	training recipes matter

	2) ViTs LARGELY outperform CNNs on out-of-distribution samples;
	self-attention-like architectures matter

	11:46 AM · Nov 11, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/AkiraTOSEI/status/1460745999404597254">https://twitter.com/AkiraTOSEI/status/1460745999404597254</a>
      <pre>
	https://arxiv.org/abs/2106.14881
	ViTはCNNに比べると学習の安定性が低いが、その原因が初期層のパッチ化にあると主張。
	最初の16x16パッチ化を3x3 Convなどを組み合わせた通常のConvに代替することにより、
	学習率の変動に頑健になり、収束も早く、CNNのSotAモデルを凌駕する。

	8:06 AM · Nov 17, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/ak92501/status/1462973854624755720">https://twitter.com/ak92501/status/1462973854624755720</a>
      <pre>
	Semi-Supervised Vision Transformers
	abs: https://arxiv.org/abs/2111.11067

	The final fused framework achieves 75.5% top-1 accuracy on ImageNet
	and outperforms the sota in semi-supervised image classification

	11:38 AM · Nov 23, 2021·Twitter Web App
      </pre>
    </li>

    <li>MobileViT
      <ul>
	<li><a href="https://twitter.com/yu4u/status/1445694510642515975">yu4u/status/1445694510642515975</a>
	  <pre>
	    おっと思って読んでみた。
	    基本精度vs.パラメータ数で比較してて、最後に実測遅いっすというオチ。
	    パラメータ数同じでも入力画像サイズ（ViTなら系列長）で推論速度大きく変わるし、
	    せめてFLOPsを…モバイル実測遅いって書いてあるけど、
	    GPUでの速度比較もないので、GPUでも遅いんじゃねとか思ったり
	  </pre>
	  7:16 PM · Oct 6, 2021
	</li>

	<li><a href="https://twitter.com/ak92501/status/1445554785952800781">https://twitter.com/ak92501/status/1445554785952800781</a>
	  <pre>
	    MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer
	    pdf: https://arxiv.org/pdf/2110.02178.pdf
	    abs: https://arxiv.org/abs/2110.02178

	    10:01 AM · Oct 6, 2021·Twitter Web App
	  </pre>
	</li>
	<li><a href="https://twitter.com/RisingSayak/status/1452104025831182338">https://twitter.com/RisingSayak/status/1452104025831182338</a>
	  <pre>
	    Combining the benefits of convs and transformers is an emerging trend in computer vision research.
	    MobileViT presents a simple yet unique way to achieve that
	    while being mobile-friendly. 

	    In my latest example, I present a minimal implementation in #Keras
	    Downwards arrow

	    11:45 AM · Oct 24, 2021·Twitter Web App
	  </pre>
	</li>

      </ul>
    </li><!-- MobileViT -->

    <li>CoAtNet
      <ul>
	<li><a href="https://twitter.com/omiita_atiimo/status/1460378563694133250">https://twitter.com/omiita_atiimo/status/1460378563694133250</a>
	  <pre>
	    畳み込み+Attention=最強！？Googleの最新画像認識モデルCoAtNet(コートネット)を解説しました！

	    https://qiita.com/omiita/items/b97e68e1bbfdfa71ba79

	    CoAtNetは、CNNとViTをうまく組み合わせることで、
	    ImageNetで新たなSoTA(90.88%)を叩き出しました。
	    CNNとAttentionの式から丁寧に解説していますので、ぜひご覧ください！

	    7:45 AM · Nov 16, 2021·Twitter Web App
	  </pre>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="audio">音関係</h3>
  <ul>
    <li><a href="https://twitter.com/yuma_koizumi/status/1450938680059645953">https://twitter.com/yuma_koizumi/status/1450938680059645953</a>
      <pre>
	Our DF-Conformer paper has received the “Best Speech Enhancement Paper Award”
	from #WASPAA2021! Yay!!

	6:35 AM · Oct 21, 2021·Twitter for iPhone
      </pre>
    </li>

    <li><a href="https://twitter.com/shinmura0/status/1452549686162644996">https://twitter.com/shinmura0/status/1452549686162644996</a>
      <pre>
	これは強い。
	ASTは、来年の音コンペ（Kaggle鳥3、DCASE2022）でスタンダードになると予想。

	今まで音コンペといえば、
	PANNs + EfficientNet (RexNet)が主流だったけど、
	これからは、ASTに統一されるかも。

	（続く）

	5:16 PM · Oct 25, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/shinmura0/status/1455375340977594368">https://twitter.com/shinmura0/status/1455375340977594368</a>
      <pre>
	環境音認識TransformerのASTを超えるモデルが早くも登場。
	pretrained-modelも公開されている模様

	paper:https://arxiv.org/abs/2110.05069
	github:https://github.com/kkoutini/PaSST

	12:24 PM · Nov 2, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/naotokui/status/1461149813441921024">https://twitter.com/naotokui/status/1461149813441921024</a>
      <pre>
	ビデオに合わせて音楽を生成する研究。
	ビデオのテンポや動きの激しさでTransformerベースの音楽生成モデルを条件づけてます。

	映像に合わせてループ素材を組み合わせるタイプのサービスはありますが、
	MIDIレベルで生成する研究は初めて見ました。　面白い！

	https://wzk1015.github.io/cmt/

	10:50 AM · Nov 18, 2021·Twitter for iPhone
      </pre>
    </li>

    <li><a href="https://twitter.com/ak92501/status/1462608261006479361">https://twitter.com/ak92501/status/1462608261006479361</a>
      <pre>
	More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech
	abs: https://arxiv.org/abs/2111.10139

	11:25 AM · Nov 22, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/naotokui/status/1455408919531098115">https://twitter.com/naotokui/status/1455408919531098115</a>
      <pre>
	PyTorchの自作モデルをオープンソースのオーディオ編集ソフトAudacity内で使える仕組み。
	モデルをHugging FaceのModel Hubにアップロードするだけ。音源分離、音声認識、楽器のラベリングなどのモデルが公開済み。素晴らしい!!

	Deep Learning Tools for Audacity https://buff.ly/3w7Kwii

	2:38 PM · Nov 2, 2021·Buffer
      </pre>
      <ul>
	<li><a href="https://twitter.com/ak92501/status/1453174460702461957">https://twitter.com/ak92501/status/1453174460702461957</a>
	  <pre>
	    Deep Learning Tools for Audacity: Helping Researchers Expand the Artist’s Toolkit
	    abs: https://arxiv.org/abs/2110.13323
	    project page: https://interactiveaudiolab.github.io/project/audacity

	    10:39 AM · Oct 27, 2021·Twitter Web App
	  </pre>
	</li>
      </ul>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="transformers">Transformer 関連</h3>
  <ul>
    <li><a href="https://twitter.com/hillbig/status/1447677922106503177">hillbig/status/1447677922106503177</a>
      <pre>
	EnformerはDNA配列からの遺伝発現予測やクロマチン状態予測タスクに対し、
	自己注意機構を使うことで長距離情報（従来は20 kbを100 kb）を
	考慮できるようにし精度を大幅に改善。
	エンハンサー-プロモーター間の相互作用も実験結果を入力使った手法に匹敵する性能を達成
      </pre>
      6:38 AM · Oct 12, 2021
    </li>

    <li><a href="https://twitter.com/facebookai/status/1448306144171024384">facebookai/status/1448306144171024384</a>
      12:14 AM · Oct 14, 2021
      <pre>
	Ever wondered if you already added an ingredient when cooking a meal?
	Or what to attach next when assembling a dresser?
	Facebook AI has built Anticipative Video Transformer,
	a model that can understand sequences of events & suggest the next step.
	Details:
	https://ai.facebook.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video

	12:14 AM · Oct 14, 2021·Twitter Web App
      </pre>
      <ul>
	<li><a href="https://twitter.com/Maxwell_110/status/1460365743623712771">https://twitter.com/Maxwell_110/status/1460365743623712771</a>
	  <pre>
	    Anticipative Video Transformer（AVT）Memo

	    https://arxiv.org/abs/2106.02036

	    AVT は各ビデオフレームを ViT で encode した後，Causal Transformer（CT）で decode し，将来の動きを事前予測する（Fig. 3）

	    その際，ViT は各フレームの重要な空間に，CT は重要なフレームに着目している

	    6:55 AM · Nov 16, 2021·TweetDeck
	  </pre>
	</li>

      </ul>
    </li>

    <li>NormFormer
      <ul>
	<li><a href="https://twitter.com/Maxwell_110/status/1455292313656340484">https://twitter.com/Maxwell_110/status/1455292313656340484</a>
	  <pre>
	    NormFormer Memo

	    https://arxiv.org/abs/2110.09456

	    FAIR の研究 Right-pointing magnifying glass

	    Transformer は学習時，前・後側層間で勾配の大きさが異なるという mismatch が存在する

	    そこで
	    - Attention Head（AH）後の Layer Norm（LN）
	    - AH の出力を head 別に scaling
	    - fc 後の LN
	    を加え（Fig. 1），perplexity 等を改善した

	    6:55 AM · Nov 2, 2021·TweetDeck
	  </pre>
	</li>

	<li><a href="https://twitter.com/sam_shleifer/status/1450813564894289923">https://twitter.com/sam_shleifer/status/1450813564894289923</a>
	  <pre>
	    Excited to release NormFormer, our new Language Modeling architecture that outperforms GPT3 at every scale we tried (up to 2.7B params) with 
	    @jaseweston
	    
	    @myleott
	    [1/N] https://arxiv.org/pdf/2110.09456.pdf

	    10:18 PM · Oct 20, 2021·Twitter Web App
	  </pre>
	</li>

	<li><a href="https://twitter.com/hillbig/status/1453111082826932234">https://twitter.com/hillbig/status/1453111082826932234</a>
	  <pre>
	    NormFormerはTransformerのヘッド毎に学習可能な係数を追加かつ、MHA後とFFNの途中にも層正規化（LN）を追加することで、学習を安定化させ、学習効率や性能が改善される

	    6:27 AM · Oct 27, 2021·Twitter Web App
	  </pre>
	</li>
      </ul>
    </li><!-- NormFormer -->

    <li>MAE (Masked AutoEncoders)
      <ul>
	<li><a href="https://arxiv.org/abs/2111.06377">arxiv:2111.06377</a>
	  (<a href="arxiv-2111.06377-MAE.pdf">local copy</a>)
	  <br />
	  Masked Autoencoders Are Scalable Vision Learners
	</li>

	<li><a href="https://twitter.com/karpathy/status/1459637813448564736">https://twitter.com/karpathy/status/1459637813448564736</a>
	  <pre>
	    Great paper and thread!
	    - that super simple MSE loss works vs. BEiT-style dVAE (multi-modal) cross-entropy
	    - <3 efficiency of asymmetric encoder/decoder
		 - detailed training recipes
		 - +1 v curious about dataset size scaling
		 - bit of lack of commentary on test-time protocol

		 6:42 AM · Nov 14, 2021·Twitter Web App
	  </pre>
	</li>
	<li><a href="https://twitter.com/giffmana/status/1459092079020285976">https://twitter.com/giffmana/status/1459092079020285976</a>
	  <pre>
	    1/N The return of patch-based self-supervision!
	    It never worked well and you had to bend over backwards with ResNets (I tried).
	    Now with ViT, very simple patch-based self-supervised pre-training rocks!
	    First BeIT, now Masked AutoEncoders i1k=87.8%
	    https://arxiv.org/pdf/2111.06377.pdf

	    6:33 PM · Nov 12, 2021·Twitter Web App
	  </pre>
	</li>
	<li><a href="https://twitter.com/Maxwell_110/status/1462540070930432002">https://twitter.com/Maxwell_110/status/1462540070930432002</a>
	  <pre>
	    Masked AutoEncoders (MAE) Memo

	    https://arxiv.org/abs/2111.06377

	    FAIR の研究で，MAE は Encoder に ViT を採用した AutoEncoder（Fig. 1）

	    Non-masked パッチのみを encode し，decoder を通して masked パッチを修復するが，
	    多くを mask しても良好な修復が可能で，巨大モデルの事前学習に最適と報告

	    6:55 AM · Nov 22, 2021·TweetDeck
	  </pre>
	</li>
      </ul>
    </li><!-- MAE (Masked AutoEncoders) -->


    <li>SWIN (Shifted Windows)
      <ul>
	<li><a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a>
	</li>
	<li><a href="https://arxiv.org/abs/2103.14030">arxiv:2103.14030</a>
	  (<a href="arxiv-2103.14030-Swin.pdf">local copy</a>)
	  <br />
	  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
	</li>
	<li><a href="https://arxiv.org/abs/2111.09883">arxiv:2111.09883</a>
	  (<a href="arxiv-2111.09883_Swin_v2.pdf">local copy</a>)
	  <br />
	  Swin Transformer V2: Scaling Up Capacity and Resolution
	</li>

	<li>日本語の解説<br />
	  <a href="https://kyla.co.jp/blog/2021/05/10/%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B%E3%80%8Eswin-transformer-hierarchical-vision-transformer-using-shifted-windows%E3%80%8F/">論文紹介『Swin Transformer: Hierarchical Vision Transformer using Shifted Windows』 2021年5月10日2021年5月12日</a>
	</li>
      </ul>
    </li>


    <li><a href="https://twitter.com/giffmana/status/1461249563466022913">https://twitter.com/giffmana/status/1461249563466022913</a>
      <pre>
	It’s about time: analog clock reading in the wild
	https://arxiv.org/abs/2111.09162

	A great example of an applied vision paper,
	let me walk you through why I like it. Yarn

	They also make good use of Spatial Transformer Networks (STN)
	one of the most elegant ideas that usually don’t work :)

	5:26 PM · Nov 18, 2021·Twitter Web App
      </pre>
    </li>


    <li><a href="https://twitter.com/ak92501/status/1462598287916208129">https://twitter.com/ak92501/status/1462598287916208129</a>
      <pre>
	UFO: A UniFied TransfOrmer for Vision-Language Representation Learning
	abs: https://arxiv.org/abs/2111.10023

	10:46 AM · Nov 22, 2021·Twitter Web App
      </pre>
    </li>


    <li>レビュー
      <ul>
	<li><a href="https://twitter.com/naoism00/status/1447774496639754246">naoism00/status/1447774496639754246</a>
	  <pre>
	    最近発表された画像認識系TransformerとMLP-mixerに関する
	    各論文の工夫と結果が簡単に整理されてて良い。
	    けどこう見るとかなり多いな。
	  </pre>
	  1:01 PM · Oct 12, 2021
	</li>
      </ul>
    </li>


    <li>Position Encoding
      <ul>
	<li><a href="https://twitter.com/icoxfog417/status/1444479600365502471">icoxfog417/status/1444479600365502471</a>
	  <pre>
	    Transformer系モデルで使用される位置ベクトルの種類と効果を検証した研究。
	    位置ベクトルに単調性(位置が離れるほど小さい)、
	    移動普遍性(位置移動量と値が連動する)がある場合タスク精度に貢献があり、
	    対称性(入力順に対し値が不変)は逆効果になる。
	    絶対位置は分類、相対位置はスパン予測に強い傾向。
	  </pre>
	  10:49 AM · Oct 3, 2021
	</li>
	<li><a href="https://twitter.com/yu4u/status/1458071612142473224">https://twitter.com/yu4u/status/1458071612142473224</a>
	  <pre>
	    おお、ありがとうございます！わかりやすいですね！
	    （新しいPEの提案もcontributionに入ってるとそれの評価はちょっと割り引いて考えたくなる）

	    10:58 PM · Nov 9, 2021·Twitter Web App
	  </pre>
	</li>

	<li><a href="https://twitter.com/sasaki_ts/status/1458073739812216836">https://twitter.com/sasaki_ts/status/1458073739812216836</a>
	  <pre>
	    こちらRPEについて気持ち網羅的に纏まってます。https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Rethinking_and_Improving_Relative_Position_Encoding_for_Vision_Transformer_ICCV_2021_paper.pdf

	    11:07 PM · Nov 9, 2021·Twitter Web App
	  </pre>
	</li>
      </ul>
    </li><!-- Position Encoding -->



    <li><a href="https://twitter.com/Maxwell_110/status/1444783065344188428">Maxwell_110/status/1444783065344188428</a>
      <pre>
	Fine-tuning 後の Transformer における各層間の表現類似性を検証 Memo

	https://arxiv.org/abs/2109.08406

	ブロック対角構造がみられ，特に前側層と後側層の各々で類似性が存在すると報告（Fig. 1 - c）

	また，後側層のモデル精度への寄与は少なく，特に top の数層は削除可能（軽量化可能）としている
      </pre>
      6:55 AM · Oct 4, 2021
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1446232616873639940">Maxwell_110/status/1446232616873639940</a>
      <pre>
	Block Pruning For Faster Transformers Memo

	https://arxiv.org/abs/2109.04838

	Hugging face による Transformer の枝刈りの研究

	Transformer の重み行列を複数ブロックに分割し，
	「各ブロックを枝刈り（マスク）するかどうか」を，
	loss にマスク量に対応した正則化項を導入することで学習（知識蒸留も併用）
      </pre>
      6:55 AM · Oct 8, 2021
    </li>


    <li><a href="https://twitter.com/hillbig/status/1452418171495845893">https://twitter.com/hillbig/status/1452418171495845893</a>
      <pre>
	Transformerの計算量を入力に対し線形化するため、
	注意計算をカーネルとみなし乱択フーリエ特徴または正乱択特徴（Performerで導入）を適用、
	その周波数成分の確率分布はGMMかGANで学習するかFastFoodでモデル化して学習、
	つまりカーネルを学習する。精度、メモリ使用量で改善

	8:34 AM · Oct 25, 2021·Twitter Web App
      </pre>
    </li>

    <li>TSLF (Three-Stage Learning Framework)<br />
      <a href="https://twitter.com/ELYZA_inc/status/1452568113501798402">https://twitter.com/ELYZA_inc/status/1452568113501798402</a>
      <pre>
	知識に基づく対話に特化したknowledge-aware Transformerを提案。
	知識と対話用のパラメータに分け
	1)Wikipediaと対話コーパスでそれぞれ学習
	2)両方を使い作成した疑似データセットで合同で学習
	3)fine-tuningという3段階で学習。
	少量の教師ありデータで他の手法を上回った。
	http://arxiv.org/abs/2109.04096

	6:30 PM · Oct 25, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/icoxfog417/status/1453587425045856260">https://twitter.com/icoxfog417/status/1453587425045856260</a>
      <pre>
	破壊的忘却なしに事前学習済みモデルの「編集」を行う研究。
	全結合のシンプルなネットワークで、転移学習した場合の重みを
	元のパラメータと大きく離れないように編集して適用する。
	ただ、勾配の次元は大きいためパラメータを直積の形式に分解して計算する。
	T5やGPTといった億単位級モデルで効果を確認

	2:00 PM · Oct 28, 2021·Twitter for iPhone
      </pre>
    </li>

    <li><a href="https://twitter.com/ak92501/status/1462965826726805511">https://twitter.com/ak92501/status/1462965826726805511</a>
      <pre>
	ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning
	abs: https://arxiv.org/abs/2111.10952

	EXT5 outperforms strong T5 baselines on SuperGLUE,
	GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of EXMIX

	11:06 AM · Nov 23, 2021·Twitter Web App
      </pre>
    </li>

  </ul>


  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="nlp">NLP</h3>
  <ul>
    <li>FLAN<br />
      Google AI Blog: <a href="https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html">Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning (Wednesday, October 6, 2021)</a>
      <br />
      <a href="Screen Shot 2021-11-24 at 16.45.02.png"><!-- 1809x1278 -->
	<img src="Screen Shot 2021-11-24 at 16.45.02_thumb.jpg" width="600" height="424" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li><a href="https://twitter.com/mr_bay_area/status/1446105869893058568">mr_bay_area/status/1446105869893058568</a>
	  <pre>
	    GPT-3よりすごい何かが出ていて、
	    もうNLPやるよりクソして寝てた方がいいんじゃないかと思い始めてる☺
	  </pre>
	  10:31 PM · Oct 7, 2021
	</li>

      </ul>
    </li><!-- FLAN -->

    <li><a href="https://twitter.com/Maxwell_110/status/1445507841125756928">Maxwell_110/status/1445507841125756928</a>
      <pre>
	FinBERT Package

	https://github.com/ProsusAI/finBERT

	金融テキストのセンチメント分類（positive・neutral・negative）が可能な BERT 系モデル

	ロイターのコーパスで再事前学習し，
	FinancialPhraseBank（センチメントデータ）で下層の学習率を抑えつつ
	Fine-tuning している

	Paper ➡︎ https://arxiv.org/abs/1908.10063
      </pre>
      6:55 AM · Oct 6, 2021
    </li>

    <li><a href="https://twitter.com/ELYZA_inc/status/1458728708039188480">https://twitter.com/ELYZA_inc/status/1458728708039188480</a>
      <pre>
	対話での情報検索において few-shot 性能の高い手法を提案。
	現在までのクエリの embedding と 文書の embedding の内積で
	関連度を測るシンプルで効率的なモデルで，
	manual oracle query（対話履歴を考慮したクエリ）で学習した
	retriever の表現を蒸留する。既存手法を上回る
	http://arxiv.org/abs/2105.04166

	6:30 PM · Nov 11, 2021·Twitter Web App
      </pre>
    </li>


    <li><a href="https://twitter.com/huggingface/status/1455916487579930629">https://twitter.com/huggingface/status/1455916487579930629</a>
      <pre>
	Part 1 of the course focused on text classification,
	part 2 will focus on all other common NLP tasks. 
	@mervenoyann
	has made videos to introduce you to each of them!
	Let’s start with Token Classification
	(giving a label to some/each word in a sentence):

	12:15 AM · Nov 4, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/ELYZA_inc/status/1462717470653902849">https://twitter.com/ELYZA_inc/status/1462717470653902849</a>
      <pre>
	MassiveSumm: a very large-scale, very multilingual, news summarisation dataset
	92言語・370のニュースサイトから1200万件以上のニュース要約データを収集した論文。
	日本語も8.7万件含まれている。

	https://aclanthology.org/2021.emnlp-main.797/

	6:39 PM · Nov 22, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/stateofai_ja/status/1459403640565760002">https://twitter.com/stateofai_ja/status/1459403640565760002</a>
      <pre>
	状態空間モデルに基づき、連続時系列・RNN・CNN的な計算をモデル化できる
	汎用系列モデル S4、超長距離系列ベンチマーク LRA で SOTA を独占。

	これまでどんなモデルでも解けなかった「Path-Xベンチマーク」を
	88% の精度で見事に解き、界隈にちょっとした衝撃が走っている

	https://arxiv.org/abs/2111.00396

	3:11 PM · Nov 13, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/hillbig/status/1449871378170068996">https://twitter.com/hillbig/status/1449871378170068996</a>
      <pre>
	言語モデル（GPT-3）のみを使って教師無し機械翻訳のSOTA性能を達成。
	1) 指示によるZero-shotで翻訳文を生成
	2) 生成された翻訳対でプロンプトを作り翻訳文を生成
	3) 生成された翻訳対でfine tuning
	4) できたモデルで翻訳を作り、逆向きもfine tuning（back translation）

	7:54 AM · Oct 18, 2021·Twitter Web App
      </pre>
      <ul>
	<li><a href="https://arxiv.org/abs/2110.05448">arxiv:2110.05448</a>
	  (<a href="arxiv-2110.05448-unsupervised-nmt.pdf">local copy</a>)
	  <br />
	  Unsupervised Neural Machine Translation with Generative Language Models Only
	</li>
      </ul>
    </li>

    <li><a href="https://twitter.com/DeepMind/status/1448304230192844802">https://twitter.com/DeepMind/status/1448304230192844802</a>
      <pre>
	Language models (LMs) can generate toxic language,
	including offensive or hateful text.
	In this work, our team evaluates methods to reduce LM toxicity,
	discusses limits of automatic eval,
	and analyses consequences for LM quality and social bias:
	http://dpmd.ai/detoxifying 1/

	12:06 AM · Oct 14, 2021·Twitter Web App
      </pre>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="stylegan">StyleGAN3</h3>
  <ul>
    <li><a href="https://arxiv.org/abs/2106.12423v4">arxiv:2106.12423</a>
      (<a href="arxiv-2106.12423v4-stylegan3.pdf">local copy</a>)
      <br />
      Alias-Free Generative Adversarial Networks
    </li>

    <li><a href="https://twitter.com/RiversHaveWings/status/1447697339385143299">RiversHaveWings/status/1447697339385143299</a>
      <pre>
	This is just wild. #StyleGAN3
      </pre>
      7:55 AM · Oct 12, 2021
      <br />
      <a href="https://twitter.com/RiversHaveWings/status/1447697339385143299">
	<img src="Screen Shot 2021-11-24 at 16.51.55_thumb.jpg" width="362" height="400" style="border: 2px #ccc solid;" /></a>
    </li>

    <li><a href="https://twitter.com/paperswithcode/status/1447912427060858882">paperswithcode/status/1447912427060858882</a>
      <pre>
	StyleGAN3 is out and results are Exploding head!

	It proposes architectural changes that suppress aliasing
	and forces the model to implement more natural hierarchical refinement
	which improves its ability to generate video and animation.

	<a href="https://paperswithcode.com/paper/alias-free-generative-adversarial-networks">https://paperswithcode.com/paper/alias-free-generative-adversarial-networks</a>
      </pre>
      10:09 PM · Oct 12, 2021
      <br />
      <a href="Screen Shot 2021-11-24 at 17.07.50.png"><!-- 1191x1316 -->
	<img src="Screen Shot 2021-11-24 at 17.07.50_thumb.jpg" width="362" height="400" style="border: 2px #ccc solid;" /></a>
    </li>

    <li><a href="https://twitter.com/nshepperd1/status/1448101894103121929">nshepperd1/status/1448101894103121929</a>
      <pre>
	Stylegan3 CLIP guidance colab notebook (kind of WIP)
	<a href="https://colab.research.google.com/drive/1eYlenR1GHPZXt-YuvXabzO9wfh9CWY36">https://colab.research.google.com/drive/1eYlenR1GHPZXt-YuvXabzO9wfh9CWY36</a>
	<br />
	10:42 AM · Oct 13, 2021
      </pre>
    </li>

    <li><a href="https://twitter.com/ak92501/status/1451368809294667777">https://twitter.com/ak92501/status/1451368809294667777</a>
      <pre>
	StyleAlign: Analysis and Applications of Aligned StyleGAN Models
	abs: https://arxiv.org/abs/2110.11323

	approach yields sota results, while requiring only simple fine-tuning and inversion

	11:04 AM · Oct 22, 2021·Twitter Web App
      </pre>
      <a href="Screen Shot 2021-11-24 at 17.15.09.png"><!-- 1189x1141 -->
	<img src="Screen Shot 2021-11-24 at 17.15.09_thumb.jpg" width="400" height="384" style="border: 2px #ccc solid;" /></a>
      <ul>
	<li><a href="https://twitter.com/zongze_wu/status/1451423960306958337">https://twitter.com/zongze_wu/status/1451423960306958337</a>
	  <pre>
	    our github link is here
	    <a href="https://github.com/betterze/StyleAlign">https://github.com/betterze/StyleAlign</a>

	    thx a lot.

	    2:43 PM · Oct 22, 2021·Twitter Web App
	  </pre>
	</li>
	<li><a href="https://arxiv.org/abs/2110.11323">arxiv:2110.11323</a>
	  (<a href="arxiv-2110.11323-stylealign.pdf">local copy</a>)
	  <br />
	  StyleAlign: Analysis and Applications of Aligned StyleGAN Models
	</li>
      </ul>
    </li>


    <li><a href="https://twitter.com/omiita_atiimo/status/1461121906552147969">https://twitter.com/omiita_atiimo/status/1461121906552147969</a>
      <pre>
	StyleGAN関連が幅広くまとまっています。

	1から3だけでなく、
	InversionやEditing系、StyleCLIPなどもまとめられていてすごいです。
	ペーパー読むのも合わせたら記事を書くのにどのくらいの時間がかかったのだろうか

	8:59 AM · Nov 18, 2021·Twitter for iPhone
      </pre>
      <ul>
	<li><a href="https://twitter.com/shion_honda/status/1458602488479707139">https://twitter.com/shion_honda/status/1458602488479707139</a>
	  <pre>
	    StyleGAN’s super-realistic images have been inspiring many application works.
	    To have some sort of organized view on them,
	    this post covers important papers with a focus on image manipulation.

	    Awesome StyleGAN Applications | Hippocampus’s Garden

	    10:08 AM · Nov 11, 2021·Twitter Web App
	  </pre>
	</li>
	<li><a href="https://hippocampus-garden.com/stylegans/">Awesome StyleGAN Applications (November 12, 2021)</a>
	  <br />
	  <a href="https://hippocampus-garden.com/stylegans/">
	    <img src="Screen Shot 2021-11-24 at 16.57.16_thumb.jpg" width="800" height="424" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>

    <li>CLIP 関係
      <ul>
	<li><a href="https://twitter.com/Norod78/status/1451590554723274755">https://twitter.com/Norod78/status/1451590554723274755</a>
	  <pre>
	    ClipArtify :p

	    1:45 AM · Oct 23, 2021·Twitter for iPhone
	  </pre>
	  <a href="Screen Shot 2021-11-24 at 16.58.53.png"><!-- 1187x1317 -->
	    <img src="Screen Shot 2021-11-24 at 16.58.53_thumb.jpg" width="361" height="400" style="border: 2px #ccc solid;" /></a>
	</li>

	<li>BASIC
	  <ul>
	    <li><a href="https://arxiv.org/abs/2111.10050">arxiv:2111.10050</a>
	      (<a href="arxiv-2111.10050-basic.pdf">local copy</a>)
	      <br />
	      Combined Scaling for Zero-shot Transfer Learning
	    </li>

	    <li><a href="https://twitter.com/ak92501/status/1462595800568147979">https://twitter.com/ak92501/status/1462595800568147979</a>
	      <pre>
		Combined Scaling for Zero-shot Transfer Learning
		abs: https://arxiv.org/abs/2111.10050

		combined scaling method called BASIC that achieves
		85.7% top-1 zero-shot accuracy on the ImageNet
		ILSVRC-2012 validation set, surpassing the best-published
		zero-shot models – CLIP and ALIGN – by 9.3%

		10:36 AM · Nov 22, 2021·Twitter Web App
	      </pre>
	    </li>

	    <li><a href="https://twitter.com/hardmaru/status/1462677068508917762">https://twitter.com/hardmaru/status/1462677068508917762</a>
	      <pre>
		Combined Scaling for Zero-shot Transfer Learning

		Another data point for Sutton’s “Bitter Lesson”:
		more data, bigger model, bigger batch sizes combined
		leads to big performance gains for zero-shot learning.
		
		New paper by 
		@hieupham789
		and others 
		@GoogleAI

		https://arxiv.org/abs/2111.10050

		3:59 PM · Nov 22, 2021·Twitter Web App
	      </pre>
	    </li>
	  </ul>
	</li><!-- BASIC -->


      </ul>
    </li><!-- CLIP 関係 -->

    <li><a href="https://twitter.com/Norod78/status/1447578622408204298">Norod78/status/1447578622408204298</a>
      <pre>
	Fine-tuned 
	@yuvalalaluf ’s pixel2style2pixel on ~4.5k FFHQ/ToonMoji pairs using 
	@Buntworthy ’s improved branch.
	<a href="https://github.com/justinpinkney/pixel2style2pixel/tree/nw">https://github.com/justinpinkney/pixel2style2pixel/tree/nw</a>
	Love the results. Cheers!
      </pre>
      12:03 AM · Oct 12, 2021
      <br />
      <a href="Screen Shot 2021-11-24 at 17.00.07.png"><!-- 1189x1099 -->
	<img src="Screen Shot 2021-11-24 at 17.00.07_thumb.jpg" width="400" height="370" style="border: 2px #ccc solid;" /></a>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="images">画像系、そのほか</h3>
  <ul>
    <li><a href="https://twitter.com/_xiongjie_/status/1462272717881692161">https://twitter.com/_xiongjie_/status/1462272717881692161</a>
      <pre>
	拡大だけでなく任意の変換に適用できる超解像手法。これは面白いな〜〜

	https://github.com/sanghyun-son/srwarp

	1:12 PM · Nov 21, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/kzykmyzw/status/1460963868113469440">https://twitter.com/kzykmyzw/status/1460963868113469440</a>
      <pre>
	GANの学習中に生成されるfake画像をreal画像として学習データに加えてやると
	少ない学習データでもそれなりに高品質な生成ができるようになる。
	fake画像をどれだけ使うかを適応的に調整するところがポイントらしい

	10:31 PM · Nov 17, 2021·Twitter Web App
      </pre>
    </li>


    <li><a href="https://twitter.com/seishin55/status/1462656713803272192">https://twitter.com/seishin55/status/1462656713803272192</a>
      <pre>
	YOLOv5のpaperがようやく出そう
	https://github.com/ultralytics/yolov5/issues/1333

	we are targeting a paper release by the start of PyTorch Dev day, December 1st.
	If the YOLOv5 paper is not published by then I will eat my hat.

	2:38 PM · Nov 22, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/stateofai_ja/status/1448043093198999555">stateofai_ja/status/1448043093198999555</a>
      <pre>
	ImageNetなど有名なデータのテストセットには平均3.4%ものラベル誤りが存在し、
	最近のSOTAモデルはこれに過適合している。
	ノイズが多い現実世界のデータでは、ResNet-18等シンプルなモデルの方が性能が良い

	https://arxiv.org/abs/2103.14749
      </pre>
      6:49 AM · Oct 13, 2021
    </li>

    <li><a href="https://twitter.com/shion_honda/status/1461714027814391820">https://twitter.com/shion_honda/status/1461714027814391820</a>
      <pre>
	最近注目されている深層生成モデルであるDiffusion Modelについての解説記事。
	GAN、VAE、Flowとの対比の図がわかりやすいですが、
	"math-heavy"のタグがついている通り、数式がかなり重いです

	What are Diffusion Models?
	https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html

	12:12 AM · Nov 20, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/AkiraTOSEI/status/1449184364151992324">https://twitter.com/AkiraTOSEI/status/1449184364151992324</a>
      <pre>
	https://arxiv.org/abs/2004.08697
	VAEで潜在表現のdisentangleな表現を学習することができるCausalVAEを提案。
	潜在層に学習パラメータAを設け、潜在変数同士の因果関係を学習させる。
	Aを三角行列にすることで、潜在変数を再現するような制約をかける。
	Toy ModelやCelebAで要素が分離できていることを確認。

	10:24 AM · Oct 16, 2021·Twitter Web App
      </pre>
    </li>

  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <h3 id="self-supervised">Self-Supervised</h3>
  <ul>
    <li><a href="https://twitter.com/Maxwell_110/status/1452393462578417669">https://twitter.com/Maxwell_110/status/1452393462578417669</a>
      <pre>
	Facebook Loves Self-Supervised Learning Globe with meridians

	https://analyticsindiamag.com/facebook-loves-self-supervised-learning-period/

	FAIR の過去四年間における，自己教師学習の開発時系列チャート ⬇

	6:56 AM · Oct 25, 2021·TweetDeck
      </pre>
      <ul>
	<li><a href="https://twitter.com/Maxwell_110/status/1452393210634899457">https://twitter.com/Maxwell_110/status/1452393210634899457</a>
	  <pre>
	    VISSL (FAIR) Package

	    https://github.com/facebookresearch/vissl

	    VISSL は画像における自己教師学習のための PyTorch-base ライブラリ

	    SimCLR・NPID 等の SOTA な自己教師学習が 60 種以上，
	    同時に，ベンチマーク（imagenet1k・COCO 等）も提供されている

	    また，ZeRO 等の大規模学習のためのモジュールも簡単に実装可能

	    6:55 AM · Oct 25, 2021·TweetDeck
	  </pre>
	</li>
      </ul>
    </li>

    <li><a href="https://twitter.com/hillbig/status/1452424459139239937">https://twitter.com/hillbig/status/1452424459139239937</a>
      <pre>
	FlexMatchは半教師あり学習で疑似ラベル導入時にカリキュラムを導入。
	クラス毎に学習の進捗度を、確信度が閾値より高くなっているサンプル割合として導入。
	進捗度が低いクラスは、誤差に考慮する閾値が下がり、より多く学習する。
	半教師あり画像認識のSOTA性能達成

	8:59 AM · Oct 25, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/wayama_ryousuke/status/1450980946912505858">https://twitter.com/wayama_ryousuke/status/1450980946912505858</a>
      <pre>
	VideoCLIPのようなマルチモーダルの対照学習を音声、動作から学ぶのが主流になるかもしれない。
	むしろimagenetみたいな人間が恣意的に付けたラベルを元に事前学習した場合、
	実世界ではロバストではないので、教師無し学習が主流になるかもしれない。
	<a href="https://github.com/pytorch/fairseq/tree/main/examples/MMPT">https://github.com/pytorch/fairseq/tree/main/examples/MMPT</a>

	9:23 AM · Oct 21, 2021·Twitter Web App
      </pre>
      <ul>
	<li><a href="https://arxiv.org/abs/2007.16189">arxiv:2007.16189</a>
	  (<a href="arxiv-2007.16189-eyes_of_a_child.pdf">local copy</a>)
	  <br />
	  Self-supervised learning through the eyes of a child
	</li>
      </ul>
    </li>


  </ul>

  <h3 id="misc">そのほか</h3>

  <ul>
    <li><a href="https://twitter.com/icoxfog417/status/1444460422996713474">icoxfog417/status/1444460422996713474</a>
      <pre>
	pandas/cuDFのデータフレームから、特徴量を自動的に作成してくれるライブラリ。
	集計キーを指定した統計特徴(平均/最大値など)、
	目的変数ごとの特徴作成(Target Encoding)などを自動で行ってくれる。
	Optunaと連携した特徴選択が可能。
      </pre>
      9:32 AM · Oct 3, 2021
    </li>

    
    <li><a href="https://twitter.com/hillbig/status/1444807667944353797">hillbig/status/1444807667944353797</a>
      <pre>
	SGDが生み出す確率論的なノイズがNNの汎化性を生み出すと広く考えられ
	様々な理論が提唱されているが、フルバッチの学習でも（実用的ではないが）
	長い学習と適切な正則化を加えることで同じ汎化性能を達成できるため、
	確率論的なノイズ以外での汎化性能獲得の説明が必要
      </pre>
      8:32 AM · Oct 4, 2021
    </li>


    <li><a href="https://twitter.com/k09ht/status/1445875119167922176">k09ht/status/1445875119167922176</a>
      <pre>
	Phys. Rev. X 11, 031059 (2021)
	-  Statistical Mechanics of Deep Linear Neural Networks:
	The Backpropagating Kernel Renormalization
      </pre>
      7:14 AM · Oct 7, 2021
    </li>


    <li><a href="https://twitter.com/thienan496/status/1446757957010481156">thienan496/status/1446757957010481156</a>
      <pre>
	Trying to solve the Burgers’ equation with a neural net
	(ie. just minimize the residuals with backprop).
	Not terribly impressed by these "physically informed" NNet
	that are in 100s of papers recentlyFace with cold sweat
	I can see this can be useful in higher dimensions though...
	#jax #PINN #maths
      </pre>
      5:42 PM · Oct 9, 2021
    </li>

    <li><a href="https://twitter.com/hillbig/status/1447314463418294276">hillbig/status/1447314463418294276</a>
      <pre>
	SEGNNはGNNが頂点や枝の属性に従来のようにスカラーだけでなく
	速度やスピンなどベクトルやテンソルを扱っても
	並進、回転、反射操作に同変となるよう、
	線形変換はクレブシュ-ゴルダンテンソル積で表し、
	同変な活性化関数を使う。
	属性は球面調和関数を使った埋め込みを使う
      </pre>
      6:33 AM · Oct 11, 2021
    </li>
    
    <li><a href="https://twitter.com/Maxwell_110/status/1447319780394881024">Maxwell_110/status/1447319780394881024</a>
      <pre>
	Grokking Memo

	https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf

	二項演算式の sequence（Fig. A.1.1）を Transformer に入力し演算の解を学習

	sequence の照応関係の単純な記憶ではなく，
	overfit の時点を遥かに超えた epoch で，
	演算規則が学習される Grokking 現象を報告（Fig.1）

	少量データ時，特に顕著に発生とのこと
      </pre>
      6:55 AM · Oct 11, 2021
    </li>



    <li><a href="https://twitter.com/Maxwell_110/status/1462210398879322113">https://twitter.com/Maxwell_110/status/1462210398879322113</a>
      <pre>
	OpenFold Package

	https://github.com/aqlaboratory/openfold

	DeepMind 社の AlphaFold 2 を PyTorch で再現

	DeepMind の公開コードと異なり，推論だけでなく学習も可能とのことで，単なる再現ではなく拡張されている感もある

	9:05 AM · Nov 21, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1461452907371372548">https://twitter.com/Maxwell_110/status/1461452907371372548</a>
      <pre>
	TorchGeo Package

	https://github.com/microsoft/torchgeo

	TorchGeo は Microsoft による地理空間分析に特化した PyTorch 用ライブラリ Fire

	- 地理データ
	- 地理変化検出（ChangeStar: https://arxiv.org/abs/2108.07002）等のモデル

	や PyTorch Lightning Class High voltage sign の学習用 API 等を実装

	Doc ➡︎ https://torchgeo.readthedocs.io/en/latest/index.html

	6:55 AM · Nov 19, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/ak92501/status/1461354957022449679">https://twitter.com/ak92501/status/1461354957022449679</a>
      <pre>
	Single-stage Keypoint-based Category-level Object Pose Estimation from an RGB Image
	abs: https://arxiv.org/abs/2109.06161
	github: https://github.com/NVlabs/CenterPose

	12:25 AM · Nov 19, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/shu65/status/1460744854263107584">https://twitter.com/shu65/status/1460744854263107584</a>
      <pre>
	JAXとPyTorchの速度の評価したまとめ。JAXさん強い・・・

	8:01 AM · Nov 17, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/hillbig/status/1460725631692406785">https://twitter.com/hillbig/status/1460725631692406785</a>
      <pre>
	SS-Convは疎な入出力の3D畳み込み操作でSE(3)同変性を実現、
	精度と高効率を両立。回転同変性を球面調和関数の線形結合で実現、
	同じ相対位置を持つactiveな入出力をまとめた後、行列間積として効率的に処理する。
	3次元姿勢推定タスクでSOTA性能達成

	6:45 AM · Nov 17, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1458916192245071933">https://twitter.com/Maxwell_110/status/1458916192245071933</a>
      <pre>
	MICLe (Google) Memo

	https://arxiv.org/abs/2101.05224

	MICLe は医療画像特有の性質に基づいた対照学習

	医療画像は，異角度での撮像など，一症例に対して複数画像が存在する事が多いが，MICLe はその複数画像間で対照学習を行う

	MICLe を使った学習 (Fig. 1) は，精度・分布シフトへの堅牢性を改善すると報告

	6:55 AM · Nov 12, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/Luke_Metz/status/1458661090326286336">https://twitter.com/Luke_Metz/status/1458661090326286336</a>
      <pre>
	New paper: when to use gradients

	https://arxiv.org/abs/2111.05803

	DL researchers often compute derivatives though just about everything
	(physics simulators, optimization procedures, renderers).
	Sometimes these gradients are useful, other times they are not.

	We explore why.

	1/7

	2:01 PM · Nov 11, 2021·Twitter Web App
      </pre>
      <ul>
	<li><a href="arxiv-2111.05803_gradients.pdf">arxiv-2111.05803_gradients.pdf</a></li>
      </ul>
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1454929925627076611">https://twitter.com/Maxwell_110/status/1454929925627076611</a>
      <pre>
	NON-Deep Networks Memo

	https://arxiv.org/abs/2110.07641

	Intel 等による研究 🖊

	異なる解像度をもつブランチで並列化された浅い CNN モデル（ParNet）で，
	深いモデルに劣らない精度を達成

	ParNet の主構成要素は，受容野を広げる Skip-Squeeze-Excitation を
	VGG block に組み込んだものになっている

	6:55 AM · Nov 1, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/hardmaru/status/1453210784268496898">https://twitter.com/hardmaru/status/1453210784268496898</a>
      <pre>
	Parameter Prediction for Unseen Deep Architectures

	Their graph hypernetwork can predict all 24M parameters of a ResNet-50,
	achieving 60% CIFAR-10 accuracy, and 50% Top-5 accuracy on ImageNet.
	A forward pass takes only a fraction of a second, even on a CPU!

	1:03 PM · Oct 27, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/hayashiyus/status/1450864051932712961">https://twitter.com/hayashiyus/status/1450864051932712961</a>
      <pre>
	漸近ニューラルネット〜ガウス過程〜自由場の対応関係があり，
	有限幅のニューラルネット〜非ガウス過程〜摂動論（場の量子論）の対応関係がある．
	深層学習と場の量子論を結びつけるとても自然な見方が提示されている(気がする)．

	Neural Networks and Quantum Field Theory
	https://cometscome.github.io/DLAP2020/#%E7%AC%AC19%E5%9B%9E

	1:38 AM · Oct 21, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/cyrildiagne/status/1450762117255204866">https://twitter.com/cyrildiagne/status/1450762117255204866</a>
      <pre>
	Today I’m thrilled to launch http://cleanup.pictures Sparkles

	A free tool to remove objects and defects from any picture.

	Personal computer Try it: https://cleanup.pictures
	Robot face Code: https://github.com/initml/cleanup.pictures
	Upwards arrow PH: https://producthunt.com/posts/cleanup-pictures

	Smiling face with 3 hearts 100% free & open-source thanks to 
	@clipdropapp

	6:53 PM · Oct 20, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1449856495642939395">https://twitter.com/Maxwell_110/status/1449856495642939395</a>
      <pre>
	Image Segmentation Using DL Memo

	https://arxiv.org/abs/2001.05566

	semantic/instance/panoptic segmentation アルゴリズムの大規模サーベイ論文

	- 約 20 のデータセットを 2/2.5/3D に分類し特色を整理

	- 2014~2020 年の 100 以上のアルゴリズムを 10 カテゴリに分類しそれぞれの特色・精度を整理

	など

	6:55 AM · Oct 18, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/_daichikonno/status/1449566585274830851">https://twitter.com/_daichikonno/status/1449566585274830851</a>
      <pre>
	ニューロン間の結合を"コピー&ペースト"することで、
	脳のネットワークを半導体上に再現できるのではないか、
	というパースペクティブ論文。

	根拠となる2019年の論文では、in vitroではあるものの
	1,728ニューロンから同時に細胞内記録(!)を達成しており、
	もの凄いです。(続く)
	https://nature.com/articles/s41928-021-00646-1

	11:43 AM · Oct 17, 2021·Twitter Web App
      </pre>
    </li>

    <li><a href="https://twitter.com/Maxwell_110/status/1449178276564377601">https://twitter.com/Maxwell_110/status/1449178276564377601</a>
      <pre>
	HyperColumn Memo

	https://arxiv.org/abs/1411.5752

	CNNs の上・下層の特徴マップはそれぞれ捉えている情報が異なっており，
	segmentation 等の fine-grained なタスクでは，decoder の下層の情報も重要である

	そこで，Hypercolumn は各 stage の特徴マップを upsample + concat し必要な情報を得ている

	10:00 AM · Oct 16, 2021·TweetDeck
      </pre>
    </li>

    <li><a href="https://twitter.com/hillbig/status/1448782925755936770">https://twitter.com/hillbig/status/1448782925755936770</a>
      <pre>
	ADOPは画像群から3次元復元でカメラ姿勢と点群を得た後、
	学習可能な特徴を保持した各点を1画素にラスタライズし複数解像度の疎な画像群を生成後、
	補間、トーンマッピングで新視点画像生成。
	全て微分可能な操作で全パラメータを最適化。新視点画像生成で驚くほどの品質を達成

	7:49 AM · Oct 15, 2021·Twitter Web App
      </pre>
    </li>

  </ul>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center>
    <div id="news2">
    <div style="font-size: 60px; font-weight: bold;">
      最近の話題から、パート２
    </div>
    </div>
  </center>

  <ul>
    <li><a href="https://zenkei-ai-forum.github.io/pages/ZAF202106/ichiki/#ichiki">ZAF-2106</a> での発表
      <ul>
	<li>OpenAI から GPT-3 の招待が来た！</li>
	<li>GitHub が copilot を発表した！</li>
      </ul>
    </li>

    <li>今月、マイクロソフトが正式に GPT-3 ベースのサービスをアナウンス
      <ul>
	<li><a href="https://blogs.microsoft.com/ai/new-azure-openai-service/">New Azure OpenAI Service combines access to powerful GPT-3 language models with Azure’s enterprise capabilities (Jennifer Langston Nov 2, 2021)</a>
	  <br />
	  <a href="https://blogs.microsoft.com/ai/new-azure-openai-service/">
	    <img src="Screen Shot 2021-11-24 at 17.57.08_thumb.jpg" width="800" height="405" style="border: 2px #ccc solid;" /></a>
	</li>
      </ul>
    </li>

    <li><a href="https://azure.microsoft.com/en-us/services/openai-service/">Azure OpenAI Service</a>
      <br />
      <a href="https://azure.microsoft.com/en-us/services/openai-service/">
	<img src="Screen Shot 2021-11-24 at 18.01.56_thumb.jpg" width="800" height="405" style="border: 2px #ccc solid;" /></a>
    </li>

  </ul>

  <h3 id="news2-copilot">copilot のレビュー</h3>

  <hr />

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />

  <center id="epilogue">
    <div style="font-size: 50px; font-weight: bolder;">
      今日のおわりに
    </div>
  </center>

  <p>……</p>

  <h3>今後の予定</h3>
  <ul>
    <li>次回 ZAF は 2021 年の最後、１２月２２日開催の予定です。</li>
    <li>ZAF 講演者、 ZAM 執筆者、絶賛、第募集中です！<br />
      お気軽にお問い合わせください！</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

  <hr />
  <hr />

  <h2 id="detailed-toc">総合目次</h2>
  <ul>
    <li>...</li>
  </ul>

  <br /><br /><br /><center>...♦...</center><br /><br /><br />

</section>

</body>           
</html>
